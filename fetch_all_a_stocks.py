#!/usr/bin/env python3
"""
Aè‚¡å…¨å¸‚åœºæ•°æ®è·å–è„šæœ¬

åŠŸèƒ½ï¼š
- è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨å’ŒæŒ‡æ•°çš„å†å²æ—¥çº¿æ•°æ®
- è‡ªåŠ¨é¢‘ç‡æ§åˆ¶ï¼Œé¿å…è¶…è¿‡TuShare APIé™åˆ¶
- æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¯ä¸­æ–­åç»§ç»­è·å–
- å®æ—¶è¿›åº¦æ˜¾ç¤ºå’Œç»Ÿè®¡ä¿¡æ¯
- æ•°æ®ä¿å­˜åˆ°Qlibå¯ç”¨æ ¼å¼

ä½¿ç”¨æ–¹å¼ï¼š
    # è®¾ç½®TuShare Tokenç¯å¢ƒå˜é‡
    export TUSHARE_TOKEN="your_token_here"

    # è¿è¡Œè„šæœ¬
    python fetch_all_a_stocks.py

    # æˆ–æŒ‡å®šå‚æ•°
    python fetch_all_a_stocks.py --start-date 20200101 --batch-size 100
"""

import os
import sys
import time
import json
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from collections import defaultdict

import pandas as pd
import numpy as np

# æ·»åŠ Qlibè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from qlib.contrib.data.tushare import TuShareConfig, TuShareProvider
from qlib.contrib.data.tushare.api_client import TuShareAPIClient
from qlib.contrib.data.tushare.utils import TuShareCodeConverter


@dataclass
class FetchConfig:
    """æ•°æ®è·å–é…ç½®"""
    # TuShare Tokené…ç½®
    token: str = field(default_factory=lambda: os.getenv("TUSHARE_TOKEN", ""))

    # æ•°æ®èŒƒå›´é…ç½®
    start_date: str = "20000101"  # å¼€å§‹æ—¥æœŸï¼ˆTuShareæ ¼å¼ï¼šYYYYMMDDï¼‰
    end_date: str = None  # ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰

    # æ‰¹é‡è·å–é…ç½®
    batch_size: int = 100  # æ¯æ‰¹è·å–çš„è‚¡ç¥¨æ•°é‡
    max_parallel: int = 1  # æœ€å¤§å¹¶è¡Œæ•°ï¼ˆæš‚æœªå®ç°ï¼‰

    # é¢‘ç‡æ§åˆ¶é…ç½®
    rate_limit: int = 180  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆç•¥ä½äºTuShareé™åˆ¶200ï¼‰
    request_interval: float = 0.35  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

    # é‡è¯•é…ç½®
    max_retries: int = 3
    retry_delay: float = 2.0

    # å­˜å‚¨é…ç½®
    data_dir: str = field(default_factory=lambda: str(Path.home() / ".qlib" / "qlib_data" / "cn_data"))
    progress_db: str = field(default_factory=lambda: str(Path.home() / ".qlib" / "fetch_progress.db"))

    # å…¶ä»–é…ç½®
    enable_cache: bool = True
    adjust_price: bool = True  # æ˜¯å¦å¤æƒ
    skip_existing: bool = True  # è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®

    def __post_init__(self):
        if self.end_date is None:
            self.end_date = datetime.now().strftime("%Y%m%d")


@dataclass
class FetchStats:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    start_time: float = field(default_factory=time.time)
    total_stocks: int = 0
    total_indices: int = 0
    success_stocks: int = 0
    success_indices: int = 0
    failed_stocks: int = 0
    failed_indices: int = 0
    skipped_stocks: int = 0
    total_requests: int = 0
    total_data_points: int = 0
    failed_codes: Dict[str, str] = field(default_factory=dict)

    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time

    @property
    def progress_percent(self) -> float:
        total = self.total_stocks + self.total_indices
        if total == 0:
            return 0.0
        completed = self.success_stocks + self.success_indices + self.failed_stocks + self.failed_indices
        return (completed / total) * 100


class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨ - ä½¿ç”¨SQLiteä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS fetch_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                code TEXT NOT NULL UNIQUE,
                type TEXT NOT NULL,
                start_date TEXT NOT NULL,
                end_date TEXT NOT NULL,
                status TEXT NOT NULL,
                data_count INTEGER DEFAULT 0,
                last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                error_message TEXT,
                retry_count INTEGER DEFAULT 0
            )
        """)

        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_code ON fetch_progress(code)
        """)

        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_status ON fetch_progress(status)
        """)

        self.conn.commit()

    def save_progress(self, code: str, data_type: str, start_date: str,
                     end_date: str, status: str, data_count: int = 0,
                     error_message: str = None):
        """ä¿å­˜è¿›åº¦"""
        self.conn.execute("""
            INSERT OR REPLACE INTO fetch_progress
            (code, type, start_date, end_date, status, data_count, error_message, last_update)
            VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """, (code, data_type, start_date, end_date, status, data_count, error_message))
        self.conn.commit()

    def get_progress(self, code: str, data_type: str,
                    start_date: str, end_date: str) -> Optional[dict]:
        """è·å–è¿›åº¦"""
        cursor = self.conn.execute("""
            SELECT * FROM fetch_progress
            WHERE code = ? AND type = ? AND start_date = ? AND end_date = ?
        """, (code, data_type, start_date, end_date))
        row = cursor.fetchone()
        if row:
            return {
                "id": row[0],
                "code": row[1],
                "type": row[2],
                "start_date": row[3],
                "end_date": row[4],
                "status": row[5],
                "data_count": row[6],
                "last_update": row[7],
                "error_message": row[8],
                "retry_count": row[9]
            }
        return None

    def get_pending_codes(self, data_type: str, start_date: str,
                         end_date: str) -> List[str]:
        """è·å–å¾…å¤„ç†çš„ä»£ç """
        cursor = self.conn.execute("""
            SELECT DISTINCT code FROM fetch_progress
            WHERE type = ? AND start_date = ? AND end_date = ?
            AND status IN ('pending', 'failed')
            ORDER BY code
        """, (data_type, start_date, end_date))
        return [row[0] for row in cursor.fetchall()]

    def get_all_codes(self, data_type: str, start_date: str,
                     end_date: str) -> List[Tuple[str, str]]:
        """è·å–æ‰€æœ‰ä»£ç åŠå…¶çŠ¶æ€"""
        cursor = self.conn.execute("""
            SELECT code, status FROM fetch_progress
            WHERE type = ? AND start_date = ? AND end_date = ?
            ORDER BY code
        """, (data_type, start_date, end_date))
        return [(row[0], row[1]) for row in cursor.fetchall()]

    def get_stats(self, data_type: str, start_date: str,
                 end_date: str) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        cursor = self.conn.execute("""
            SELECT status, COUNT(*) as count
            FROM fetch_progress
            WHERE type = ? AND start_date = ? AND end_date = ?
            GROUP BY status
        """, (data_type, start_date, end_date))
        return {row[0]: row[1] for row in cursor.fetchall()}

    def reset_failed(self, data_type: str, start_date: str, end_date: str):
        """é‡ç½®å¤±è´¥çš„ä»»åŠ¡"""
        self.conn.execute("""
            UPDATE fetch_progress
            SET status = 'pending', retry_count = 0, error_message = NULL
            WHERE type = ? AND start_date = ? AND end_date = ?
            AND status = 'failed'
        """, (data_type, start_date, end_date))
        self.conn.commit()

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()


class AStockFetcher:
    """Aè‚¡æ•°æ®è·å–å™¨"""

    def __init__(self, config: FetchConfig):
        self.config = config
        self.stats = FetchStats()
        self.progress = None
        self.provider = None
        self.api_client = None

        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = Path(config.data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        self.progress = ProgressManager(config.progress_db)

        # æ‰“å°é…ç½®ä¿¡æ¯
        self._print_config()

    def _print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print("=" * 80)
        print("ğŸ“Š Aè‚¡å…¨å¸‚åœºæ•°æ®è·å–è„šæœ¬")
        print("=" * 80)
        print(f"æ•°æ®èŒƒå›´: {self.config.start_date} - {self.config.end_date}")
        print(f"æ‰¹é‡å¤§å°: {self.config.batch_size}")
        print(f"é¢‘ç‡é™åˆ¶: {self.config.rate_limit} è¯·æ±‚/åˆ†é’Ÿ")
        print(f"è¯·æ±‚é—´éš”: {self.config.request_interval:.2f} ç§’")
        print(f"æ•°æ®ç›®å½•: {self.config.data_dir}")
        print(f"è¿›åº¦æ•°æ®åº“: {self.config.progress_db}")
        print(f"å¤æƒ: {'æ˜¯' if self.config.adjust_price else 'å¦'}")
        print(f"è·³è¿‡å·²å­˜åœ¨: {'æ˜¯' if self.config.skip_existing else 'å¦'}")
        print("=" * 80)
        print()

    def _init_provider(self):
        """åˆå§‹åŒ–TuShareæä¾›è€…"""
        tushare_config = TuShareConfig(
            token=self.config.token,
            rate_limit=self.config.rate_limit,
            max_retries=self.config.max_retries,
            retry_delay=self.config.retry_delay,
            adjust_price=self.config.adjust_price,
            enable_cache=self.config.enable_cache,
            enable_api_logging=True,
            log_level="INFO"
        )

        self.provider = TuShareProvider(tushare_config)
        self.api_client = self.provider.api_client

        print("âœ… TuShareæä¾›è€…åˆå§‹åŒ–å®Œæˆ")
        print()

    def get_all_stock_codes(self) -> List[Tuple[str, str]]:
        """è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä»£ç """
        print("ğŸ“ˆ è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä»£ç ...")

        try:
            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_basic = self.api_client.get_stock_basic(
                list_status="L",  # åªè·å–ä¸Šå¸‚è‚¡ç¥¨
                fields="ts_code,symbol,name,area,industry,market,list_date"
            )

            if stock_basic.empty:
                print("âŒ æœªè·å–åˆ°è‚¡ç¥¨ä»£ç ")
                return []

            # å¤„ç†å­—æ®µæ˜ å°„åçš„åˆ—åï¼ˆts_code -> instrument æˆ–ä¿æŒ ts_codeï¼‰
            code_col = "instrument" if "instrument" in stock_basic.columns else "ts_code"
            codes = [(row[code_col], row["name"]) for _, row in stock_basic.iterrows()]
            print(f"âœ… è·å–åˆ° {len(codes)} åªAè‚¡è‚¡ç¥¨")
            print()

            return codes

        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            return []

    def get_all_index_codes(self) -> List[Tuple[str, str]]:
        """è·å–æ‰€æœ‰æŒ‡æ•°ä»£ç """
        print("ğŸ“Š è·å–æ‰€æœ‰æŒ‡æ•°ä»£ç ...")

        try:
            # ä¸»è¦æŒ‡æ•°ä»£ç åˆ—è¡¨
            index_codes = [
                ("000001.SH", "ä¸Šè¯æŒ‡æ•°"),
                ("399001.SZ", "æ·±è¯æˆæŒ‡"),
                ("399006.SZ", "åˆ›ä¸šæ¿æŒ‡"),
                ("000300.SH", "æ²ªæ·±300"),
                ("000016.SH", "ä¸Šè¯50"),
                ("399905.SZ", "ä¸­è¯500"),
                ("000688.SH", "ç§‘åˆ›50"),
                ("000905.SH", "ä¸­è¯1000"),
            ]

            print(f"âœ… è·å–åˆ° {len(index_codes)} ä¸ªä¸»è¦æŒ‡æ•°")
            print()

            return index_codes

        except Exception as e:
            print(f"âŒ è·å–æŒ‡æ•°ä»£ç å¤±è´¥: {e}")
            return []

    def fetch_stock_data(self, code: str, name: str) -> bool:
        """è·å–å•åªè‚¡ç¥¨æ•°æ®"""
        try:
            # è½¬æ¢ä¸ºQlibæ ¼å¼
            qlib_code = TuShareCodeConverter.to_qlib_format(code)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.config.skip_existing:
                stock_file = self.data_dir / "stock_data" / f"{qlib_code}.csv"
                if stock_file.exists():
                    existing_df = pd.read_csv(stock_file, parse_dates=["date"])
                    if not existing_df.empty:
                        last_date = existing_df["date"].max().strftime("%Y%m%d")
                        if last_date >= self.config.end_date:
                            # æ•°æ®å·²ç»æ˜¯æœ€æ–°çš„
                            self.stats.skipped_stocks += 1
                            self.progress.save_progress(
                                code, "stock", self.config.start_date, self.config.end_date,
                                "skipped", len(existing_df)
                            )
                            return True

            # è·å–æ—¥çº¿æ•°æ®
            df = self.api_client.get_daily_data(
                ts_code=code,
                start_date=self.config.start_date,
                end_date=self.config.end_date,
                adj="qfq" if self.config.adjust_price else None
            )

            if df.empty:
                # æ²¡æœ‰æ•°æ®ï¼ˆå¯èƒ½æ˜¯æ–°è‚¡ï¼‰
                self.progress.save_progress(
                    code, "stock", self.config.start_date, self.config.end_date,
                    "no_data", 0, "No data available"
                )
                return False

            # ä¿å­˜æ•°æ®
            stock_file = self.data_dir / "stock_data" / f"{qlib_code}.csv"
            stock_file.parent.mkdir(parents=True, exist_ok=True)
            df.to_csv(stock_file, index=False)

            self.stats.success_stocks += 1
            self.stats.total_data_points += len(df)
            self.stats.total_requests += 1

            self.progress.save_progress(
                code, "stock", self.config.start_date, self.config.end_date,
                "success", len(df)
            )

            return True

        except Exception as e:
            error_msg = str(e)
            self.stats.failed_codes[code] = error_msg
            self.stats.failed_stocks += 1

            self.progress.save_progress(
                code, "stock", self.config.start_date, self.config.end_date,
                "failed", 0, error_msg
            )

            print(f"  âŒ è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥ {code} ({name}): {error_msg}")
            return False

    def fetch_index_data(self, code: str, name: str) -> bool:
        """è·å–å•ä¸ªæŒ‡æ•°æ•°æ®"""
        try:
            # è½¬æ¢ä¸ºQlibæ ¼å¼
            qlib_code = TuShareCodeConverter.to_qlib_format(code)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.config.skip_existing:
                index_file = self.data_dir / "index_data" / f"{qlib_code}.csv"
                if index_file.exists():
                    existing_df = pd.read_csv(index_file, parse_dates=["date"])
                    if not existing_df.empty:
                        last_date = existing_df["date"].max().strftime("%Y%m%d")
                        if last_date >= self.config.end_date:
                            # æ•°æ®å·²ç»æ˜¯æœ€æ–°çš„
                            self.stats.skipped_stocks += 1
                            self.progress.save_progress(
                                code, "index", self.config.start_date, self.config.end_date,
                                "skipped", len(existing_df)
                            )
                            return True

            # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
            df = self.api_client.get_index_daily(
                ts_code=code,
                start_date=self.config.start_date,
                end_date=self.config.end_date
            )

            if df.empty:
                # æ²¡æœ‰æ•°æ®
                self.progress.save_progress(
                    code, "index", self.config.start_date, self.config.end_date,
                    "no_data", 0, "No data available"
                )
                return False

            # ä¿å­˜æ•°æ®
            index_file = self.data_dir / "index_data" / f"{qlib_code}.csv"
            index_file.parent.mkdir(parents=True, exist_ok=True)
            df.to_csv(index_file, index=False)

            self.stats.success_indices += 1
            self.stats.total_data_points += len(df)
            self.stats.total_requests += 1

            self.progress.save_progress(
                code, "index", self.config.start_date, self.config.end_date,
                "success", len(df)
            )

            return True

        except Exception as e:
            error_msg = str(e)
            self.stats.failed_codes[code] = error_msg
            self.stats.failed_indices += 1

            self.progress.save_progress(
                code, "index", self.config.start_date, self.config.end_date,
                "failed", 0, error_msg
            )

            print(f"  âŒ è·å–æŒ‡æ•°æ•°æ®å¤±è´¥ {code} ({name}): {error_msg}")
            return False

    def fetch_stocks_batch(self, codes: List[Tuple[str, str]], batch_idx: int, total_batches: int):
        """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®"""
        batch_size = len(codes)
        print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx}/{total_batches} ({batch_size} åªè‚¡ç¥¨)")

        for i, (code, name) in enumerate(codes, 1):
            # æ˜¾ç¤ºè¿›åº¦
            print(f"  [{i}/{batch_size}] {code} {name}...", end="\r")

            # é¢‘ç‡æ§åˆ¶
            time.sleep(self.config.request_interval)

            # è·å–æ•°æ®
            success = self.fetch_stock_data(code, name)

            if success:
                print(f"  [{i}/{batch_size}] âœ… {code} {name}")

            # å®šæœŸæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if i % 10 == 0:
                self._print_progress()

    def fetch_indices_batch(self, codes: List[Tuple[str, str]]):
        """æ‰¹é‡è·å–æŒ‡æ•°æ•°æ®"""
        print(f"ğŸ“Š è·å–æŒ‡æ•°æ•°æ® ({len(codes)} ä¸ªæŒ‡æ•°)")

        for i, (code, name) in enumerate(codes, 1):
            # æ˜¾ç¤ºè¿›åº¦
            print(f"  [{i}/{len(codes)}] {code} {name}...", end="\r")

            # é¢‘ç‡æ§åˆ¶
            time.sleep(self.config.request_interval)

            # è·å–æ•°æ®
            success = self.fetch_index_data(code, name)

            if success:
                print(f"  [{i}/{len(codes)}] âœ… {code} {name}")

        self._print_progress()

    def _print_progress(self):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        elapsed = self.stats.elapsed_time
        success = self.stats.success_stocks + self.stats.success_indices
        failed = self.stats.failed_stocks + self.stats.failed_indices
        skipped = self.stats.skipped_stocks
        total = self.stats.total_stocks + self.stats.total_indices

        print(f"    è¿›åº¦: {success}/{total} ({self.stats.progress_percent:.1f}%) | "
              f"æˆåŠŸ: {success} | å¤±è´¥: {failed} | è·³è¿‡: {skipped} | "
              f"è€—æ—¶: {elapsed:.0f}ç§’ | æ•°æ®ç‚¹: {self.stats.total_data_points:,}")

    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        elapsed = self.stats.elapsed_time
        success_stocks = self.stats.success_stocks
        success_indices = self.stats.success_indices
        failed_stocks = self.stats.failed_stocks
        failed_indices = self.stats.failed_indices
        skipped_stocks = self.stats.skipped_stocks
        total_stocks = self.stats.total_stocks
        total_indices = self.stats.total_indices

        print()
        print("=" * 80)
        print("ğŸ“Š æ•°æ®è·å–å®Œæˆï¼")
        print("=" * 80)
        print(f"è‚¡ç¥¨ç»Ÿè®¡:")
        print(f"  æ€»æ•°: {total_stocks}")
        print(f"  æˆåŠŸ: {success_stocks}")
        print(f"  å¤±è´¥: {failed_stocks}")
        print(f"  è·³è¿‡: {skipped_stocks}")
        print()
        print(f"æŒ‡æ•°ç»Ÿè®¡:")
        print(f"  æ€»æ•°: {total_indices}")
        print(f"  æˆåŠŸ: {success_indices}")
        print(f"  å¤±è´¥: {failed_indices}")
        print()
        print(f"æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ•°æ®ç‚¹: {self.stats.total_data_points:,}")
        print(f"  æ€»è¯·æ±‚æ•°: {self.stats.total_requests}")
        print(f"  æ€»è€—æ—¶: {elapsed:.0f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        print()
        print(f"æ•°æ®ä¿å­˜ä½ç½®: {self.data_dir}")
        print("=" * 80)

        if self.stats.failed_codes:
            print()
            print(f"âš ï¸ å¤±è´¥çš„è‚¡ç¥¨ä»£ç  ({len(self.stats.failed_codes)}):")
            for code, error in list(self.stats.failed_codes.items())[:10]:
                print(f"  {code}: {error}")
            if len(self.stats.failed_codes) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.stats.failed_codes) - 10} ä¸ªå¤±è´¥")

    def run(self, resume: bool = False):
        """è¿è¡Œæ•°æ®è·å–"""
        try:
            # åˆå§‹åŒ–æä¾›è€…
            self._init_provider()

            # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            stock_codes = self.get_all_stock_codes()
            if not stock_codes:
                print("âŒ æœªè·å–åˆ°è‚¡ç¥¨ä»£ç ï¼Œé€€å‡º")
                return

            self.stats.total_stocks = len(stock_codes)

            # è·å–æ‰€æœ‰æŒ‡æ•°ä»£ç 
            index_codes = self.get_all_index_codes()
            self.stats.total_indices = len(index_codes)

            # åˆå§‹åŒ–è¿›åº¦è®°å½•
            if not resume:
                # æ¸…é™¤æ—§çš„è¿›åº¦è®°å½•
                self.progress.conn.execute("""
                    DELETE FROM fetch_progress
                    WHERE start_date = ? AND end_date = ?
                """, (self.config.start_date, self.config.end_date))
                self.progress.conn.commit()

                # æ·»åŠ æ–°çš„è¿›åº¦è®°å½•
                for code, name in stock_codes:
                    self.progress.save_progress(
                        code, "stock", self.config.start_date, self.config.end_date,
                        "pending", 0
                    )

                for code, name in index_codes:
                    self.progress.save_progress(
                        code, "index", self.config.start_date, self.config.end_date,
                        "pending", 0
                    )
            else:
                # ä»è¿›åº¦æ¢å¤
                pending_stocks = self.progress.get_pending_codes(
                    "stock", self.config.start_date, self.config.end_date
                )
                stock_codes = [(c, "") for c in pending_stocks if c in [sc[0] for sc in stock_codes]]

                print(f"ğŸ”„ ä»æ–­ç‚¹æ¢å¤ï¼Œå‰©ä½™ {len(stock_codes)} åªè‚¡ç¥¨å¾…è·å–")
                print()

            # åˆ†æ‰¹è·å–è‚¡ç¥¨æ•°æ®
            batch_size = self.config.batch_size
            total_batches = (len(stock_codes) + batch_size - 1) // batch_size

            for batch_idx in range(1, total_batches + 1):
                start_idx = (batch_idx - 1) * batch_size
                end_idx = min(start_idx + batch_size, len(stock_codes))
                batch_codes = stock_codes[start_idx:end_idx]

                self.fetch_stocks_batch(batch_codes, batch_idx, total_batches)

                # ä¿å­˜ç»Ÿè®¡æ•°æ®
                self._save_stats()

            # è·å–æŒ‡æ•°æ•°æ®
            if index_codes:
                self.fetch_indices_batch(index_codes)

            # æ‰“å°æœ€ç»ˆç»Ÿè®¡
            self._print_final_stats()

        except KeyboardInterrupt:
            print()
            print()
            print("âš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ•°æ®è·å–å·²æš‚åœ")
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥ä½¿ç”¨ --resume å‚æ•°ç»§ç»­è·å–")
            print()
            self._print_progress()
            self._save_stats()

        except Exception as e:
            print()
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        finally:
            # æ¸…ç†èµ„æº
            if self.provider:
                self.provider.close()
            if self.progress:
                self.progress.close()

    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶"""
        stats_file = self.data_dir / "fetch_stats.json"
        stats_data = {
            "start_date": self.config.start_date,
            "end_date": self.config.end_date,
            "total_stocks": self.stats.total_stocks,
            "total_indices": self.stats.total_indices,
            "success_stocks": self.stats.success_stocks,
            "success_indices": self.stats.success_indices,
            "failed_stocks": self.stats.failed_stocks,
            "failed_indices": self.stats.failed_indices,
            "skipped_stocks": self.stats.skipped_stocks,
            "total_data_points": self.stats.total_data_points,
            "total_requests": self.stats.total_requests,
            "last_update": datetime.now().isoformat()
        }
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Aè‚¡å…¨å¸‚åœºæ•°æ®è·å–è„šæœ¬")
    parser.add_argument("--token", type=str, default=os.getenv("TUSHARE_TOKEN", ""),
                       help="TuShare Tokenï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡TUSHARE_TOKENè¯»å–ï¼‰")
    parser.add_argument("--start-date", type=str, default="20000101",
                       help="å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼Œé»˜è®¤ï¼š20000101ï¼‰")
    parser.add_argument("--end-date", type=str, default=None,
                       help="ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼Œé»˜è®¤ï¼šä»Šå¤©ï¼‰")
    parser.add_argument("--batch-size", type=int, default=100,
                       help="æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤ï¼š100ï¼‰")
    parser.add_argument("--rate-limit", type=int, default=180,
                       help="é¢‘ç‡é™åˆ¶ï¼ˆè¯·æ±‚/åˆ†é’Ÿï¼Œé»˜è®¤ï¼š180ï¼‰")
    parser.add_argument("--data-dir", type=str,
                       default=str(Path.home() / ".qlib" / "qlib_data" / "cn_data"),
                       help="æ•°æ®ä¿å­˜ç›®å½•")
    parser.add_argument("--resume", action="store_true",
                       help="ä»æ–­ç‚¹æ¢å¤")
    parser.add_argument("--no-adjust", action="store_true",
                       help="ä¸å¤æƒ")
    parser.add_argument("--no-skip", action="store_true",
                       help="ä¸è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®")

    args = parser.parse_args()

    # æ£€æŸ¥Token
    if not args.token:
        print("âŒ é”™è¯¯: æœªè®¾ç½®TuShare Token")
        print("è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®Tokenï¼š")
        print("  1. ç¯å¢ƒå˜é‡: export TUSHARE_TOKEN='your_token'")
        print("  2. å‘½ä»¤è¡Œå‚æ•°: --token your_token")
        sys.exit(1)

    # åˆ›å»ºé…ç½®
    config = FetchConfig(
        token=args.token,
        start_date=args.start_date,
        end_date=args.end_date,
        batch_size=args.batch_size,
        rate_limit=args.rate_limit,
        data_dir=args.data_dir,
        adjust_price=not args.no_adjust,
        skip_existing=not args.no_skip
    )

    # åˆ›å»ºè·å–å™¨å¹¶è¿è¡Œ
    fetcher = AStockFetcher(config)
    fetcher.run(resume=args.resume)


if __name__ == "__main__":
    main()
