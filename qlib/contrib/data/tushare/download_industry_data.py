#!/usr/bin/env python3
"""
è¡Œä¸šæ¿å—å’Œæ¦‚å¿µæ¿å—æ•°æ®ä¸‹è½½å·¥å…·

åŠŸèƒ½ï¼š
1. ä¸‹è½½æ‰€æœ‰è¡Œä¸šæ¿å—æ•°æ®ï¼ˆç”³ä¸‡ã€è¯ç›‘ä¼šã€ä¸­ä¿¡ï¼‰
2. ä¸‹è½½æ‰€æœ‰æ¦‚å¿µæ¿å—æ•°æ®
3. ä¸‹è½½æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
4. ä¿å­˜ä¸ºä¾¿äºåˆ†æçš„æ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python download_industry_data.py

æˆ–ä½¿ç”¨TuShare Tokenï¼š
    export TUSHARE_TOKEN="your_token"
    python download_industry_data.py
"""

import os
import sys
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import time

# æ·»åŠ qlibè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent.parent))

try:
    from qlib.contrib.data.tushare.config import TuShareConfig
    from qlib.contrib.data.tushare.api_client import TuShareAPIClient
    QLIB_AVAILABLE = True
except ImportError:
    QLIB_AVAILABLE = False
    print("âš ï¸ Qlibæœªå®‰è£…ï¼Œä½¿ç”¨åŸç”Ÿtushare")
    import tushare as ts


class IndustryDataDownloader:
    """è¡Œä¸šæ¿å—æ•°æ®ä¸‹è½½å™¨"""

    def __init__(self, token: str = None, output_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            token: TuShare Token
            output_dir: è¾“å‡ºç›®å½•
        """
        self.token = token or os.getenv("TUSHARE_TOKEN")
        if not self.token:
            raise ValueError("è¯·è®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡æˆ–ä¼ å…¥tokenå‚æ•°")

        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = Path(__file__).parent / "industry_data"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        if QLIB_AVAILABLE:
            config = TuShareConfig(token=self.token)
            self.client = TuShareAPIClient(config)
            self.use_proxied_api = True
        else:
            import tushare as ts
            self.client = ts.pro_api(self.token)
            self.use_proxied_api = False

        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   APIæ¨¡å¼: {'Qlibå°è£…' if self.use_proxied_api else 'åŸç”ŸTuShare'}")

    def download_industry_classification(
        self,
        sources: List[str] = ["SW2021", "ZJH", "CITIC"],
        levels: List[str] = ["L1", "L2", "L3"]
    ) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½è¡Œä¸šåˆ†ç±»æ•°æ®

        Args:
            sources: è¡Œä¸šåˆ†ç±»æ¥æºåˆ—è¡¨
            levels: è¡Œä¸šçº§åˆ«åˆ—è¡¨

        Returns:
            è¡Œä¸šåˆ†ç±»æ•°æ®å­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½è¡Œä¸šåˆ†ç±»æ•°æ®")
        print("="*80)

        industry_data = {}

        for source in sources:
            print(f"\nğŸ“Š ä¸‹è½½ {source} è¡Œä¸šåˆ†ç±»...")

            for level in levels:
                key = f"{source}_{level}"
                print(f"   - {level} çº§åˆ«...", end=" ")

                try:
                    # ä½¿ç”¨åŸç”ŸTuShare APIï¼ˆæ›´ç¨³å®šï¼‰
                    import tushare as ts
                    ts_client = ts.pro_api(self.token)

                    # ä½¿ç”¨index_classifyæ¥å£è·å–è¡Œä¸šåˆ†ç±»
                    df = ts_client.index_classify(
                        level=level,
                        source=source
                    )

                    if df is not None and not df.empty:
                        industry_data[key] = df
                        print(f"âœ… {len(df)} æ¡è®°å½•")
                    else:
                        print(f"âš ï¸ æ— æ•°æ®")

                    # é¿å…APIé¢‘ç‡é™åˆ¶
                    time.sleep(0.2)

                except Exception as e:
                    print(f"âŒ å¤±è´¥: {str(e)}")

        print(f"\nâœ… è¡Œä¸šåˆ†ç±»ä¸‹è½½å®Œæˆï¼Œå…±è·å– {len(industry_data)} ä¸ªåˆ†ç±»")

        return industry_data

    def download_concept(self) -> pd.DataFrame:
        """
        ä¸‹è½½æ¦‚å¿µæ¿å—æ•°æ®

        Returns:
            æ¦‚å¿µæ¿å—DataFrame
        """
        print("\n" + "="*80)
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ¦‚å¿µæ¿å—æ•°æ®")
        print("="*80)

        print("\nğŸ“Š ä¸‹è½½æ¦‚å¿µæ¿å—åˆ—è¡¨...", end=" ")

        try:
            # ä½¿ç”¨åŸç”ŸTuShare APIï¼ˆæ›´ç¨³å®šï¼‰
            import tushare as ts
            ts_client = ts.pro_api(self.token)
            concept_df = ts_client.concept()

            if concept_df is not None and not concept_df.empty:
                print(f"âœ… {len(concept_df)} ä¸ªæ¦‚å¿µæ¿å—")
                return concept_df
            else:
                print("âš ï¸ æ— æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            print(f"âŒ å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def download_concept_members(
        self,
        concept_df: pd.DataFrame,
        max_concepts: int = None
    ) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½æ¦‚å¿µæ¿å—æˆåˆ†è‚¡

        Args:
            concept_df: æ¦‚å¿µæ¿å—DataFrame
            max_concepts: æœ€å¤šä¸‹è½½æ¦‚å¿µæ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰

        Returns:
            æ¦‚å¿µæˆåˆ†è‚¡å­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ¦‚å¿µæ¿å—æˆåˆ†è‚¡")
        print("="*80)

        if concept_df.empty:
            print("âš ï¸ æ¦‚å¿µæ¿å—æ•°æ®ä¸ºç©º")
            return {}

        concept_members = {}

        # è·å–æ¦‚å¿µIDåˆ—è¡¨
        if 'id' in concept_df.columns:
            concept_ids = concept_df['id'].unique()
        elif 'concept_code' in concept_df.columns:
            concept_ids = concept_df['concept_code'].unique()
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ¦‚å¿µIDåˆ—")
            return {}

        # é™åˆ¶ä¸‹è½½æ•°é‡
        if max_concepts:
            concept_ids = concept_ids[:max_concepts]

        print(f"\nğŸ“Š å…± {len(concept_ids)} ä¸ªæ¦‚å¿µæ¿å—")

        for i, concept_id in enumerate(concept_ids, 1):
            concept_name = concept_df[concept_df['id'] == concept_id]['concept_name'].values[0] if 'concept_name' in concept_df.columns else concept_id
            print(f"   [{i}/{len(concept_ids)}] {concept_name} ({concept_id})...", end=" ")

            try:
                # TuShareæ²¡æœ‰ç›´æ¥çš„æ¦‚å¿µæˆåˆ†è‚¡æ¥å£
                # éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è·å–
                print("âš ï¸ æš‚ä¸æ”¯æŒï¼ˆéœ€è¦å…¶ä»–æ¥å£ï¼‰")
                time.sleep(0.1)

            except Exception as e:
                print(f"âŒ å¤±è´¥: {str(e)}")

        print(f"\nâœ… æ¦‚å¿µæˆåˆ†è‚¡ä¸‹è½½å®Œæˆ")

        return concept_members

    def download_index_members(
        self,
        index_codes: List[str] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®

        Args:
            index_codes: æŒ‡æ•°ä»£ç åˆ—è¡¨

        Returns:
            æŒ‡æ•°æˆåˆ†è‚¡å­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")
        print("="*80)

        if index_codes is None:
            # é»˜è®¤ä¸»è¦æŒ‡æ•°
            index_codes = [
                "000001.SH",  # ä¸Šè¯ç»¼æŒ‡
                "399001.SZ",  # æ·±è¯æˆæŒ‡
                "000300.SH",  # æ²ªæ·±300
                "000905.SH",  # ä¸­è¯500
                "000016.SH",  # ä¸Šè¯50
                "399006.SZ",  # åˆ›ä¸šæ¿æŒ‡
                "000688.SH",  # ç§‘åˆ›50
                "000903.SH",  # ä¸­è¯100
                "000852.SH",  # ä¸­è¯1000
            ]

        print(f"\nğŸ“Š å…± {len(index_codes)} ä¸ªæŒ‡æ•°")

        index_members = {}

        for i, index_code in enumerate(index_codes, 1):
            print(f"   [{i}/{len(index_codes)}] {index_code}...", end=" ")

            try:
                # ä½¿ç”¨åŸç”ŸTuShare APIï¼ˆæ›´ç¨³å®šï¼‰
                import tushare as ts
                ts_client = ts.pro_api(self.token)
                df = ts_client.index_member(index_code=index_code)

                if df is not None and not df.empty:
                    index_members[index_code] = df
                    print(f"âœ… {len(df)} åªæˆåˆ†è‚¡")
                else:
                    print("âš ï¸ æ— æ•°æ®")

                time.sleep(0.2)

            except Exception as e:
                print(f"âŒ å¤±è´¥: {str(e)}")

        print(f"\nâœ… æŒ‡æ•°æˆåˆ†è‚¡ä¸‹è½½å®Œæˆï¼Œå…± {len(index_members)} ä¸ªæŒ‡æ•°")

        return index_members

    def download_index_classify(
        self,
        index_codes: List[str] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        ä¸‹è½½æŒ‡æ•°è¡Œä¸šåˆ†ç±»æ•°æ®

        Args:
            index_codes: æŒ‡æ•°ä»£ç åˆ—è¡¨

        Returns:
            æŒ‡æ•°è¡Œä¸šåˆ†ç±»å­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æŒ‡æ•°è¡Œä¸šåˆ†ç±»æ•°æ®")
        print("="*80)

        if index_codes is None:
            index_codes = ["000300.SH", "000905.SH", "000016.SH"]

        print(f"\nğŸ“Š å…± {len(index_codes)} ä¸ªæŒ‡æ•°")

        index_classify = {}
        sources = ["SW2021"]
        levels = ["L1", "L2"]

        for source in sources:
            for level in levels:
                key = f"{source}_{level}"
                print(f"\n   - {source} {level}...", end=" ")

                try:
                    # ä½¿ç”¨åŸç”ŸTuShare APIï¼ˆæ›´ç¨³å®šï¼‰
                    import tushare as ts
                    ts_client = ts.pro_api(self.token)
                    df = ts_client.index_classify(
                        level=level,
                        source=source
                    )

                    if df is not None and not df.empty:
                        index_classify[key] = df
                        print(f"âœ… {len(df)} æ¡è®°å½•")
                    else:
                        print("âš ï¸ æ— æ•°æ®")

                    time.sleep(0.2)

                except Exception as e:
                    print(f"âŒ å¤±è´¥: {str(e)}")

        print(f"\nâœ… æŒ‡æ•°è¡Œä¸šåˆ†ç±»ä¸‹è½½å®Œæˆ")

        return index_classify

    def save_data(
        self,
        industry_data: Dict[str, pd.DataFrame],
        concept_data: pd.DataFrame = None,
        index_members: Dict[str, pd.DataFrame] = None,
        index_classify: Dict[str, pd.DataFrame] = None
    ):
        """
        ä¿å­˜ä¸‹è½½çš„æ•°æ®

        Args:
            industry_data: è¡Œä¸šåˆ†ç±»æ•°æ®
            concept_data: æ¦‚å¿µæ¿å—æ•°æ®
            index_members: æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
            index_classify: æŒ‡æ•°è¡Œä¸šåˆ†ç±»æ•°æ®
        """
        print("\n" + "="*80)
        print("ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®")
        print("="*80)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜è¡Œä¸šåˆ†ç±»æ•°æ®
        if industry_data:
            print(f"\nğŸ“ ä¿å­˜è¡Œä¸šåˆ†ç±»æ•°æ®...")

            # ä¿å­˜å•ä¸ªæ–‡ä»¶
            all_industry = pd.concat(industry_data.values(), ignore_index=True)
            industry_file = self.output_dir / f"industry_all_{timestamp}.csv"
            all_industry.to_csv(industry_file, index=False, encoding='utf-8-sig')
            print(f"   âœ… {industry_file.name}")

            # ä¿å­˜åˆ†ç±»åˆ°ä¸åŒæ–‡ä»¶
            for key, df in industry_data.items():
                file = self.output_dir / f"industry_{key}_{timestamp}.csv"
                df.to_csv(file, index=False, encoding='utf-8-sig')
                print(f"   âœ… {file.name} ({len(df)} æ¡)")

            # ä¿å­˜ä¸ºJSON
            json_file = self.output_dir / f"industry_all_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(
                    {k: v.to_dict('records') for k, v in industry_data.items()},
                    f,
                    ensure_ascii=False,
                    indent=2
                )
            print(f"   âœ… {json_file.name}")

        # ä¿å­˜æ¦‚å¿µæ¿å—æ•°æ®
        if concept_data is not None and not concept_data.empty:
            print(f"\nğŸ“ ä¿å­˜æ¦‚å¿µæ¿å—æ•°æ®...")
            concept_file = self.output_dir / f"concept_{timestamp}.csv"
            concept_data.to_csv(concept_file, index=False, encoding='utf-8-sig')
            print(f"   âœ… {concept_file.name} ({len(concept_data)} æ¡)")

            # ä¿å­˜ä¸ºJSON
            json_file = self.output_dir / f"concept_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(concept_data.to_dict('records'), f, ensure_ascii=False, indent=2)
            print(f"   âœ… {json_file.name}")

        # ä¿å­˜æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®
        if index_members:
            print(f"\nğŸ“ ä¿å­˜æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®...")

            # åˆå¹¶æ‰€æœ‰æŒ‡æ•°
            all_members = []
            for index_code, df in index_members.items():
                df_copy = df.copy()
                df_copy['index_code'] = index_code
                all_members.append(df_copy)

            if all_members:
                all_members_df = pd.concat(all_members, ignore_index=True)
                members_file = self.output_dir / f"index_members_all_{timestamp}.csv"
                all_members_df.to_csv(members_file, index=False, encoding='utf-8-sig')
                print(f"   âœ… {members_file.name} ({len(all_members_df)} æ¡)")

                # ä¿å­˜ä¸ºJSON
                json_file = self.output_dir / f"index_members_all_{timestamp}.json"
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(all_members_df.to_dict('records'), f, ensure_ascii=False, indent=2)
                print(f"   âœ… {json_file.name}")

            # å•ç‹¬ä¿å­˜æ¯ä¸ªæŒ‡æ•°
            for index_code, df in index_members.items():
                code_safe = index_code.replace('.', '_')
                file = self.output_dir / f"index_members_{code_safe}_{timestamp}.csv"
                df.to_csv(file, index=False, encoding='utf-8-sig')
                print(f"   âœ… {file.name} ({len(df)} æ¡)")

        # ä¿å­˜æŒ‡æ•°è¡Œä¸šåˆ†ç±»
        if index_classify:
            print(f"\nğŸ“ ä¿å­˜æŒ‡æ•°è¡Œä¸šåˆ†ç±»æ•°æ®...")

            for key, df in index_classify.items():
                file = self.output_dir / f"index_classify_{key}_{timestamp}.csv"
                df.to_csv(file, index=False, encoding='utf-8-sig')
                print(f"   âœ… {file.name} ({len(df)} æ¡)")

        # åˆ›å»ºæœ€æ–°ç‰ˆæœ¬é“¾æ¥ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰
        print(f"\nğŸ“ åˆ›å»ºæœ€æ–°ç‰ˆæœ¬é“¾æ¥...")
        self._create_latest_links()

        print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}")

    def _create_latest_links(self):
        """åˆ›å»ºæŒ‡å‘æœ€æ–°æ•°æ®çš„é“¾æ¥ï¼ˆå¤åˆ¶æ–‡ä»¶ï¼‰"""
        # æ‰¾åˆ°æœ€æ–°çš„æ–‡ä»¶
        csv_files = list(self.output_dir.glob("*.csv"))
        json_files = list(self.output_dir.glob("*.json"))

        # å¤åˆ¶æœ€æ–°æ–‡ä»¶ä¸ºä¸å«æ—¶é—´æˆ³çš„ç‰ˆæœ¬
        for file in csv_files:
            if '_latest' not in file.name:
                latest_name = file.stem.split('_')[0] + '_' + '_latest.csv'
                latest_file = self.output_dir / latest_name
                import shutil
                shutil.copy2(file, latest_file)

        for file in json_files:
            if '_latest' not in file.name:
                latest_name = file.stem.split('_')[0] + '_' + '_latest.json'
                latest_file = self.output_dir / latest_name
                import shutil
                shutil.copy2(file, latest_file)

    def generate_summary_report(self) -> str:
        """
        ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š

        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆæ•°æ®æ‘˜è¦æŠ¥å‘Š")
        print("="*80)

        report_lines = []
        report_lines.append("# è¡Œä¸šæ¿å—æ•°æ®æ‘˜è¦æŠ¥å‘Š")
        report_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"\næ•°æ®ç›®å½•: {self.output_dir}")

        # ç»Ÿè®¡æ–‡ä»¶
        csv_files = list(self.output_dir.glob("*.csv"))
        json_files = list(self.output_dir.glob("*.json"))

        report_lines.append(f"\n## æ–‡ä»¶ç»Ÿè®¡")
        report_lines.append(f"- CSVæ–‡ä»¶: {len(csv_files)}")
        report_lines.append(f"- JSONæ–‡ä»¶: {len(json_files)}")

        # è¡Œä¸šåˆ†ç±»ç»Ÿè®¡
        industry_files = [f for f in csv_files if f.name.startswith('industry_') and 'all' not in f.name]
        if industry_files:
            report_lines.append(f"\n## è¡Œä¸šåˆ†ç±»æ•°æ® ({len(industry_files)} ä¸ªæ–‡ä»¶)")
            for file in sorted(industry_files):
                df = pd.read_csv(file)
                parts = file.stem.replace('industry_', '').split('_')
                source, level = parts[0], parts[1] if len(parts) > 1 else 'N/A'
                report_lines.append(f"- {source} {level}: {len(df)} æ¡")

        # æ¦‚å¿µæ¿å—ç»Ÿè®¡
        concept_files = [f for f in csv_files if f.name.startswith('concept_')]
        if concept_files:
            report_lines.append(f"\n## æ¦‚å¿µæ¿å—æ•°æ®")
            for file in concept_files:
                df = pd.read_csv(file)
                report_lines.append(f"- æ¦‚å¿µæ¿å—: {len(df)} ä¸ª")

        # æŒ‡æ•°æˆåˆ†è‚¡ç»Ÿè®¡
        member_files = [f for f in csv_files if 'index_members_' in f.name and 'all' in f.name]
        if member_files:
            report_lines.append(f"\n## æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®")
            for file in member_files:
                df = pd.read_csv(file)
                if 'index_code' in df.columns:
                    index_counts = df['index_code'].value_counts()
                    report_lines.append(f"- æ€»æˆåˆ†è‚¡: {len(df)} æ¡")
                    report_lines.append(f"- è¦†ç›–æŒ‡æ•°: {len(index_counts)} ä¸ª")
                    for idx_code, count in index_counts.head(10).items():
                        report_lines.append(f"  - {idx_code}: {count} åª")

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "data_summary_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file.name}")

        return '\n'.join(report_lines)

    def run_full_download(self):
        """æ‰§è¡Œå®Œæ•´ä¸‹è½½æµç¨‹"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹å®Œæ•´ä¸‹è½½æµç¨‹")
        print("="*80)

        # 1. ä¸‹è½½è¡Œä¸šåˆ†ç±»
        industry_data = self.download_industry_classification()

        # 2. ä¸‹è½½æ¦‚å¿µæ¿å—
        concept_data = self.download_concept()

        # 3. ä¸‹è½½æŒ‡æ•°æˆåˆ†è‚¡
        index_members = self.download_index_members()

        # 4. ä¸‹è½½æŒ‡æ•°è¡Œä¸šåˆ†ç±»
        index_classify = self.download_index_classify()

        # 5. ä¿å­˜æ‰€æœ‰æ•°æ®
        self.save_data(
            industry_data=industry_data,
            concept_data=concept_data,
            index_members=index_members,
            index_classify=index_classify
        )

        # 6. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        report = self.generate_summary_report()

        print("\n" + "="*80)
        print("âœ… å®Œæ•´ä¸‹è½½æµç¨‹å®Œæˆï¼")
        print("="*80)

        return {
            'industry_data': industry_data,
            'concept_data': concept_data,
            'index_members': index_members,
            'index_classify': index_classify
        }


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥Token
    token = os.getenv("TUSHARE_TOKEN")
    if not token:
        print("âŒ è¯·è®¾ç½® TUSHARE_TOKEN ç¯å¢ƒå˜é‡")
        print("   export TUSHARE_TOKEN='your_token_here'")
        sys.exit(1)

    print("ğŸ¯ TuShare è¡Œä¸šæ¿å—æ•°æ®ä¸‹è½½å·¥å…·")
    print("="*80)

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = IndustryDataDownloader(token=token)

    # æ‰§è¡Œä¸‹è½½
    try:
        result = downloader.run_full_download()

        print("\nğŸ“Š ä¸‹è½½æ‘˜è¦:")
        print(f"  - è¡Œä¸šåˆ†ç±»: {len(result['industry_data'])} ä¸ª")
        print(f"  - æ¦‚å¿µæ¿å—: {len(result['concept_data']) if result['concept_data'] is not None else 0} ä¸ª")
        print(f"  - æŒ‡æ•°æˆåˆ†è‚¡: {len(result['index_members'])} ä¸ªæŒ‡æ•°")
        print(f"  - æŒ‡æ•°è¡Œä¸šåˆ†ç±»: {len(result['index_classify'])} ä¸ª")

        print("\nâœ… æ‰€æœ‰æ•°æ®ä¸‹è½½å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
