#!/usr/bin/env python3
"""
ä¸‹è½½å…¨éƒ¨Aè‚¡è´¢åŠ¡æ•°æ®ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä¸‹è½½å…¨éƒ¨Aè‚¡è´¢åŠ¡æ•°æ®
2. æ”¯æŒæ–­ç‚¹ç»­ä¼ 
3. æ‰¹é‡ä¿å­˜ï¼Œé¿å…æ•°æ®ä¸¢å¤±
4. å®æ—¶è¿›åº¦æ˜¾ç¤º
"""

import os
import sys
import time
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List

def download_all_a_share_financial():
    """ä¸‹è½½å…¨éƒ¨Aè‚¡è´¢åŠ¡æ•°æ®"""

    print("\nğŸ¯ ä¸‹è½½å…¨éƒ¨Aè‚¡è´¢åŠ¡æ•°æ®")
    print("="*80)

    # æ£€æŸ¥Token
    token = os.getenv("TUSHARE_TOKEN")
    if not token:
        print("âŒ è¯·è®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡")
        sys.exit(1)

    # è¾“å‡ºç›®å½•
    output_dir = Path.home() / ".qlib/qlib_data/cn_data/financial_data"
    output_dir.mkdir(parents=True, exist_ok=True)

    # çŠ¶æ€æ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
    state_file = output_dir / "download_state.json"

    # åˆå§‹åŒ–TuShare
    import tushare as ts
    pro = ts.pro_api(token)
    print(f"âœ… TuShareåˆå§‹åŒ–æˆåŠŸ")

    # è·å–å…¨éƒ¨Aè‚¡åˆ—è¡¨
    print("\nğŸ“Š è·å–Aè‚¡åˆ—è¡¨...")
    stock_list = pro.stock_basic(
        exchange='',
        list_status='L',
        fields='ts_code,name'
    ).sort_values('ts_code')

    total_stocks = len(stock_list)
    print(f"âœ… Aè‚¡æ€»æ•°: {total_stocks} åª")

    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¸‹è½½
    downloaded_codes = set()
    start_index = 0

    if state_file.exists():
        with open(state_file, 'r') as f:
            state = json.load(f)
            downloaded_codes = set(state.get('downloaded_codes', []))
            start_index = state.get('current_index', 0)

        print(f"ğŸ“¥ æ£€æµ‹åˆ°æœªå®Œæˆä¸‹è½½: å·²ä¸‹è½½ {len(downloaded_codes)} åª")
        print(f"   å°†ä»ç¬¬ {start_index + 1} åªè‚¡ç¥¨ç»§ç»­")
    else:
        print("ğŸ“¥ å¼€å§‹å…¨æ–°ä¸‹è½½")

    # é…ç½®
    BATCH_SIZE = 50  # æ¯æ‰¹50åª
    DELAY = 0.3      # è¯·æ±‚é—´éš”300ms
    START_DATE = "20230101"  # ä»2023å¹´å¼€å§‹
    END_DATE = "20241231"

    print(f"\nâš™ï¸  ä¸‹è½½é…ç½®:")
    print(f"   æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"   è¯·æ±‚é—´éš”: {DELAY}ç§’")
    print(f"   æ—¶é—´èŒƒå›´: {START_DATE} - {END_DATE}")
    print(f"   é¢„è®¡è€—æ—¶: {total_stocks * DELAY / 60:.1f} åˆ†é’Ÿ")
    print("="*80)

    # å¼€å§‹ä¸‹è½½
    all_data = []
    batch_data = []
    batch_num = start_index // BATCH_SIZE + 1
    success_count = len(downloaded_codes)

    for idx in range(start_index, total_stocks):
        row = stock_list.iloc[idx]
        ts_code = row['ts_code']
        name = row['name']

        # è·³è¿‡å·²ä¸‹è½½
        if ts_code in downloaded_codes:
            continue

        # è¿›åº¦æ˜¾ç¤º
        progress = (idx + 1) / total_stocks * 100
        print(f"[{idx+1}/{total_stocks}] ({progress:.1f}%) {ts_code} {name}...", end=" ", flush=True)

        try:
            # ä¸‹è½½è´¢åŠ¡æ•°æ®
            df = pro.fina_indicator(
                ts_code=ts_code,
                start_date=START_DATE,
                end_date=END_DATE
            )

            if df is not None and not df.empty:
                batch_data.append(df)
                success_count += 1
                downloaded_codes.add(ts_code)
                print(f"âœ… {len(df)} æ¡")
            else:
                print("âš ï¸ æ— æ•°æ®")

        except Exception as e:
            print(f"âŒ {str(e)[:40]}")

        # æ¯æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡
        if (idx + 1) % BATCH_SIZE == 0 or (idx + 1) == total_stocks:
            # ä¿å­˜æ‰¹æ¬¡æ•°æ®
            if batch_data:
                batch_df = pd.concat(batch_data, ignore_index=True)

                batch_file = output_dir / f"batch_{batch_num:04d}.csv"
                batch_df.to_csv(batch_file, index=False, encoding='utf-8-sig')
                print(f"  ğŸ’¾ æ‰¹æ¬¡ {batch_num} å·²ä¿å­˜ ({len(batch_df)} æ¡)")

                all_data.append(batch_df)
                batch_data = []

                # æ›´æ–°çŠ¶æ€
                state = {
                    'current_index': idx + 1,
                    'downloaded_codes': list(downloaded_codes),
                    'last_update': datetime.now().isoformat()
                }
                with open(state_file, 'w') as f:
                    json.dump(state, f)

                batch_num += 1

        # APIé™æµ
        time.sleep(DELAY)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print("\n" + "="*80)
    print("ğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...")

    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)

        # å»é‡
        combined_df = combined_df.drop_duplicates(
            subset=['ts_code', 'end_date'],
            keep='last'
        )

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å®Œæ•´æ•°æ®
        final_file = output_dir / f"a_share_financial_{timestamp}.csv"
        combined_df.to_csv(final_file, index=False, encoding='utf-8-sig')
        print(f"âœ… å®Œæ•´æ•°æ®: {final_file}")

        # ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        latest_file = output_dir / "a_share_financial_latest.csv"
        combined_df.to_csv(latest_file, index=False, encoding='utf-8-sig')
        print(f"âœ… æœ€æ–°ç‰ˆæœ¬: {latest_file}")

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡:")
        print(f"   æˆåŠŸ: {success_count}/{total_stocks} åªè‚¡ç¥¨")
        print(f"   æ€»è®°å½•: {len(combined_df)} æ¡")
        print(f"   å­—æ®µæ•°: {len(combined_df.columns)} ä¸ª")
        print(f"   è‚¡ç¥¨æ•°: {combined_df['ts_code'].nunique()} åª")
        print(f"   æ—¶é—´èŒƒå›´: {combined_df['end_date'].min()} - {combined_df['end_date'].max()}")

        # åˆ é™¤çŠ¶æ€æ–‡ä»¶
        if state_file.exists():
            state_file.unlink()
            print(f"   å·²æ¸…ç†çŠ¶æ€æ–‡ä»¶")

    else:
        print("âš ï¸ æ²¡æœ‰ä¸‹è½½åˆ°æ–°æ•°æ®")

    print("\nâœ… å…¨éƒ¨å®Œæˆ!")

if __name__ == "__main__":
    try:
        download_all_a_share_financial()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        print("ğŸ’¡ çŠ¶æ€å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
