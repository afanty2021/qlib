#!/usr/bin/env python3
"""
ä¸‹è½½2000-2025å¹´å…¨éƒ¨Aè‚¡è´¢åŠ¡æ•°æ®

åˆ†æ‰¹ä¸‹è½½ç­–ç•¥ï¼š
- æ‰¹æ¬¡1: 2000-2004å¹´ (20ä¸ªå­£åº¦)
- æ‰¹æ¬¡2: 2005-2009å¹´ (20ä¸ªå­£åº¦)
- æ‰¹æ¬¡3: 2010-2014å¹´ (20ä¸ªå­£åº¦)
- æ‰¹æ¬¡4: 2015-2019å¹´ (20ä¸ªå­£åº¦)
- æ‰¹æ¬¡5: 2020-2022å¹´ (12ä¸ªå­£åº¦ï¼Œå·²æœ‰2023-2024)
- æ‰¹æ¬¡6: 2025å¹´ (å·²å…¬å‘Šæ•°æ®)
"""

import os
import sys
import time
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def download_extended_financial_data():
    """ä¸‹è½½æ‰©å±•è´¢åŠ¡æ•°æ® (2000-2025)"""

    print("\nğŸš€ ä¸‹è½½æ‰©å±•è´¢åŠ¡æ•°æ® (2000-2025)")
    print("="*80)

    # æ£€æŸ¥Token
    token = os.getenv("TUSHARE_TOKEN")
    if not token:
        print("âŒ è¯·è®¾ç½®TUSHARE_TOKENç¯å¢ƒå˜é‡")
        sys.exit(1)

    # è¾“å‡ºç›®å½•
    output_dir = Path.home() / ".qlib/qlib_data/cn_data/financial_data"
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ–TuShare
    import tushare as ts
    pro = ts.pro_api(token)
    print(f"âœ… TuShareåˆå§‹åŒ–æˆåŠŸ")

    # å®šä¹‰ä¸‹è½½æ‰¹æ¬¡
    download_batches = [
        {
            "name": "2020-2022 (è¡¥å……)",
            "start": "20200101",
            "end": "20221231",
            "priority": 1
        },
        {
            "name": "2015-2019",
            "start": "20150101",
            "end": "20191231",
            "priority": 2
        },
        {
            "name": "2010-2014",
            "start": "20100101",
            "end": "20141231",
            "priority": 3
        },
        {
            "name": "2005-2009",
            "start": "20050101",
            "end": "20091231",
            "priority": 4
        },
        {
            "name": "2000-2004",
            "start": "20000101",
            "end": "20041231",
            "priority": 5
        },
        {
            "name": "2025 (å·²å…¬å‘Š)",
            "start": "20250101",
            "end": "20251231",
            "priority": 6
        }
    ]

    print(f"\nğŸ“‹ ä¸‹è½½è®¡åˆ’: {len(download_batches)} ä¸ªæ‰¹æ¬¡")
    for i, batch in enumerate(download_batches, 1):
        print(f"   {i}. {batch['name']}: {batch['start']} - {batch['end']}")

    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\nğŸ“Š è·å–Aè‚¡åˆ—è¡¨...")
    stock_list = pro.stock_basic(
        exchange='',
        list_status='L',
        fields='ts_code,name,list_date'
    ).sort_values('ts_code')

    total_stocks = len(stock_list)
    print(f"âœ… Aè‚¡æ€»æ•°: {total_stocks} åª")

    # æ£€æŸ¥ç°æœ‰æ•°æ®
    existing_file = output_dir / "a_share_financial_latest.csv"
    existing_data = None
    existing_periods = set()

    if existing_file.exists():
        existing_data = pd.read_csv(existing_file)
        existing_data['end_date'] = existing_data['end_date'].astype(int)
        existing_periods = set(existing_data['end_date'].unique())
        print(f"âœ… å·²æœ‰æ•°æ®: {len(existing_data)} æ¡")
        print(f"   å·²è¦†ç›–æœŸæ•°: {len(existing_periods)} ä¸ª")
        print(f"   æ—¶é—´èŒƒå›´: {min(existing_periods)} - {max(existing_periods)}")

    # é…ç½®
    BATCH_SIZE = 100  # æ¯æ‰¹100åªè‚¡ç¥¨
    DELAY = 0.3       # 300msé—´éš”

    print(f"\nâš™ï¸  ä¸‹è½½é…ç½®:")
    print(f"   æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"   è¯·æ±‚é—´éš”: {DELAY}ç§’")

    # å¼€å§‹åˆ†æ‰¹ä¸‹è½½
    for batch_info in download_batches:
        batch_name = batch_info['name']
        start_date = batch_info['start']
        end_date = batch_info['end']

        print("\n" + "="*80)
        print(f"ğŸ“¥ æ‰¹æ¬¡: {batch_name}")
        print(f"   æ—¶é—´èŒƒå›´: {start_date} - {end_date}")
        print("="*80)

        # åˆ›å»ºæ‰¹æ¬¡çŠ¶æ€æ–‡ä»¶
        state_file = output_dir / f"state_{batch_name.replace(' ', '_').replace('-', '_')}.json"

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if state_file.exists():
            with open(state_file, 'r') as f:
                state = json.load(f)
            if state.get('completed', False):
                print(f"âœ… {batch_name} å·²å®Œæˆï¼Œè·³è¿‡")
                continue

        # åˆå§‹åŒ–çŠ¶æ€
        downloaded_codes = set()
        if state_file.exists():
            with open(state_file, 'r') as f:
                state = json.load(f)
                downloaded_codes = set(state.get('downloaded_codes', []))

        all_batch_data = []
        success_count = len(downloaded_codes)

        # ä¸‹è½½æœ¬æ‰¹æ¬¡æ•°æ®
        for idx in range(total_stocks):
            row = stock_list.iloc[idx]
            ts_code = row['ts_code']
            name = row['name']
            list_date = row['list_date']

            # è·³è¿‡å·²ä¸‹è½½
            if ts_code in downloaded_codes:
                continue

            # è·³è¿‡æœªä¸Šå¸‚çš„è‚¡ç¥¨
            if list_date and int(list_date) > int(end_date[:4]):
                continue

            # è¿›åº¦æ˜¾ç¤º
            progress = (idx + 1) / total_stocks * 100
            print(f"[{idx+1}/{total_stocks}] ({progress:.1f}%) {ts_code} {name}...", end=" ", flush=True)

            try:
                # ä¸‹è½½è´¢åŠ¡æ•°æ®
                df = pro.fina_indicator(
                    ts_code=ts_code,
                    start_date=start_date,
                    end_date=end_date
                )

                if df is not None and not df.empty:
                    all_batch_data.append(df)
                    success_count += 1
                    downloaded_codes.add(ts_code)
                    print(f"âœ… {len(df)} æ¡")
                else:
                    print("âš ï¸ æ— æ•°æ®")

            except Exception as e:
                error_msg = str(e)[:50]
                print(f"âŒ {error_msg}")

            # æ¯BATCH_SIZEåªä¿å­˜ä¸€æ¬¡
            if (idx + 1) % BATCH_SIZE == 0 or (idx + 1) == total_stocks:
                # ä¿å­˜æ‰¹æ¬¡æ•°æ®
                if all_batch_data:
                    batch_df = pd.concat(all_batch_data, ignore_index=True)

                    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    temp_file = output_dir / f"temp_{batch_name.replace(' ', '_').replace('-', '_')}.csv"
                    batch_df.to_csv(temp_file, index=False, encoding='utf-8-sig')
                    print(f"  ğŸ’¾ ä¸´æ—¶ä¿å­˜: {len(batch_df)} æ¡")

                    all_batch_data = []

                # æ›´æ–°çŠ¶æ€
                state = {
                    'current_index': idx + 1,
                    'downloaded_codes': list(downloaded_codes),
                    'last_update': datetime.now().isoformat(),
                    'completed': False
                }
                with open(state_file, 'w') as f:
                    json.dump(state, f)

            # APIé™æµ
            time.sleep(DELAY)

        # ä¿å­˜æœ¬æ‰¹æ¬¡æœ€ç»ˆæ•°æ®
        if all_batch_data:
            final_batch_df = pd.concat(all_batch_data, ignore_index=True)

            # ä¿å­˜
            batch_file = output_dir / f"{batch_name.replace(' ', '_').replace('-', '_')}.csv"
            final_batch_df.to_csv(batch_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… {batch_name} å®Œæˆ: {len(final_batch_df)} æ¡")

            # æ ‡è®°å®Œæˆ
            state['completed'] = True
            with open(state_file, 'w') as f:
                json.dump(state, f)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print("\n" + "="*80)
    print("ğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...")
    print("="*80)

    # è¯»å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    all_data = []

    # è¯»å–ç°æœ‰æ•°æ®
    if existing_data is not None:
        all_data.append(existing_data)
        print(f"âœ… ç°æœ‰æ•°æ®: {len(existing_data)} æ¡")

    # è¯»å–æ–°æ‰¹æ¬¡æ•°æ®
    for batch_info in download_batches:
        batch_file = output_dir / f"{batch_info['name'].replace(' ', '_').replace('-', '_')}.csv"
        if batch_file.exists():
            batch_df = pd.read_csv(batch_file)
            all_data.append(batch_df)
            print(f"âœ… {batch_info['name']}: {len(batch_df)} æ¡")

    # è¯»å–ä¸´æ—¶æ–‡ä»¶
    temp_files = list(output_dir.glob("temp_*.csv"))
    for temp_file in temp_files:
        temp_df = pd.read_csv(temp_file)
        all_data.append(temp_df)
        print(f"âœ… ä¸´æ—¶æ•°æ®: {len(temp_df)} æ¡")
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()

    if all_data:
        # åˆå¹¶
        combined_df = pd.concat(all_data, ignore_index=True)

        # å»é‡ï¼ˆä¿ç•™æœ€æ–°ï¼‰
        combined_df = combined_df.sort_values(['ts_code', 'end_date', 'ann_date'])
        combined_df = combined_df.drop_duplicates(
            subset=['ts_code', 'end_date'],
            keep='last'
        )

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å®Œæ•´æ•°æ®
        final_file = output_dir / f"a_share_financial_full_{timestamp}.csv"
        combined_df.to_csv(final_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… å®Œæ•´æ•°æ®: {final_file}")

        # ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        latest_file = output_dir / "a_share_financial_latest.csv"
        combined_df.to_csv(latest_file, index=False, encoding='utf-8-sig')
        print(f"âœ… æœ€æ–°ç‰ˆæœ¬: {latest_file}")

        # ç»Ÿè®¡
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»è®°å½•: {len(combined_df):,} æ¡")
        print(f"   è‚¡ç¥¨æ•°: {combined_df['ts_code'].nunique()} åª")
        print(f"   å­—æ®µæ•°: {len(combined_df.columns)} ä¸ª")
        print(f"   æŠ¥å‘ŠæœŸ: {combined_df['end_date'].nunique()} ä¸ª")

        # æ—¶é—´èŒƒå›´
        combined_df['year'] = combined_df['end_date'].astype(str).str[:4]
        years = sorted(combined_df['year'].unique())
        print(f"   å¹´ä»½èŒƒå›´: {years[0]} - {years[-1]} ({len(years)} å¹´)")

    print("\nâœ… å…¨éƒ¨å®Œæˆ!")


if __name__ == "__main__":
    try:
        download_extended_financial_data()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        print("ğŸ’¡ çŠ¶æ€å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
