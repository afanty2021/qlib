#!/usr/bin/env python3
"""
2000-2025è´¢åŠ¡æ•°æ®éªŒè¯å’Œåˆå¹¶å·¥å…·

ä¸‹è½½å®Œæˆåçš„æ•°æ®å¤„ç†ï¼š
1. éªŒè¯æ‰€æœ‰é˜¶æ®µæ•°æ®å®Œæ•´æ€§
2. åˆå¹¶æ‰€æœ‰é˜¶æ®µæ•°æ®
3. å»é‡å’Œæ•°æ®æ¸…æ´—
4. ç”Ÿæˆæœ€ç»ˆæ•°æ®é›†
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

class FinancialDataValidator:
    """è´¢åŠ¡æ•°æ®éªŒè¯å™¨"""

    def __init__(self, data_dir: Path):
        self.data_dir = data_dir

    def check_phase_completeness(self) -> Dict[str, pd.DataFrame]:
        """æ£€æŸ¥æ‰€æœ‰é˜¶æ®µæ–‡ä»¶å®Œæ•´æ€§"""
        print("\nğŸ“‹ æ£€æŸ¥é˜¶æ®µæ–‡ä»¶...")
        print("="*80)

        phase_files = sorted(self.data_dir.glob("phase_*.csv"))

        if not phase_files:
            print("âŒ æœªæ‰¾åˆ°é˜¶æ®µæ–‡ä»¶")
            return {}

        phases = {}

        for phase_file in phase_files:
            phase_name = phase_file.stem.replace("phase_", "").replace("_", " ")
            df = pd.read_csv(phase_file)

            # åŸºæœ¬ç»Ÿè®¡
            stats = {
                "records": len(df),
                "stocks": df['ts_code'].nunique(),
                "quarters": df['end_date'].nunique(),
                "fields": len(df.columns),
                "size_mb": phase_file.stat().st_size / 1024 / 1024,
                "file": phase_file
            }

            # å¹´ä»½èŒƒå›´
            df['year'] = df['end_date'].astype(str).str[:4]
            years = sorted(df['year'].unique())
            stats['year_range'] = f"{years[0]}-{years[-1]}" if years else "N/A"

            # å­£åº¦åˆ†å¸ƒ
            quarter_counts = df['end_date'].value_counts().sort_index()
            stats['quarter_distribution'] = quarter_counts

            phases[phase_name] = df

            print(f"\nâœ… {phase_name}")
            print(f"   è®°å½•æ•°: {stats['records']:,} æ¡")
            print(f"   è‚¡ç¥¨æ•°: {stats['stocks']} åª")
            print(f"   å­£åº¦æ•°: {stats['quarters']} ä¸ª")
            print(f"   å¹´ä»½èŒƒå›´: {stats['year_range']}")
            print(f"   å­—æ®µæ•°: {stats['fields']} ä¸ª")
            print(f"   æ–‡ä»¶å¤§å°: {stats['size_mb']:.2f} MB")

        return phases

    def validate_data_quality(self, df: pd.DataFrame) -> Dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
        print("="*80)

        report = {}

        # 1. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_stats = df.isnull().sum()
        missing_pct = (missing_stats / len(df) * 100).round(2)

        high_missing = missing_pct[missing_pct > 50].sort_values(ascending=False)

        report['missing_fields'] = len(high_missing)
        report['total_missing'] = df.isnull().sum().sum()

        print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(f"   æ€»ç¼ºå¤±å€¼: {report['total_missing']:,} ä¸ª")
        print(f"   é«˜ç¼ºå¤±å­—æ®µ(>50%): {report['missing_fields']} ä¸ª")

        if len(high_missing) > 0:
            print(f"\n   é«˜ç¼ºå¤±å­—æ®µ Top 10:")
            for field, pct in high_missing.head(10).items():
                print(f"     - {field}: {pct}%")

        # 2. é‡å¤å€¼æ£€æŸ¥
        duplicates = df.duplicated(subset=['ts_code', 'end_date']).sum()
        report['duplicates'] = duplicates

        print(f"\né‡å¤è®°å½•:")
        print(f"   åŸºäº(ts_code, end_date)çš„é‡å¤: {duplicates} æ¡")

        # 3. æ•°æ®èŒƒå›´æ£€æŸ¥
        df['year'] = df['end_date'].astype(str).str[:4]
        year_range = (df['year'].min(), df['year'].max())
        report['year_range'] = year_range

        print(f"\nå¹´ä»½èŒƒå›´:")
        print(f"   {year_range[0]} - {year_range[1]} ({df['year'].nunique()} å¹´)")

        # 4. è‚¡ç¥¨è¦†ç›–ç‡
        total_stocks = df['ts_code'].nunique()
        report['total_stocks'] = total_stocks

        print(f"\nè‚¡ç¥¨è¦†ç›–:")
        print(f"   æ€»è‚¡ç¥¨æ•°: {total_stocks} åª")

        # 5. å­£åº¦è¦†ç›–ç‡
        quarters = df['end_date'].nunique()
        report['total_quarters'] = quarters

        print(f"\nå­£åº¦è¦†ç›–:")
        print(f"   æ€»å­£åº¦æ•°: {quarters} ä¸ª")

        # 6. å…³é”®å­—æ®µæ£€æŸ¥
        key_fields = ['roe', 'roa', 'or_yoy', 'pe', 'pb']
        key_coverage = {}
        for field in key_fields:
            if field in df.columns:
                valid_count = df[field].notna().sum()
                coverage = (valid_count / len(df) * 100)
                key_coverage[field] = coverage
                print(f"   {field}: {coverage:.1f}% è¦†ç›–ç‡")

        report['key_coverage'] = key_coverage

        return report

    def merge_all_phases(self, phases: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰é˜¶æ®µæ•°æ®"""
        print("\nğŸ”„ åˆå¹¶æ‰€æœ‰é˜¶æ®µæ•°æ®")
        print("="*80)

        if not phases:
            print("âŒ æ²¡æœ‰é˜¶æ®µæ•°æ®å¯åˆå¹¶")
            return pd.DataFrame()

        # åˆå¹¶
        print(f"\nåˆå¹¶ {len(phases)} ä¸ªé˜¶æ®µ...")
        combined_df = pd.concat(phases.values(), ignore_index=True)

        print(f"åˆå¹¶å‰æ€»è®°å½•: {len(combined_df):,} æ¡")

        # æ’åºï¼ˆæŒ‰è‚¡ç¥¨ã€æŠ¥å‘ŠæœŸã€å…¬å‘Šæ—¥æœŸï¼‰
        print("\næ’åºæ•°æ®...")
        combined_df['end_date'] = combined_df['end_date'].astype(int)
        combined_df = combined_df.sort_values(['ts_code', 'end_date', 'ann_date'])

        # å»é‡ï¼ˆä¿ç•™æœ€æ–°å…¬å‘Šï¼‰
        print("å»é‡å¤„ç†...")
        before_dedup = len(combined_df)
        combined_df = combined_df.drop_duplicates(
            subset=['ts_code', 'end_date'],
            keep='last'  # ä¿ç•™æœ€åï¼ˆæœ€æ–°ï¼‰çš„è®°å½•
        )
        dedup_count = before_dedup - len(combined_df)

        print(f"å»é‡åæ€»è®°å½•: {len(combined_df):,} æ¡ (åˆ é™¤ {dedup_count} æ¡é‡å¤)")

        return combined_df

    def generate_summary_report(self, merged_df: pd.DataFrame) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        print("="*80)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç»Ÿè®¡ä¿¡æ¯
        total_records = len(merged_df)
        total_stocks = merged_df['ts_code'].nunique()
        total_fields = len(merged_df.columns)
        total_quarters = merged_df['end_date'].nunique()

        # å¹´ä»½ç»Ÿè®¡
        merged_df['year'] = merged_df['end_date'].astype(str).str[:4]
        year_counts = merged_df['year'].value_counts().sort_index()

        # æŠ¥å‘Šå†…å®¹
        report = f"""
# Aè‚¡è´¢åŠ¡æ•°æ®æ±‡æ€»æŠ¥å‘Š (2000-2025)

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ•°æ®æ¥æº: TuShare Pro

## ğŸ“Š æ•°æ®ç»Ÿè®¡

- **æ€»è®°å½•æ•°**: {total_records:,} æ¡
- **è‚¡ç¥¨æ•°é‡**: {total_stocks} åª
- **æ•°æ®å­—æ®µ**: {total_fields} ä¸ª
- **å­£åº¦æ•°é‡**: {total_quarters} ä¸ª

## ğŸ“… å¹´ä»½åˆ†å¸ƒ

"""

        for year, count in year_counts.items():
            stocks_in_year = merged_df[merged_df['year'] == year]['ts_code'].nunique()
            report += f"- **{year}**: {count:,} æ¡è®°å½• ({stocks_in_year} åªè‚¡ç¥¨)\n"

        # å­—æ®µè¯´æ˜
        report += "\n## ğŸ“‹ ä¸»è¦å­—æ®µ\n\n"

        key_categories = {
            "åŸºæœ¬ä¿¡æ¯": ["ts_code", "ann_date", "end_date"],
            "æ¯è‚¡æŒ‡æ ‡": ["eps", "dt_eps", "bps", "ocfps", "cfps"],
            "ç›ˆåˆ©èƒ½åŠ›": ["roe", "roa", "npta", "roic", "grossprofit_margin", "netprofit_margin"],
            "å¿å€ºèƒ½åŠ›": ["debt_to_assets", "current_ratio", "quick_ratio"],
            "è¿è¥èƒ½åŠ›": ["assets_turnover", "ar_turn", "ca_turn"],
            "æˆé•¿èƒ½åŠ›": ["or_yoy", "op_yoy", "netprofit_yoy"],
            "ä¼°å€¼æŒ‡æ ‡": ["pe", "pb", "ps", "pcf"]
        }

        for category, fields in key_categories.items():
            available_fields = [f for f in fields if f in merged_df.columns]
            if available_fields:
                report += f"**{category}**: {', '.join(available_fields)}\n"

        # æ•°æ®è´¨é‡
        report += "\n## âœ… æ•°æ®è´¨é‡\n\n"

        missing_pct = (merged_df.isnull().sum() / len(merged_df) * 100)
        good_quality_fields = missing_pct[missing_pct < 30]

        report += f"- é«˜è´¨é‡å­—æ®µ(ç¼ºå¤±<30%): {len(good_quality_fields)}/{total_fields} ä¸ª\n"
        report += f"- å¹³å‡è®°å½•è¦†ç›–ç‡: {(1 - merged_df.isnull().sum().sum() / (total_records * total_fields)) * 100:.1f}%\n"

        # æ–‡ä»¶ä¿¡æ¯
        report += f"\n## ğŸ“ æ–‡ä»¶ä¿¡æ¯\n\n"
        report += f"- **å®Œæ•´æ•°æ®æ–‡ä»¶**: `a_share_financial_2000_2025_{timestamp}.csv`\n"
        report += f"- **æœ€æ–°ç‰ˆæœ¬**: `a_share_financial_latest.csv`\n"

        return report

    def save_final_dataset(self, merged_df: pd.DataFrame):
        """ä¿å­˜æœ€ç»ˆæ•°æ®é›†"""
        print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ•°æ®é›†")
        print("="*80)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„å®Œæ•´æ–‡ä»¶
        final_file = self.data_dir / f"a_share_financial_2000_2025_{timestamp}.csv"
        merged_df.to_csv(final_file, index=False, encoding='utf-8-sig')
        file_size_mb = final_file.stat().st_size / 1024 / 1024

        print(f"\nâœ… å®Œæ•´æ•°æ®: {final_file}")
        print(f"   å¤§å°: {file_size_mb:.2f} MB")

        # ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        latest_file = self.data_dir / "a_share_financial_latest.csv"
        merged_df.to_csv(latest_file, index=False, encoding='utf-8-sig')

        print(f"âœ… æœ€æ–°ç‰ˆæœ¬: {latest_file}")

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        report = self.generate_summary_report(merged_df)
        report_file = self.data_dir / "FINANCIAL_DATA_2000_2025_SUMMARY.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"âœ… æ±‡æ€»æŠ¥å‘Š: {report_file}")

        # ç”Ÿæˆæ•°æ®æ ·æœ¬
        print("\nğŸ“Š æ•°æ®æ ·æœ¬:")
        print("-" * 40)

        sample_stocks = ['000001.SZ', '000002.SZ', '600000.SH']
        for stock in sample_stocks:
            if stock in merged_df['ts_code'].values:
                stock_data = merged_df[merged_df['ts_code'] == stock].sort_values('end_date')
                print(f"\n{stock} (æœ€æ–°5ä¸ªå­£åº¦):")
                display_cols = ['end_date', 'roe', 'or_yoy', 'pe', 'pb']
                available_cols = [c for c in display_cols if c in stock_data.columns]
                print(stock_data[available_cols].tail().to_string(index=False))

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ”§ 2000-2025è´¢åŠ¡æ•°æ®éªŒè¯å’Œåˆå¹¶")
    print("="*80)

    # æ•°æ®ç›®å½•
    data_dir = Path.home() / ".qlib/qlib_data/cn_data/financial_data"

    # åˆ›å»ºéªŒè¯å™¨
    validator = FinancialDataValidator(data_dir)

    # 1. æ£€æŸ¥é˜¶æ®µæ–‡ä»¶
    phases = validator.check_phase_completeness()

    if not phases:
        print("\nâŒ æœªæ‰¾åˆ°é˜¶æ®µæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œä¸‹è½½è„šæœ¬")
        return

    # 2. åˆå¹¶æ•°æ®
    merged_df = validator.merge_all_phases(phases)

    if merged_df.empty:
        print("\nâŒ åˆå¹¶å¤±è´¥")
        return

    # 3. éªŒè¯æ•°æ®è´¨é‡
    quality_report = validator.validate_data_quality(merged_df)

    # 4. ä¿å­˜æœ€ç»ˆæ•°æ®é›†
    validator.save_final_dataset(merged_df)

    # 5. å®Œæˆ
    print("\n" + "="*80)
    print("âœ… æ•°æ®éªŒè¯å’Œåˆå¹¶å®Œæˆ!")
    print("="*80)

    print("\nğŸ“ˆ æ•°æ®å¯ç”¨äº:")
    print("   â€¢ é•¿æœŸå›æµ‹ (2000-2025 = 26å¹´å†å²)")
    print("   â€¢ è´¢åŠ¡å› å­åˆ†æ")
    print("   â€¢ ä»·å€¼æŠ•èµ„ç­–ç•¥")
    print("   â€¢ è¡Œä¸šå¯¹æ¯”ç ”ç©¶")

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. ä½¿ç”¨ Qlib é›†æˆè´¢åŠ¡æ•°æ®")
    print("   2. è®¡ç®—è‡ªå®šä¹‰è´¢åŠ¡å› å­")
    print("   3. æ„å»ºå›æµ‹ç­–ç•¥")
    print("   4. åˆ†æå› å­æœ‰æ•ˆæ€§")

if __name__ == "__main__":
    main()
