[æ ¹ç›®å½•](../../../CLAUDE.md) > [qlib](../../CLAUDE.md) > [contrib](../CLAUDE.md) > **model**

# æ¨¡å‹æ‰©å±•æ¨¡å— (model)

> Qlib çš„ä¸°å¯Œæœºå™¨å­¦ä¹ æ¨¡å‹åº“ï¼Œæä¾›ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ çš„å®Œæ•´æ¨¡å‹ç”Ÿæ€ã€‚

## æ¨¡å—èŒè´£

æ¨¡å‹æ‰©å±•æ¨¡å—ä¸ºé‡åŒ–æŠ•èµ„æä¾›ï¼š
- ä¸°å¯Œçš„é¢„ç½®æ¨¡å‹å®ç°ï¼Œå¼€ç®±å³ç”¨
- ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼Œä¾¿äºåˆ‡æ¢å’Œå¯¹æ¯”
- é«˜æ€§èƒ½çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†
- çµæ´»çš„è¶…å‚æ•°é…ç½®å’Œè°ƒä¼˜æ”¯æŒ

## æ¨¡å‹æ¶æ„

```mermaid
graph TD
    A["æ¨¡å‹æ‰©å±•æ¨¡å—"] --> B["ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"];
    A --> C["æ·±åº¦å­¦ä¹ æ¨¡å‹"];
    A --> D["æ¨¡å‹å·¥å…·"];
    A --> E["é«˜çº§æ¨¡å‹"];

    B --> B1["gbdt.py - LightGBM/XGBoost"];
    B --> B2["linear.py - çº¿æ€§æ¨¡å‹"];
    B --> B3["catboost_model.py - CatBoost"];
    B --> B4["double_ensemble.py - åŒé‡é›†æˆ"];

    C --> C1["pytorch_nn.py - æ·±åº¦ç¥ç»ç½‘ç»œ"];
    C --> C2["pytorch_lstm*.py - LSTM ç³»åˆ—"];
    C --> C3["pytorch_gru*.py - GRU ç³»åˆ—"];
    C --> C4["pytorch_transformer*.py - Transformer ç³»åˆ—"];
    C --> C5["pytorch_gats*.py - å›¾æ³¨æ„åŠ›ç½‘ç»œ"];
    C --> C6["pytorch_tcn*.py - æ—¶é—´å·ç§¯ç½‘ç»œ"];

    D --> D1["pytorch_utils.py - PyTorch å·¥å…·"];
    D --> D2["æ¨¡å‹è®­ç»ƒå™¨"];
    D --> D3["ç‰¹å¾é€‰æ‹©å™¨"];
    D --> D4["æ€§èƒ½è¯„ä¼°å™¨"];

    E --> E1["pytorch_tabnet.py - TabNet"];
    E --> E2["pytorch_sandwich.py - Sandwich æ¨¡å‹"];
    E --> E3["pytorch_sfm.py - SFM æ¨¡å‹"];
    E --> E4["pytorch_tra.py - TRA æ¨¡å‹"];
    E --> E5["pytorch_localformer*.py - LocalFormer"];

    click B1 "./gbdt.py" "æŸ¥çœ‹ GBDT æ¨¡å‹å®ç°"
    click C1 "./pytorch_nn.py" "æŸ¥çœ‹ DNN æ¨¡å‹å®ç°"
    click D1 "./pytorch_utils.py" "æŸ¥çœ‹ PyTorch å·¥å…·"
    click E1 "./pytorch_tabnet.py" "æŸ¥çœ‹ TabNet æ¨¡å‹"
```

## ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹

### 1. LightGBM æ¨¡å‹ (LGBModel)
- **ç‰¹ç‚¹**ï¼šé«˜æ€§èƒ½æ¢¯åº¦æå‡æ¡†æ¶
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒé€Ÿåº¦å¿«ã€å†…å­˜å ç”¨å°‘ã€æ”¯æŒGPU
- **é€‚ç”¨åœºæ™¯**ï¼šè¡¨æ ¼æ•°æ®ã€ç‰¹å¾å·¥ç¨‹å®Œå–„çš„æ•°æ®é›†

```python
from qlib.contrib.model.gbdt import LGBModel

model = LGBModel(
    loss="mse",                    # æŸå¤±å‡½æ•°
    early_stopping_rounds=50,      # æ—©åœè½®æ•°
    num_boost_round=1000,          # æœ€å¤§è¿­ä»£æ¬¡æ•°
    learning_rate=0.1,             # å­¦ä¹ ç‡
    num_leaves=31,                 # å¶å­èŠ‚ç‚¹æ•°
    feature_fraction=0.9           # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
)
```

### 2. XGBoost æ¨¡å‹ (XGBModel)
- **ç‰¹ç‚¹**ï¼šæç«¯æ¢¯åº¦æå‡
- **ä¼˜åŠ¿**ï¼šæ­£åˆ™åŒ–æ”¯æŒã€å¤„ç†ç¼ºå¤±å€¼ã€å¹¶è¡ŒåŒ–
- **é€‚ç”¨åœºæ™¯**ï¼šç‰¹å¾ç»´åº¦é«˜ã€éœ€è¦æ­£åˆ™åŒ–çš„åœºæ™¯

### 3. çº¿æ€§æ¨¡å‹ (LinearModel)
- **ç‰¹ç‚¹**ï¼šç»å…¸ç»Ÿè®¡å­¦ä¹ æ¨¡å‹
- **ä¼˜åŠ¿**ï¼šå¯è§£é‡Šæ€§å¼ºã€è®¡ç®—ç®€å•ã€ä¸æ˜“è¿‡æ‹Ÿåˆ
- **é€‚ç”¨åœºæ™¯**ï¼šçº¿æ€§å…³ç³»æ˜æ˜¾ã€ç‰¹å¾é‡è¦æ€§åˆ†æ

### 4. CatBoost æ¨¡å‹ (CatBoostModel)
- **ç‰¹ç‚¹**ï¼šç±»åˆ«ç‰¹å¾æ¢¯åº¦æå‡
- **ä¼˜åŠ¿**ï¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾ã€å‡å°‘è¿‡æ‹Ÿåˆ
- **é€‚ç”¨åœºæ™¯**ï¼šåŒ…å«å¤§é‡ç±»åˆ«ç‰¹å¾çš„æ•°æ®

## æ·±åº¦å­¦ä¹ æ¨¡å‹ç³»åˆ—

### 1. æ·±åº¦ç¥ç»ç½‘ç»œ (DNNModelPytorch)
```python
from qlib.contrib.model.pytorch_nn import DNNModelPytorch

model = DNNModelPytorch(
    input_dim=100,                 # è¾“å…¥ç»´åº¦
    output_dim=1,                  # è¾“å‡ºç»´åº¦
    layers=(256, 128, 64),         # éšè—å±‚ç»“æ„
    lr=0.001,                      # å­¦ä¹ ç‡
    dropout=0.2,                   # Dropout æ¯”ä¾‹
    batch_size=2000,               # æ‰¹å¤§å°
    early_stopping_rounds=50       # æ—©åœè½®æ•°
)
```

### 2. LSTM ç³»åˆ—
- **ALSTM**ï¼šæ³¨æ„åŠ›å¢å¼º LSTM
- **LSTM**ï¼šæ ‡å‡†é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ
- **LSTM_TS**ï¼šæ—¶é—´åºåˆ—ä¸“ç”¨ LSTM
- **ç‰¹ç‚¹**ï¼šé€‚åˆåºåˆ—å»ºæ¨¡ã€é•¿æœŸä¾èµ–å­¦ä¹ 

### 3. GRU ç³»åˆ—
- **GRU**ï¼šé—¨æ§å¾ªç¯å•å…ƒ
- **GRU_TS**ï¼šæ—¶é—´åºåˆ—ä¸“ç”¨ GRU
- **ç‰¹ç‚¹**ï¼šè®¡ç®—æ•ˆç‡é«˜äº LSTMã€é€‚åˆé•¿åºåˆ—

### 4. Transformer ç³»åˆ—
- **Transformer**ï¼šæ ‡å‡† Transformer
- **LocalFormer**ï¼šå±€éƒ¨æ³¨æ„åŠ› Transformer
- **ç‰¹ç‚¹**ï¼šå¹¶è¡Œè®¡ç®—ã€æ³¨æ„åŠ›æœºåˆ¶ã€å…¨å±€ä¾èµ–

### 5. æ—¶åºä¸“ç”¨æ¨¡å‹
- **TCN**ï¼šæ—¶é—´å·ç§¯ç½‘ç»œ
- **TCN_TS**ï¼šæ—¶é—´åºåˆ—ä¸“ç”¨ TCN
- **ç‰¹ç‚¹**ï¼šå› æœå·ç§¯ã€é•¿æœŸè®°å¿†ã€å¹¶è¡Œè®­ç»ƒ

### 6. å›¾ç¥ç»ç½‘ç»œ
- **GATs**ï¼šå›¾æ³¨æ„åŠ›ç½‘ç»œ
- **GATs_TS**ï¼šæ—¶é—´åºåˆ—å›¾ç½‘ç»œ
- **ç‰¹ç‚¹**ï¼šå…³ç³»å»ºæ¨¡ã€æ³¨æ„åŠ›æœºåˆ¶ã€è‚¡ç¥¨å…³è”æ€§

## é«˜çº§æ¨¡å‹

### 1. TabNet æ¨¡å‹
- **ç‰¹ç‚¹**ï¼šå¯è§£é‡Šçš„è¡¨æ ¼æ·±åº¦å­¦ä¹ æ¨¡å‹
- **ä¼˜åŠ¿**ï¼šç‰¹å¾é€‰æ‹©ã€å¯è§£é‡Šæ€§ã€é«˜æ€§èƒ½
- **é€‚ç”¨åœºæ™¯**ï¼šè¡¨æ ¼æ•°æ®ã€éœ€è¦ç‰¹å¾é‡è¦æ€§çš„åœºæ™¯

### 2. Sandwich æ¨¡å‹
- **ç‰¹ç‚¹**ï¼šä¸‰æ˜æ²»æ¶æ„æ¨¡å‹
- **ä¼˜åŠ¿**ï¼šç»“åˆä¸åŒç±»å‹ç½‘ç»œçš„ä¼˜åŠ¿
- **é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚æ¨¡å¼è¯†åˆ«

### 3. SFM æ¨¡å‹ (Selective Feature Mixing)
- **ç‰¹ç‚¹**ï¼šé€‰æ‹©æ€§ç‰¹å¾æ··åˆ
- **ä¼˜åŠ¿**ï¼šè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ã€å™ªå£°é²æ£’
- **é€‚ç”¨åœºæ™¯**ï¼šé«˜ç»´æ•°æ®ã€ç‰¹å¾é€‰æ‹©

## ä½¿ç”¨æ¥å£

### ç»Ÿä¸€çš„æ¨¡å‹æ¥å£
æ‰€æœ‰æ¨¡å‹éƒ½ç»§æ‰¿è‡ª `Model` æˆ– `ModelFT` åŸºç±»ï¼Œæä¾›ç»Ÿä¸€æ¥å£ï¼š

```python
# æ•°æ®å‡†å¤‡
from qlib.data.dataset import DatasetH
from qlib.data.dataset.handler import DataHandlerLP

dataset = DatasetH(handler=DataHandlerLP(...))

# æ¨¡å‹è®­ç»ƒ
model = YourModel(**params)
model.fit(dataset)

# æ¨¡å‹é¢„æµ‹
predictions = model.predict(dataset)

# æ¨¡å‹è¯„ä¼°
score = model.score(dataset)
```

### è®­ç»ƒé…ç½®
```python
# é€šç”¨è®­ç»ƒå‚æ•°
training_params = {
    "loss": "mse",                   # æŸå¤±å‡½æ•°
    "early_stopping_rounds": 50,    # æ—©åœ
    "eval_metric": ["l2", "rmse"],  # è¯„ä¼°æŒ‡æ ‡
    "verbose_eval": 10,             # è®­ç»ƒè¾“å‡ºé¢‘ç‡
    "save_freq": 5                  # æ¨¡å‹ä¿å­˜é¢‘ç‡
}
```

## æ¨¡å‹ç‰¹ç‚¹å¯¹æ¯”

| æ¨¡å‹ç±»å‹ | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹é€Ÿåº¦ | å†…å­˜å ç”¨ | å¯è§£é‡Šæ€§ | æ¨èåœºæ™¯ |
|---------|---------|---------|---------|---------|---------|
| LightGBM | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | è¡¨æ ¼æ•°æ®ã€å¿«é€Ÿè¿­ä»£ |
| XGBoost | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ | é«˜ç»´ç‰¹å¾ã€æ­£åˆ™åŒ– |
| DNN | â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­ | å¤æ‚éçº¿æ€§å…³ç³» |
| LSTM | â­â­ | â­â­â­ | â­â­ | â­ | æ—¶åºå»ºæ¨¡ã€é•¿æœŸä¾èµ– |
| Transformer | â­â­ | â­â­â­â­ | â­ | â­ | å¹¶è¡Œè®¡ç®—ã€æ³¨æ„åŠ› |
| TabNet | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | å¯è§£é‡Šæ€§è¦æ±‚é«˜ |

## é«˜çº§åŠŸèƒ½

### 1. ç‰¹å¾é‡è¦æ€§åˆ†æ
```python
# LightGBM ç‰¹å¾é‡è¦æ€§
importance = model.feature_importance_
feature_names = model.feature_name_

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
import matplotlib.pyplot as plt
plt.barh(feature_names, importance)
```

### 2. æ¨¡å‹è§£é‡Š
```python
# SHAP å€¼åˆ†æ (LightGBM)
import shap
explainer = shap.TreeExplainer(model.model)
shap_values = explainer.shap_values(X_test)

# ç‰¹å¾è´¡çŒ®åˆ†æ
shap.summary_plot(shap_values, X_test)
```

### 3. è¶…å‚æ•°ä¼˜åŒ–
```python
from qlib.contrib.tuner import ParamTuner

tuner = ParamTuner(
    model_class=LGBModel,
    param_space={
        "learning_rate": [0.01, 0.05, 0.1],
        "num_leaves": [31, 63, 127],
        "feature_fraction": [0.8, 0.9, 1.0]
    }
)

best_params = tuner.tune(dataset)
```

## æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿ
```python
# PyTorch æ¨¡å‹ GPU æ”¯æŒ
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = DNNModelPytorch(..., device=device)
```

### æ‰¹é‡æ¨ç†
```python
# å¤§æ•°æ®é‡æ‰¹é‡é¢„æµ‹
def batch_predict(model, dataset, batch_size=10000):
    predictions = []
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        pred = model.predict(batch)
        predictions.append(pred)
    return np.concatenate(predictions)
```

### æ¨¡å‹å‹ç¼©
```python
# æ¨¡å‹é‡åŒ– (PyTorch)
model_int8 = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

## æµ‹è¯•ä¸éªŒè¯

### äº¤å‰éªŒè¯
```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
scores = []

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    scores.append(score)
```

### æ—¶é—´åºåˆ—éªŒè¯
```python
# æ»šåŠ¨çª—å£éªŒè¯
def rolling_validation(model, data, window_size=252):
    predictions = []
    for i in range(window_size, len(data)):
        train_data = data[i-window_size:i]
        test_data = data[i:i+1]

        model.fit(train_data)
        pred = model.predict(test_data)
        predictions.append(pred)
    return predictions
```

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ
- **æ•°æ®é‡å°** (< 10K)ï¼šé€‰æ‹© LightGBM æˆ–çº¿æ€§æ¨¡å‹
- **åºåˆ—æ•°æ®**ï¼šé€‰æ‹© LSTMã€GRU æˆ– TCN
- **éœ€è¦è§£é‡Šæ€§**ï¼šé€‰æ‹© LightGBM æˆ– TabNet
- **å¤æ‚å…³ç³»**ï¼šé€‰æ‹© DNN æˆ– Transformer

### Q2: å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿ
```python
# LightGBM è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼
model = LGBModel(missing_value=np.nan)

# å…¶ä»–æ¨¡å‹éœ€è¦é¢„å¤„ç†
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
```

### Q3: å¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ
```python
# æ—©åœæœºåˆ¶
model = LGBModel(early_stopping_rounds=50)

# æ­£åˆ™åŒ–
model = DNNModelPytorch(dropout=0.3, weight_decay=1e-4)

# äº¤å‰éªŒè¯
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
```

## ç›¸å…³æ–‡ä»¶æ¸…å•

### ä¼ ç»Ÿæœºå™¨å­¦ä¹ 
- `gbdt.py` - LightGBM æ¨¡å‹å®ç°
- `xgboost.py` - XGBoost æ¨¡å‹å®ç°
- `linear.py` - çº¿æ€§æ¨¡å‹å®ç°
- `catboost_model.py` - CatBoost æ¨¡å‹å®ç°
- `double_ensemble.py` - åŒé‡é›†æˆæ¨¡å‹

### æ·±åº¦å­¦ä¹ æ ¸å¿ƒ
- `pytorch_nn.py` - æ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€
- `pytorch_utils.py` - PyTorch å·¥å…·å‡½æ•°
- `pytorch_add.py` - è‡ªåŠ¨å·®åˆ†æ¨¡å‹

### æ—¶åºæ¨¡å‹
- `pytorch_lstm.py` - LSTM å®ç°
- `pytorch_alstm.py` - æ³¨æ„åŠ› LSTM
- `pytorch_gru.py` - GRU å®ç°
- `pytorch_tcn.py` - æ—¶é—´å·ç§¯ç½‘ç»œ

### é«˜çº§æ¨¡å‹
- `pytorch_transformer.py` - Transformer
- `pytorch_gats.py` - å›¾æ³¨æ„åŠ›ç½‘ç»œ
- `pytorch_tabnet.py` - TabNet å®ç°
- `pytorch_localformer.py` - LocalFormer

## å˜æ›´è®°å½• (Changelog)

### 2025-11-17 12:35:11
- âœ¨ åˆ›å»ºæ¨¡å‹æ‰©å±•æ¨¡å—è¯¦ç»†æ–‡æ¡£
- ğŸ“Š å®Œæˆæ¨¡å‹åˆ†ç±»å’Œæ¶æ„åˆ†æ
- ğŸ”— å»ºç«‹æ¨¡å‹å…³ç³»å’Œä½¿ç”¨æŒ‡å—
- ğŸ“ è¡¥å……æ€§èƒ½å¯¹æ¯”å’Œé€‰æ‹©å»ºè®®
- ğŸ”§ æ·»åŠ è¶…å‚æ•°ä¼˜åŒ–æŒ‡å—