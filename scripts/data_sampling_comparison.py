#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡æŠ½æ ·æ¯”å¯¹å·¥å…·

ä¸ investment_data é¡¹ç›®çš„ Dolt æ•°æ®åº“è¿›è¡ŒæŠ½æ ·å¯¹æ¯”ï¼Œè¯„ä¼°æœ¬åœ°æ•°æ®è´¨é‡ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æŠ½æ ·é€‰æ‹©è‚¡ç¥¨å’Œæ—¶é—´æ®µ
- ä» Dolt æ•°æ®åº“è·å–å‚è€ƒæ•°æ®
- å¯¹æ¯”æœ¬åœ°æ•°æ®å’Œå‚è€ƒæ•°æ®
- ç”Ÿæˆè¯¦ç»†çš„è´¨é‡æŠ¥å‘Š
- å¯è§†åŒ–æ¯”å¯¹ç»“æœ

ä¾èµ–é¡¹ï¼š
    pip install dolt pymysql

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/data_sampling_comparison.py
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import warnings
import subprocess
import json

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

warnings.filterwarnings('ignore')


class DoltDataFetcher:
    """
    Dolt æ•°æ®åº“æ•°æ®è·å–å™¨

    ä» Dolt æ•°æ®åº“è·å–å‚è€ƒæ•°æ®ç”¨äºæ¯”å¯¹ã€‚
    """

    def __init__(self, dolt_db_path: str = None):
        """
        åˆå§‹åŒ– Dolt æ•°æ®è·å–å™¨

        Args:
            dolt_db_path: æœ¬åœ° Dolt æ•°æ®åº“è·¯å¾„
        """
        self.dolt_db_path = dolt_db_path
        self.logger = self._setup_logger()

        # æ£€æŸ¥ Dolt æ˜¯å¦å®‰è£…
        self.dolt_available = self._check_dolt()

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("DoltDataFetcher")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _check_dolt(self) -> bool:
        """æ£€æŸ¥ Dolt æ˜¯å¦å®‰è£…"""
        try:
            result = subprocess.run(
                ["dolt", "version"],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                self.logger.info(f"âœ… Dolt å·²å®‰è£…: {result.stdout.strip()}")
                return True
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass

        self.logger.warning("âš ï¸  Dolt æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return False

    def clone_dolt_db(self, db_name: str = "chenditc/investment_data", target_dir: str = None):
        """
        å…‹éš† Dolt æ•°æ®åº“

        Args:
            db_name: Dolt æ•°æ®åº“åç§°
            target_dir: ç›®æ ‡ç›®å½•

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.dolt_available:
            self.logger.error("âŒ Dolt æœªå®‰è£…ï¼Œæ— æ³•å…‹éš†æ•°æ®åº“")
            self.logger.info("ğŸ’¡ å®‰è£… Dolt: curl - https://www.dolthub.com/install.sh")
            return False

        if target_dir is None:
            target_dir = Path.cwd() / "dolt_data"

        try:
            target_dir = Path(target_dir)
            target_dir.mkdir(parents=True, exist_ok=True)

            self.logger.info(f"æ­£åœ¨å…‹éš† Dolt æ•°æ®åº“: {db_name}")

            result = subprocess.run(
                ["dolt", "clone", db_name, str(target_dir)],
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )

            if result.returncode == 0:
                self.dolt_db_path = str(target_dir / db_name)
                self.logger.info(f"âœ… æ•°æ®åº“å…‹éš†æˆåŠŸ: {self.dolt_db_path}")
                return True
            else:
                self.logger.error(f"âŒ å…‹éš†å¤±è´¥: {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            self.logger.error("âŒ å…‹éš†è¶…æ—¶ï¼ˆæ•°æ®åº“å¯èƒ½å¾ˆå¤§ï¼‰")
            return False
        except Exception as e:
            self.logger.error(f"âŒ å…‹éš†å¤±è´¥: {e}")
            return False

    def query_dolt_db(self, sql: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢ Dolt æ•°æ®åº“

        Args:
            sql: SQL æŸ¥è¯¢è¯­å¥

        Returns:
            æŸ¥è¯¢ç»“æœ DataFrame
        """
        if not self.dolt_db_path:
            self.logger.error("âŒ Dolt æ•°æ®åº“è·¯å¾„æœªè®¾ç½®")
            return pd.DataFrame()

        try:
            result = subprocess.run(
                ["dolt", "sql", "-q", sql],
                cwd=self.dolt_db_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode == 0:
                # è§£æè¾“å‡ºï¼ˆå‡è®¾æ˜¯ CSV æ ¼å¼ï¼‰
                from io import StringIO
                df = pd.read_csv(StringIO(result.stdout))
                return df
            else:
                self.logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {result.stderr}")
                return pd.DataFrame()

        except subprocess.TimeoutExpired:
            self.logger.error("âŒ æŸ¥è¯¢è¶…æ—¶")
            return pd.DataFrame()
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_sample_data_from_dolt(
        self,
        symbols: List[str] = None,
        start_date: str = None,
        end_date: str = None,
        limit: int = 1000
    ) -> pd.DataFrame:
        """
        ä» Dolt æ•°æ®åº“è·å–æŠ½æ ·æ•°æ®

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: é™åˆ¶è®°å½•æ•°

        Returns:
            æŠ½æ ·æ•°æ® DataFrame
        """
        self.logger.info("ä» Dolt æ•°æ®åº“è·å–æŠ½æ ·æ•°æ®...")

        # æ„å»ºæŸ¥è¯¢
        where_clauses = []
        if symbols:
            symbol_list = "', '".join(symbols)
            where_clauses.append(f"symbol IN ('{symbol_list}')")

        if start_date:
            where_clauses.append(f"tradedate >= '{start_date}'")

        if end_date:
            where_clauses.append(f"tradedate <= '{end_date}'")

        where_sql = f"WHERE {' AND '.join(where_clauses)}" if where_clauses else ""

        sql = f"""
        SELECT tradedate, symbol, open, high, low, close, volume, amount
        FROM final_a_stock_eod_price
        {where_sql}
        ORDER BY tradedate, symbol
        LIMIT {limit}
        """

        self.logger.info(f"SQL: {sql}")

        df = self.query_dolt_db(sql)

        if not df.empty:
            self.logger.info(f"âœ… è·å–åˆ° {len(df)} æ¡ Dolt æ•°æ®")
        else:
            self.logger.warning("âš ï¸  æœªè·å–åˆ° Dolt æ•°æ®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

        return df


class DataComparisonAnalyzer:
    """
    æ•°æ®å¯¹æ¯”åˆ†æå™¨

    å¯¹æ¯”æœ¬åœ°æ•°æ®å’Œå‚è€ƒæ•°æ®ï¼Œè¯„ä¼°æ•°æ®è´¨é‡ã€‚
    """

    def __init__(self, local_data_dir: str = None):
        """
        åˆå§‹åŒ–å¯¹æ¯”åˆ†æå™¨

        Args:
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•
        """
        self.local_data_dir = Path(local_data_dir or "~/.qlib/qlib_data/cn_data").expanduser()
        self.logger = self._setup_logger()

        self.comparison_results = {
            "summary": {},
            "price_comparison": {},
            "volume_comparison": {},
            "completeness_comparison": {},
            "sample_details": []
        }

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("DataComparisonAnalyzer")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def load_local_data(
        self,
        file_name: str = "stock_data.csv",
        symbols: List[str] = None,
        start_date: str = None,
        end_date: str = None
    ) -> pd.DataFrame:
        """
        åŠ è½½æœ¬åœ°æ•°æ®

        Args:
            file_name: æ•°æ®æ–‡ä»¶å
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æœ¬åœ°æ•°æ® DataFrame
        """
        file_path = self.local_data_dir / file_name

        if not file_path.exists():
            self.logger.error(f"âŒ æœ¬åœ°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()

        try:
            df = pd.read_csv(file_path)

            # ç¡®ä¿ tradedate æ˜¯å­—ç¬¦ä¸²ç±»å‹
            if "tradedate" in df.columns:
                df["tradedate"] = df["tradedate"].astype(str)

            # è¿‡æ»¤æ•°æ®
            if symbols:
                df = df[df["symbol"].isin(symbols)]

            if start_date:
                df = df[df["tradedate"] >= start_date]

            if end_date:
                df = df[df["tradedate"] <= end_date]

            self.logger.info(f"âœ… åŠ è½½æœ¬åœ°æ•°æ®: {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def select_sample_symbols(
        self,
        n_symbols: int = 20,
        method: str = "random",
        file_name: str = "stock_data.csv"
    ) -> List[str]:
        """
        é€‰æ‹©æŠ½æ ·çš„è‚¡ç¥¨

        Args:
            n_symbols: æŠ½æ ·æ•°é‡
            method: æŠ½æ ·æ–¹æ³• ("random", "market_cap", "index")
            file_name: æ•°æ®æ–‡ä»¶å

        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        self.logger.info(f"é€‰æ‹©æŠ½æ ·è‚¡ç¥¨: æ–¹æ³•={method}, æ•°é‡={n_symbols}")

        # ä»æœ¬åœ°æ•°æ®ä¸­è·å–å®é™…å¯ç”¨çš„è‚¡ç¥¨
        file_path = self.local_data_dir / file_name

        if file_path.exists():
            try:
                df = pd.read_csv(file_path)
                available_symbols = df["symbol"].unique().tolist()

                if len(available_symbols) == 0:
                    self.logger.warning("âš ï¸  æœ¬åœ°æ•°æ®æ²¡æœ‰è‚¡ç¥¨ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨")
                    available_symbols = [
                        "000001.SZ", "000002.SZ", "000063.SZ", "000066.SZ", "000333.SZ",
                        "000338.SZ", "000401.SZ", "000402.SZ", "000651.SZ", "000652.SZ"
                    ]
                else:
                    self.logger.info(f"ğŸ“Š æœ¬åœ°æ•°æ®å…±æœ‰ {len(available_symbols)} åªè‚¡ç¥¨")
            except Exception as e:
                self.logger.warning(f"âš ï¸  è¯»å–æœ¬åœ°æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨")
                available_symbols = [
                    "000001.SZ", "000002.SZ", "000063.SZ", "000066.SZ", "000333.SZ",
                    "000338.SZ", "000401.SZ", "000402.SZ", "000651.SZ", "000652.SZ"
                ]
        else:
            self.logger.warning(f"âš ï¸  æœ¬åœ°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨")
            available_symbols = [
                "000001.SZ", "000002.SZ", "000063.SZ", "000066.SZ", "000333.SZ",
                "000338.SZ", "000401.SZ", "000402.SZ", "000651.SZ", "000652.SZ"
            ]

        # ä»å¯ç”¨è‚¡ç¥¨ä¸­é€‰æ‹©
        n_select = min(n_symbols, len(available_symbols))

        if method == "random":
            selected = np.random.choice(available_symbols, n_select, replace=False)
        elif method == "index":
            # ä¼˜å…ˆé€‰æ‹©æŒ‡æ•°æˆåˆ†è‚¡
            index_stocks = [s for s in available_symbols if s.startswith(("60", "00"))]
            selected = index_stocks[:n_select] if len(index_stocks) >= n_select else available_symbols[:n_select]
        else:
            selected = available_symbols[:n_select]

        self.logger.info(f"âœ… é€‰æ‹© {len(selected)} åªè‚¡ç¥¨: {', '.join(selected[:5])}...")

        return list(selected)

    def compare_price_data(
        self,
        local_df: pd.DataFrame,
        reference_df: pd.DataFrame
    ) -> Dict:
        """
        å¯¹æ¯”ä»·æ ¼æ•°æ®

        Args:
            local_df: æœ¬åœ°æ•°æ®
            reference_df: å‚è€ƒæ•°æ®

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¯¹æ¯”ä»·æ ¼æ•°æ®")
        self.logger.info("=" * 60)

        # åˆå¹¶æ•°æ®
        merged = pd.merge(
            local_df,
            reference_df,
            on=["tradedate", "symbol"],
            how="inner",
            suffixes=("_local", "_ref")
        )

        if merged.empty:
            self.logger.warning("âš ï¸  æ²¡æœ‰å…±åŒçš„æ•°æ®ç‚¹å¯ä»¥å¯¹æ¯”")
            return {"match_rate": 0, "total_compared": 0}

        # å¯¹æ¯”å­—æ®µ
        price_fields = ["open", "high", "low", "close"]
        comparison_results = {}

        for field in price_fields:
            local_col = f"{field}_local"
            ref_col = f"{field}_ref"

            if local_col in merged.columns and ref_col in merged.columns:
                # è®¡ç®—åå·®
                merged[f"{field}_diff"] = merged[local_col] - merged[ref_col]
                merged[f"{field}_diff_pct"] = (merged[local_col] - merged[ref_col]) / merged[ref_col]

                # ç»Ÿè®¡
                total = len(merged)
                exact_match = (merged[f"{field}_diff"] == 0).sum()
                small_diff = (merged[f"{field}_diff"].abs() < 0.01).sum()  # å·®å¼‚å°äº0.01
                large_diff = (merged[f"{field}_diff_pct"].abs() > 0.05).sum()  # å·®å¼‚è¶…è¿‡5%

                comparison_results[field] = {
                    "total_compared": total,
                    "exact_match": exact_match,
                    "exact_match_rate": exact_match / total if total > 0 else 0,
                    "small_diff": small_diff,
                    "small_diff_rate": small_diff / total if total > 0 else 0,
                    "large_diff": large_diff,
                    "large_diff_rate": large_diff / total if total > 0 else 0,
                    "mean_diff": merged[f"{field}_diff"].mean(),
                    "std_diff": merged[f"{field}_diff"].std(),
                    "mean_abs_diff_pct": merged[f"{field}_diff_pct"].abs().mean()
                }

                self.logger.info(f"\n{field.upper()} å­—æ®µå¯¹æ¯”:")
                self.logger.info(f"  å¯¹æ¯”è®°å½•æ•°: {total}")
                self.logger.info(f"  å®Œå…¨ä¸€è‡´: {exact_match} ({exact_match/total:.2%})")
                self.logger.info(f"  å°å·®å¼‚(<0.01): {small_diff} ({small_diff/total:.2%})")
                self.logger.info(f"  å¤§å·®å¼‚(>5%): {large_diff} ({large_diff/total:.2%})")
                self.logger.info(f"  å¹³å‡åå·®: {comparison_results[field]['mean_diff']:.6f}")
                self.logger.info(f"  æ ‡å‡†å·®: {comparison_results[field]['std_diff']:.6f}")

        self.comparison_results["price_comparison"] = comparison_results

        # ä¿å­˜è¯¦ç»†æ¯”å¯¹æ•°æ®
        self.comparison_results["sample_details"] = merged

        return comparison_results

    def compare_volume_data(
        self,
        local_df: pd.DataFrame,
        reference_df: pd.DataFrame
    ) -> Dict:
        """
        å¯¹æ¯”æˆäº¤é‡æ•°æ®

        Args:
            local_df: æœ¬åœ°æ•°æ®
            reference_df: å‚è€ƒæ•°æ®

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¯¹æ¯”æˆäº¤é‡æ•°æ®")
        self.logger.info("=" * 60)

        # åˆå¹¶æ•°æ®
        merged = pd.merge(
            local_df,
            reference_df,
            on=["tradedate", "symbol"],
            how="inner",
            suffixes=("_local", "_ref")
        )

        if merged.empty:
            self.logger.warning("âš ï¸  æ²¡æœ‰å…±åŒçš„æ•°æ®ç‚¹å¯ä»¥å¯¹æ¯”")
            return {}

        # å¯¹æ¯”æˆäº¤é‡
        if "volume_local" in merged.columns and "volume_ref" in merged.columns:
            merged["volume_diff"] = merged["volume_local"] - merged["volume_ref"]
            merged["volume_diff_pct"] = (
                (merged["volume_local"] - merged["volume_ref"]) /
                merged["volume_ref"].replace(0, np.nan)
            )

            # ç»Ÿè®¡
            total = len(merged)
            exact_match = (merged["volume_diff"] == 0).sum()
            small_diff = (merged["volume_diff"].abs() < 1000).sum()  # å·®å¼‚å°äº1000æ‰‹

            volume_stats = {
                "total_compared": total,
                "exact_match": exact_match,
                "exact_match_rate": exact_match / total if total > 0 else 0,
                "small_diff": small_diff,
                "small_diff_rate": small_diff / total if total > 0 else 0,
                "mean_diff": merged["volume_diff"].mean(),
                "std_diff": merged["volume_diff"].std(),
                "correlation": merged["volume_local"].corr(merged["volume_ref"])
            }

            self.logger.info(f"\næˆäº¤é‡å¯¹æ¯”:")
            self.logger.info(f"  å¯¹æ¯”è®°å½•æ•°: {total}")
            self.logger.info(f"  å®Œå…¨ä¸€è‡´: {exact_match} ({exact_match/total:.2%})")
            self.logger.info(f"  ç›¸å…³ç³»æ•°: {volume_stats['correlation']:.4f}")

            self.comparison_results["volume_comparison"] = volume_stats

        return self.comparison_results.get("volume_comparison", {})

    def compare_completeness(
        self,
        local_df: pd.DataFrame,
        reference_df: pd.DataFrame
    ) -> Dict:
        """
        å¯¹æ¯”æ•°æ®å®Œæ•´æ€§

        Args:
            local_df: æœ¬åœ°æ•°æ®
            reference_df: å‚è€ƒæ•°æ®

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¯¹æ¯”æ•°æ®å®Œæ•´æ€§")
        self.logger.info("=" * 60)

        # ç»Ÿè®¡æœ¬åœ°æ•°æ®
        local_dates = set(local_df["tradedate"].unique()) if "tradedate" in local_df.columns else set()
        local_symbols = set(local_df["symbol"].unique()) if "symbol" in local_df.columns else set()

        # ç»Ÿè®¡å‚è€ƒæ•°æ®
        ref_dates = set(reference_df["tradedate"].unique()) if "tradedate" in reference_df.columns else set()
        ref_symbols = set(reference_df["symbol"].unique()) if "symbol" in reference_df.columns else set()

        # å¯¹æ¯”
        missing_dates = ref_dates - local_dates
        missing_symbols = ref_symbols - local_symbols
        extra_dates = local_dates - ref_dates
        extra_symbols = local_symbols - ref_symbols

        completeness_stats = {
            "local_date_count": len(local_dates),
            "ref_date_count": len(ref_dates),
            "local_symbol_count": len(local_symbols),
            "ref_symbol_count": len(ref_symbols),
            "missing_dates": len(missing_dates),
            "missing_symbols": len(missing_symbols),
            "extra_dates": len(extra_dates),
            "extra_symbols": len(extra_symbols),
            "date_coverage": len(local_dates & ref_dates) / len(ref_dates) if ref_dates else 0,
            "symbol_coverage": len(local_symbols & ref_symbols) / len(ref_symbols) if ref_symbols else 0
        }

        self.logger.info(f"\næ•°æ®å®Œæ•´æ€§å¯¹æ¯”:")
        self.logger.info(f"  æœ¬åœ°äº¤æ˜“æ—¥æ•°: {completeness_stats['local_date_count']}")
        self.logger.info(f"  å‚è€ƒäº¤æ˜“æ—¥æ•°: {completeness_stats['ref_date_count']}")
        self.logger.info(f"  æ—¥æœŸè¦†ç›–ç‡: {completeness_stats['date_coverage']:.2%}")
        self.logger.info(f"  æœ¬åœ°è‚¡ç¥¨æ•°: {completeness_stats['local_symbol_count']}")
        self.logger.info(f"  å‚è€ƒè‚¡ç¥¨æ•°: {completeness_stats['ref_symbol_count']}")
        self.logger.info(f"  è‚¡ç¥¨è¦†ç›–ç‡: {completeness_stats['symbol_coverage']:.2%}")

        self.comparison_results["completeness_comparison"] = completeness_stats

        return completeness_stats

    def generate_comparison_report(self) -> str:
        """
        ç”Ÿæˆæ¯”å¯¹æŠ¥å‘Š

        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = "\n" + "=" * 60
        report += "\næ•°æ®è´¨é‡æŠ½æ ·æ¯”å¯¹æŠ¥å‘Š"
        report += "\n" + "=" * 60

        # æ¦‚è¦
        if self.comparison_results["summary"]:
            report += "\n\n## æ¦‚è¦\n"
            for key, value in self.comparison_results["summary"].items():
                report += f"- {key}: {value}\n"

        # ä»·æ ¼å¯¹æ¯”
        if self.comparison_results.get("price_comparison"):
            report += "\n\n## ä»·æ ¼å¯¹æ¯”è¯¦æƒ…\n"
            price_comp = self.comparison_results["price_comparison"]

            for field, stats in price_comp.items():
                report += f"\n### {field.upper()}\n"
                report += f"- å¯¹æ¯”è®°å½•æ•°: {stats['total_compared']:,}\n"
                report += f"- å®Œå…¨ä¸€è‡´ç‡: {stats['exact_match_rate']:.2%}\n"
                report += f"- å°å·®å¼‚ç‡: {stats['small_diff_rate']:.2%}\n"
                report += f"- å¤§å·®å¼‚ç‡: {stats['large_diff_rate']:.2%}\n"
                report += f"- å¹³å‡åå·®: {stats['mean_diff']:.6f}\n"

        # æˆäº¤é‡å¯¹æ¯”
        if self.comparison_results.get("volume_comparison"):
            report += "\n\n## æˆäº¤é‡å¯¹æ¯”è¯¦æƒ…\n"
            vol_comp = self.comparison_results["volume_comparison"]

            report += f"- å¯¹æ¯”è®°å½•æ•°: {vol_comp['total_compared']:,}\n"
            report += f"- å®Œå…¨ä¸€è‡´ç‡: {vol_comp['exact_match_rate']:.2%}\n"
            report += f"- ç›¸å…³ç³»æ•°: {vol_comp['correlation']:.4f}\n"

        # å®Œæ•´æ€§å¯¹æ¯”
        if self.comparison_results.get("completeness_comparison"):
            report += "\n\n## æ•°æ®å®Œæ•´æ€§å¯¹æ¯”\n"
            comp_comp = self.comparison_results["completeness_comparison"]

            report += f"- æ—¥æœŸè¦†ç›–ç‡: {comp_comp['date_coverage']:.2%}\n"
            report += f"- è‚¡ç¥¨è¦†ç›–ç‡: {comp_comp['symbol_coverage']:.2%}\n"
            report += f"- ç¼ºå¤±äº¤æ˜“æ—¥: {comp_comp['missing_dates']}\n"
            report += f"- ç¼ºå¤±è‚¡ç¥¨: {comp_comp['missing_symbols']}\n"

        # æ€»ä½“è¯„ä»·
        report += "\n\n## æ€»ä½“è¯„ä»·\n"
        overall_score = self._calculate_overall_score()
        report += f"æ•°æ®è´¨é‡è¯„åˆ†: {overall_score:.1f}/100\n"

        if overall_score >= 90:
            report += "è¯„ä»·: â­â­â­â­â­ ä¼˜ç§€ - æ•°æ®è´¨é‡å¾ˆé«˜\n"
        elif overall_score >= 75:
            report += "è¯„ä»·: â­â­â­â­ è‰¯å¥½ - æ•°æ®è´¨é‡è‰¯å¥½\n"
        elif overall_score >= 60:
            report += "è¯„ä»·: â­â­â­ ä¸­ç­‰ - æ•°æ®è´¨é‡å¯æ¥å—\n"
        else:
            report += "è¯„ä»·: â­â­ è¾ƒå·® - æ•°æ®éœ€è¦æ”¹è¿›\n"

        return report

    def _calculate_overall_score(self) -> float:
        """
        è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†

        Returns:
            è¯„åˆ† (0-100)
        """
        score = 100.0

        # ä»·æ ¼ä¸€è‡´æ€§ (æƒé‡ 40%)
        if self.comparison_results.get("price_comparison"):
            price_comp = self.comparison_results["price_comparison"]
            if price_comp:
                avg_exact_match_rate = np.mean([
                    stats["exact_match_rate"]
                    for stats in price_comp.values()
                ])
                score -= (1 - avg_exact_match_rate) * 40

        # æˆäº¤é‡ä¸€è‡´æ€§ (æƒé‡ 20%)
        if self.comparison_results.get("volume_comparison"):
            vol_comp = self.comparison_results["volume_comparison"]
            if vol_comp:
                score -= (1 - vol_comp["exact_match_rate"]) * 20

        # å®Œæ•´æ€§ (æƒé‡ 40%)
        if self.comparison_results.get("completeness_comparison"):
            comp_comp = self.comparison_results["completeness_comparison"]
            if comp_comp:
                avg_coverage = (comp_comp["date_coverage"] + comp_comp["symbol_coverage"]) / 2
                score -= (1 - avg_coverage) * 40

        return max(0, score)

    def visualize_comparison(self, output_dir: str = None):
        """
        å¯è§†åŒ–æ¯”å¯¹ç»“æœ

        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        if self.comparison_results.get("sample_details") is None:
            self.logger.warning("âš ï¸  æ²¡æœ‰æ¯”å¯¹æ•°æ®å¯ä»¥å¯è§†åŒ–")
            return

        merged = self.comparison_results["sample_details"]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.local_data_dir / "comparison_charts"
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. æ”¶ç›˜ä»·å¯¹æ¯”æ•£ç‚¹å›¾
        if "close_local" in merged.columns and "close_ref" in merged.columns:
            ax = axes[0, 0]
            ax.scatter(merged["close_ref"], merged["close_local"], alpha=0.5, s=1)
            ax.plot([merged["close_ref"].min(), merged["close_ref"].max()],
                    [merged["close_ref"].min(), merged["close_ref"].max()],
                    'r--', label='å®Œç¾åŒ¹é…çº¿')
            ax.set_xlabel("å‚è€ƒæ”¶ç›˜ä»·")
            ax.set_ylabel("æœ¬åœ°æ”¶ç›˜ä»·")
            ax.set_title("æ”¶ç›˜ä»·å¯¹æ¯”")
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 2. æ”¶ç›˜ä»·å·®å¼‚åˆ†å¸ƒ
        if "close_diff_pct" in merged.columns:
            ax = axes[0, 1]
            ax.hist(merged["close_diff_pct"].dropna(), bins=50, edgecolor='black')
            ax.set_xlabel("ä»·æ ¼å·®å¼‚ç™¾åˆ†æ¯”")
            ax.set_ylabel("é¢‘æ•°")
            ax.set_title("ä»·æ ¼å·®å¼‚åˆ†å¸ƒ")
            ax.axvline(0, color='r', linestyle='--', label='é›¶å·®å¼‚çº¿')
            ax.axvline(0.05, color='orange', linestyle='--', label='Â±5%é˜ˆå€¼')
            ax.axvline(-0.05, color='orange', linestyle='--')
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 3. æˆäº¤é‡å¯¹æ¯”æ•£ç‚¹å›¾
        if "volume_local" in merged.columns and "volume_ref" in merged.columns:
            ax = axes[1, 0]
            ax.scatter(merged["volume_ref"], merged["volume_local"], alpha=0.5, s=1)
            ax.plot([merged["volume_ref"].min(), merged["volume_ref"].max()],
                    [merged["volume_ref"].min(), merged["volume_ref"].max()],
                    'r--', label='å®Œç¾åŒ¹é…çº¿')
            ax.set_xlabel("å‚è€ƒæˆäº¤é‡")
            ax.set_ylabel("æœ¬åœ°æˆäº¤é‡")
            ax.set_title("æˆäº¤é‡å¯¹æ¯”")
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 4. ä»·æ ¼å·®å¼‚æ—¶é—´åºåˆ—
        if "tradedate" in merged.columns and "close_diff_pct" in merged.columns:
            ax = axes[1, 1]
            plot_data = merged.groupby("tradedate")["close_diff_pct"].mean()
            ax.plot(range(len(plot_data)), plot_data.values)
            ax.axhline(0, color='r', linestyle='--', label='é›¶å·®å¼‚çº¿')
            ax.axhline(0.05, color='orange', linestyle='--', label='Â±5%é˜ˆå€¼')
            ax.axhline(-0.05, color='orange', linestyle='--')
            ax.set_xlabel("äº¤æ˜“æ—¥")
            ax.set_ylabel("å¹³å‡ä»·æ ¼å·®å¼‚(%)")
            ax.set_title("ä»·æ ¼å·®å¼‚æ—¶é—´è¶‹åŠ¿")
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        output_file = output_dir / f"comparison_{timestamp}.png"
        plt.savefig(output_file, dpi=150, bbox_inches='tight')
        plt.close()

        self.logger.info(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {output_file}")

    def run_comparison(
        self,
        reference_fetcher: DoltDataFetcher = None,
        n_samples: int = 20,
        start_date: str = None,
        end_date: str = None
    ) -> Dict:
        """
        è¿è¡Œå®Œæ•´æ¯”å¯¹

        Args:
            reference_fetcher: å‚è€ƒæ•°æ®è·å–å™¨
            n_samples: æŠ½æ ·æ•°é‡
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ¯”å¯¹ç»“æœ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ•°æ®è´¨é‡æŠ½æ ·æ¯”å¯¹")
        self.logger.info("=" * 60)

        # è®¾ç½®æ—¥æœŸèŒƒå›´ï¼ˆé»˜è®¤æœ€è¿‘3ä¸ªæœˆï¼‰
        if end_date is None:
            end_date = datetime.now().strftime("%Y%m%d")

        if start_date is None:
            start_date = (datetime.now() - timedelta(days=90)).strftime("%Y%m%d")

        self.logger.info(f"æ¯”å¯¹æ—¶é—´èŒƒå›´: {start_date} -> {end_date}")

        # é€‰æ‹©æŠ½æ ·è‚¡ç¥¨
        sample_symbols = self.select_sample_symbols(n_samples, file_name="stock_data.csv")

        # åŠ è½½æœ¬åœ°æ•°æ®
        local_df = self.load_local_data(
            symbols=sample_symbols,
            start_date=start_date,
            end_date=end_date
        )

        if local_df.empty:
            self.logger.error("âŒ æœ¬åœ°æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ¯”å¯¹")
            return {"success": False}

        # è·å–å‚è€ƒæ•°æ®
        reference_df = None

        if reference_fetcher:
            reference_df = reference_fetcher.get_sample_data_from_dolt(
                symbols=sample_symbols,
                start_date=start_date,
                end_date=end_date
            )
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            self.logger.warning("âš ï¸  æœªæä¾›å‚è€ƒæ•°æ®è·å–å™¨ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®çš„ä¸€åŠä½œä¸ºå‚è€ƒ")
            reference_df = local_df.sample(frac=0.5).copy()
            # æ·»åŠ ä¸€äº›éšæœºå™ªå£°æ¨¡æ‹Ÿå‚è€ƒæ•°æ®
            noise = np.random.normal(0, 0.01, len(reference_df))
            reference_df["close"] *= (1 + noise)

        if reference_df.empty:
            self.logger.error("âŒ å‚è€ƒæ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ¯”å¯¹")
            return {"success": False}

        # æ‰§è¡Œæ¯”å¯¹
        self.compare_price_data(local_df, reference_df)
        self.compare_volume_data(local_df, reference_df)
        self.compare_completeness(local_df, reference_df)

        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_comparison_report()
        print(report)

        # å¯è§†åŒ–
        try:
            self.visualize_comparison()
        except Exception as e:
            self.logger.warning(f"âš ï¸  å¯è§†åŒ–å¤±è´¥: {e}")

        # ä¿å­˜æ¯”å¯¹ç»“æœ
        output_file = self.local_data_dir / f"comparison_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            # è½¬æ¢æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_results = self.comparison_results.copy()
            if "sample_details" in serializable_results:
                serializable_results["sample_details"] = serializable_results["sample_details"].to_dict('records')
            json.dump(serializable_results, f, indent=2, default=str)

        self.logger.info(f"âœ… æ¯”å¯¹ç»“æœå·²ä¿å­˜: {output_file}")

        return {
            "success": True,
            "results": self.comparison_results,
            "report": report
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ•°æ®è´¨é‡æŠ½æ ·æ¯”å¯¹å·¥å…·")
    parser.add_argument("--local-data-dir", default="~/.qlib/qlib_data/cn_data", help="æœ¬åœ°æ•°æ®ç›®å½•")
    parser.add_argument("--dolt-db-path", help="Dolt æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--n-samples", type=int, default=20, help="æŠ½æ ·è‚¡ç¥¨æ•°é‡")
    parser.add_argument("--start-date", help="å¼€å§‹æ—¥æœŸ (YYYYMMDD)")
    parser.add_argument("--end-date", help="ç»“æŸæ—¥æœŸ (YYYYMMDD)")

    args = parser.parse_args()

    print("\nğŸ” æ•°æ®è´¨é‡æŠ½æ ·æ¯”å¯¹å·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥æœ¬åœ°æ•°æ®
    local_file = Path(args.local_data_dir) / "stock_data.csv"
    if not local_file.exists():
        print(f"âŒ æœ¬åœ°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {local_file}")
        print("\nğŸ’¡ æç¤º:")
        print("   1. å…ˆè¿è¡Œå¢é‡æ›´æ–°è„šæœ¬è·å–æ•°æ®:")
        print("      python scripts/tushare_incremental_update.py")
        print("   2. æˆ–ä½¿ç”¨æ¼”ç¤ºæ•°æ®:")
        print("      python scripts/data_merge_demo.py")
        return 1

    # åˆ›å»º Dolt æ•°æ®è·å–å™¨ï¼ˆå¯é€‰ï¼‰
    dolt_fetcher = None
    if args.dolt_db_path:
        dolt_fetcher = DoltDataFetcher(args.dolt_db_path)
    else:
        print("âš ï¸  æœªæŒ‡å®š Dolt æ•°æ®åº“è·¯å¾„")
        print("ğŸ’¡ å¦‚æœæœ‰ Dolt æ•°æ®åº“ï¼Œå¯ä»¥æŒ‡å®š:")
        print("   --dolt-db-path /path/to/dolt/chenditc/investment_data")
        print("\nğŸ’¡ æˆ–è€…å…‹éš†æ•°æ®åº“ï¼ˆéœ€è¦å‡ åˆ†é’Ÿï¼‰:")
        print("   dolt clone chenditc/investment_data")
        print("   ç„¶åä½¿ç”¨: --dolt-db-path ./investment_data")

    # åˆ›å»ºæ¯”å¯¹åˆ†æå™¨
    analyzer = DataComparisonAnalyzer(local_data_dir=args.local_data_dir)

    # è¿è¡Œæ¯”å¯¹
    result = analyzer.run_comparison(
        reference_fetcher=dolt_fetcher,
        n_samples=args.n_samples,
        start_date=args.start_date,
        end_date=args.end_date
    )

    return 0 if result["success"] else 1


if __name__ == "__main__":
    sys.exit(main())
