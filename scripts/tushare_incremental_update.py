#!/usr/bin/env python3
"""
TuShare Aè‚¡æ•°æ®å¢é‡æ›´æ–°è„šæœ¬

å€Ÿé‰´ investment_data é¡¹ç›®çš„å¢é‡æ›´æ–°æ€è·¯ï¼Œç»“åˆ Qlib çš„ TuShare é›†æˆï¼Œ
å®ç°æœ¬åœ° Aè‚¡è¡Œæƒ…æ•°æ®çš„å¢é‡æ›´æ–°åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- è‡ªåŠ¨æ£€æµ‹æœ€æ–°æ•°æ®æ—¥æœŸ
- å¢é‡æ›´æ–°è‚¡ç¥¨æ—¥çº¿æ•°æ®
- å¢é‡æ›´æ–°æŒ‡æ•°æ•°æ®
- å¢é‡æ›´æ–°æŒ‡æ•°æƒé‡æ•°æ®
- æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†
- è¿›åº¦è·Ÿè¸ªå’Œæ—¥å¿—è®°å½•

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/tushare_incremental_update.py

ç¯å¢ƒå˜é‡ï¼š
    TUSHARE_TOKEN: TuShare API Tokenï¼ˆå¿…éœ€ï¼‰

é…ç½®æ–‡ä»¶ï¼š
    scripts/tushare_incremental_config.yaml: å¢é‡æ›´æ–°é…ç½®
"""

import os
import sys
import logging
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import pandas as pd
import numpy as np
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ° sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from qlib.contrib.data.tushare.api_client import TuShareAPIClient
from qlib.contrib.data.tushare.config import TuShareConfig
from qlib.contrib.data.tushare.utils import (
    TuShareCodeConverter,
    TuShareDateUtils
)


class IncrementalDataUpdater:
    """
    å¢é‡æ•°æ®æ›´æ–°å™¨

    å®ç°æœ¬åœ°æ•°æ®çš„å¢é‡æ›´æ–°é€»è¾‘ï¼Œé¿å…é‡å¤ä¸‹è½½å·²æœ‰æ•°æ®ã€‚
    """

    def __init__(self, config: Dict, data_dir: str = None):
        """
        åˆå§‹åŒ–å¢é‡æ›´æ–°å™¨

        Args:
            config: é…ç½®å­—å…¸
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.config = config
        self.data_dir = Path(data_dir or config.get("data_dir", "~/.qlib/qlib_data/cn_data"))
        self.data_dir = self.data_dir.expanduser()

        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ– TuShare å®¢æˆ·ç«¯
        tushare_config = TuShareConfig(
            token=os.getenv("TUSHARE_TOKEN"),
            max_retries=config.get("max_retries", 3),
            retry_delay=config.get("retry_delay", 1.0),
            rate_limit=config.get("rate_limit", 200),
            enable_api_logging=config.get("enable_api_logging", False)
        )
        self.client = TuShareAPIClient(tushare_config)

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # è·Ÿè¸ªç»Ÿè®¡
        self.stats = {
            "start_time": datetime.now(),
            "stocks_updated": 0,
            "indices_updated": 0,
            "index_weights_updated": 0,
            "errors": []
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_level = self.config.get("log_level", "INFO")
        log_file = self.data_dir / "incremental_update.log"

        # é…ç½®æ—¥å¿—æ ¼å¼
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        date_format = "%Y-%m-%d %H:%M:%S"

        # åˆ›å»ºæ—¥å¿—å™¨
        self.logger = logging.getLogger("IncrementalUpdater")
        self.logger.setLevel(getattr(logging, log_level))

        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        self.logger.handlers.clear()

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter(log_format, date_format))

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(getattr(logging, log_level))
        console_handler.setFormatter(logging.Formatter(log_format, date_format))

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

        self.logger.info("=" * 60)
        self.logger.info("å¢é‡æ•°æ®æ›´æ–°å¼€å§‹")
        self.logger.info("=" * 60)

    def get_latest_date(self, data_type: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šæ•°æ®ç±»å‹çš„æœ€æ–°æ—¥æœŸ

        Args:
            data_type: æ•°æ®ç±»å‹ ('stock', 'index', 'index_weight')

        Returns:
            æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å› None
        """
        file_map = {
            "stock": self.data_dir / "stock_data.csv",
            "index": self.data_dir / "index_data.csv",
            "index_weight": self.data_dir / "index_weight.csv"
        }

        file_path = file_map.get(data_type)
        if not file_path or not file_path.exists():
            return None

        try:
            # è¯»å–CSVæ–‡ä»¶è·å–æœ€æ–°æ—¥æœŸ
            df = pd.read_csv(file_path, nrows=1)

            # æ ¹æ®æ•°æ®ç±»å‹ç¡®å®šæ—¥æœŸåˆ—
            date_columns = {
                "stock": ["trade_date", "date", "tradedate"],
                "index": ["trade_date", "date", "tradedate"],
                "index_weight": ["trade_date", "date", "tradedate"]
            }

            for col in date_columns.get(data_type, []):
                if col in df.columns:
                    latest_date = df[col].iloc[0]
                    # è½¬æ¢ä¸º TuShare æ—¥æœŸæ ¼å¼
                    if isinstance(latest_date, str):
                        latest_date = latest_date.replace("-", "")
                    return latest_date

            return None
        except Exception as e:
            self.logger.warning(f"æ— æ³•è¯»å– {data_type} æ•°æ®çš„æœ€æ–°æ—¥æœŸ: {e}")
            return None

    def update_stock_data(self) -> bool:
        """
        å¢é‡æ›´æ–°è‚¡ç¥¨æ—¥çº¿æ•°æ®

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ›´æ–°è‚¡ç¥¨æ—¥çº¿æ•°æ®")
        self.logger.info("=" * 60)

        try:
            # è·å–æœ€æ–°æ•°æ®æ—¥æœŸ
            latest_date = self.get_latest_date("stock")
            if latest_date:
                self.logger.info(f"æœ¬åœ°æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date}")
                start_date = latest_date
            else:
                self.logger.info("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®ï¼Œä¸‹è½½å…¨éƒ¨å†å²æ•°æ®")
                start_date = self.config.get("stock_start_date", "20200101")

            # è·å–å½“å‰æ—¥æœŸ
            end_date = datetime.now().strftime("%Y%m%d")

            # è·å–äº¤æ˜“æ—¥å†
            self.logger.info(f"è·å–äº¤æ˜“æ—¥å†: {start_date} -> {end_date}")
            trade_cal = self.client.get_trade_cal(
                exchange="SSE",
                start_date=start_date,
                end_date=end_date,
                is_open="1"
            )

            if trade_cal.empty:
                self.logger.warning("æ²¡æœ‰æ–°çš„äº¤æ˜“æ—¥")
                return True

            trade_dates = trade_cal["cal_date"].tolist()
            self.logger.info(f"å‘ç° {len(trade_dates)} ä¸ªæ–°äº¤æ˜“æ—¥")

            # è·å–è‚¡ç¥¨åˆ—è¡¨
            self.logger.info("è·å–è‚¡ç¥¨åˆ—è¡¨...")
            stock_basic = self.client.get_stock_basic(
                exchange="",
                list_status="L"
            )

            if stock_basic.empty:
                self.logger.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return False

            stock_codes = stock_basic["ts_code"].tolist()
            self.logger.info(f"å…±æœ‰ {len(stock_codes)} åªè‚¡ç¥¨")

            # æŒ‰äº¤æ˜“æ—¥æ›´æ–°æ•°æ®
            output_file = self.data_dir / "stock_data.csv"
            all_data = []

            for i, trade_date in enumerate(trade_dates, 1):
                self.logger.info(f"[{i}/{len(trade_dates)}] æ›´æ–° {trade_date} çš„æ•°æ®")

                try:
                    # è·å–å½“æ—¥æ‰€æœ‰è‚¡ç¥¨æ•°æ®
                    data = self.client._make_request("daily", {
                        "trade_date": trade_date
                    })

                    if data and "items" in data and len(data["items"]) > 0:
                        # è½¬æ¢ä¸º DataFrame
                        df = pd.DataFrame(data["items"], columns=data["fields"])

                        # å­—æ®µæ˜ å°„
                        column_mapping = {
                            "ts_code": "symbol",
                            "trade_date": "tradedate",
                            "open": "open",
                            "high": "high",
                            "low": "low",
                            "close": "close",
                            "vol": "volume",
                            "amount": "amount"
                        }

                        # é€‰æ‹©éœ€è¦çš„åˆ—å¹¶é‡å‘½å
                        existing_cols = [col for col in column_mapping.keys() if col in df.columns]
                        df = df[existing_cols].rename(columns={k: column_mapping[k] for k in existing_cols})

                        all_data.append(df)
                        self.stats["stocks_updated"] += len(df)

                    # é¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(0.5)

                except Exception as e:
                    self.logger.error(f"æ›´æ–° {trade_date} æ•°æ®å¤±è´¥: {e}")
                    self.stats["errors"].append(f"stock_{trade_date}: {str(e)}")
                    continue

            # ä¿å­˜æ•°æ®
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)

                # å¦‚æœå­˜åœ¨æ—§æ•°æ®ï¼Œåˆå¹¶åå»é‡
                if output_file.exists():
                    old_df = pd.read_csv(output_file)
                    combined_df = pd.concat([old_df, combined_df])
                    combined_df = combined_df.drop_duplicates(
                        subset=["tradedate", "symbol"],
                        keep="last"
                    )
                    combined_df = combined_df.sort_values(["tradedate", "symbol"])

                combined_df.to_csv(output_file, index=False)
                self.logger.info(f"âœ… è‚¡ç¥¨æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
                self.logger.info(f"   å…±æ›´æ–° {len(all_data)} æ¡è®°å½•")
                return True
            else:
                self.logger.warning("æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return False

    def update_index_data(self) -> bool:
        """
        å¢é‡æ›´æ–°æŒ‡æ•°æ—¥çº¿æ•°æ®

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ›´æ–°æŒ‡æ•°æ—¥çº¿æ•°æ®")
        self.logger.info("=" * 60)

        try:
            # è·å–éœ€è¦æ›´æ–°çš„æŒ‡æ•°åˆ—è¡¨
            index_list = self.config.get("index_list", [
                "399300.SZ",  # æ²ªæ·±300
                "000905.SH",  # ä¸­è¯500
                "000300.SH",  # æ²ªæ·±300
                "000906.SH",  # ä¸­è¯800
                "000852.SH",  # ä¸­è¯1000
                "000985.SH"   # ä¸­è¯å…¨æŒ‡
            ])

            self.logger.info(f"éœ€è¦æ›´æ–°çš„æŒ‡æ•°: {', '.join(index_list)}")

            # è·å–æœ€æ–°æ•°æ®æ—¥æœŸ
            latest_date = self.get_latest_date("index")
            if latest_date:
                self.logger.info(f"æœ¬åœ°æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date}")
                start_date = latest_date
            else:
                self.logger.info("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®ï¼Œä¸‹è½½å…¨éƒ¨å†å²æ•°æ®")
                start_date = self.config.get("index_start_date", "20200101")

            end_date = datetime.now().strftime("%Y%m%d")

            # è·å–äº¤æ˜“æ—¥å†
            self.logger.info(f"è·å–äº¤æ˜“æ—¥å†: {start_date} -> {end_date}")
            trade_cal = self.client.get_trade_cal(
                exchange="SSE",
                start_date=start_date,
                end_date=end_date,
                is_open="1"
            )

            if trade_cal.empty:
                self.logger.warning("æ²¡æœ‰æ–°çš„äº¤æ˜“æ—¥")
                return True

            trade_dates = trade_cal["cal_date"].tolist()
            self.logger.info(f"å‘ç° {len(trade_dates)} ä¸ªæ–°äº¤æ˜“æ—¥")

            # æ›´æ–°æ¯ä¸ªæŒ‡æ•°çš„æ•°æ®
            all_data = []

            for index_code in index_list:
                self.logger.info(f"æ›´æ–°æŒ‡æ•° {index_code}")

                try:
                    # è·å–æŒ‡æ•°æ•°æ®
                    data = self.client.get_index_daily(
                        ts_code=index_code,
                        start_date=start_date,
                        end_date=end_date
                    )

                    if not data.empty:
                        # å­—æ®µæ˜ å°„
                        df = data.rename(columns={
                            "ts_code": "symbol",
                            "trade_date": "tradedate"
                        })

                        all_data.append(df)
                        self.stats["indices_updated"] += len(df)

                    # é¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(0.3)

                except Exception as e:
                    self.logger.error(f"æ›´æ–°æŒ‡æ•° {index_code} å¤±è´¥: {e}")
                    self.stats["errors"].append(f"index_{index_code}: {str(e)}")
                    continue

            # ä¿å­˜æ•°æ®
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)

                output_file = self.data_dir / "index_data.csv"

                # å¦‚æœå­˜åœ¨æ—§æ•°æ®ï¼Œåˆå¹¶åå»é‡
                if output_file.exists():
                    old_df = pd.read_csv(output_file)
                    combined_df = pd.concat([old_df, combined_df])
                    combined_df = combined_df.drop_duplicates(
                        subset=["tradedate", "symbol"],
                        keep="last"
                    )
                    combined_df = combined_df.sort_values(["tradedate", "symbol"])

                combined_df.to_csv(output_file, index=False)
                self.logger.info(f"âœ… æŒ‡æ•°æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
                self.logger.info(f"   å…±æ›´æ–° {len(all_data)} æ¡è®°å½•")
                return True
            else:
                self.logger.warning("æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°æŒ‡æ•°æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return False

    def update_index_weight(self) -> bool:
        """
        å¢é‡æ›´æ–°æŒ‡æ•°æƒé‡æ•°æ®

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ›´æ–°æŒ‡æ•°æƒé‡æ•°æ®")
        self.logger.info("=" * 60)

        try:
            # è·å–éœ€è¦æ›´æ–°çš„æŒ‡æ•°åˆ—è¡¨
            index_list = self.config.get("index_weight_list", [
                "000905.SH",  # ä¸­è¯500
                "399300.SZ",  # æ²ªæ·±300
                "000906.SH",  # ä¸­è¯800
                "000852.SH",  # ä¸­è¯1000
                "000985.SH"   # ä¸­è¯å…¨æŒ‡
            ])

            self.logger.info(f"éœ€è¦æ›´æ–°æƒé‡çš„æŒ‡æ•°: {', '.join(index_list)}")

            # è·å–æœ€æ–°æ•°æ®æ—¥æœŸ
            latest_date = self.get_latest_date("index_weight")
            if latest_date:
                self.logger.info(f"æœ¬åœ°æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date}")
                start_date_obj = datetime.strptime(latest_date, "%Y%m%d")
                start_date = start_date_obj.strftime("%Y%m%d")
            else:
                self.logger.info("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®ï¼Œä¸‹è½½å…¨éƒ¨å†å²æ•°æ®")
                start_date = None

            end_date = datetime.now().strftime("%Y%m%d")

            all_data = []

            for index_code in index_list:
                self.logger.info(f"æ›´æ–°æŒ‡æ•° {index_code} çš„æƒé‡")

                try:
                    # è·å–æŒ‡æ•°åŸºæœ¬ä¿¡æ¯
                    if not start_date:
                        stock_basic = self.client.get_stock_basic()
                        index_info = stock_basic[stock_basic["ts_code"] == index_code]
                        if not index_info.empty:
                            list_date = index_info["list_date"].iloc[0]
                            start_date_obj = datetime.strptime(list_date, "%Y%m%d")
                        else:
                            start_date_obj = datetime.now() - timedelta(days=365)
                    else:
                        start_date_obj = datetime.strptime(start_date, "%Y%m%d")

                    # æŒ‰15å¤©æ­¥é•¿è·å–æ•°æ®
                    time_step = timedelta(days=15)
                    index_start_date = start_date_obj
                    index_end_date = index_start_date + time_step

                    empty_count = 0
                    index_data_list = []

                    while index_end_date < datetime.now():
                        try:
                            data = self.client.get_index_weight(
                                index_code=index_code,
                                start_date=index_start_date.strftime("%Y%m%d"),
                                end_date=index_end_date.strftime("%Y%m%d")
                            )

                            if not data.empty:
                                index_data_list.append(data)
                                empty_count = 0
                            else:
                                empty_count += 1
                                if empty_count >= 20:
                                    self.logger.warning(
                                        f"æŒ‡æ•° {index_code} è¿ç»­20æ¬¡æ— æ•°æ®ï¼Œåœæ­¢è·å–"
                                    )
                                    break

                            # é¿å…é¢‘ç‡é™åˆ¶
                            time.sleep(0.3)

                        except Exception as e:
                            self.logger.error(f"è·å– {index_code} æƒé‡å¤±è´¥: {e}")

                        index_start_date += time_step
                        index_end_date += time_step

                    if index_data_list:
                        combined_index_data = pd.concat(index_data_list, ignore_index=True)

                        # å­—æ®µæ˜ å°„
                        df = combined_index_data.rename(columns={
                            "con_code": "stock_code",
                            "index_code": "symbol",
                            "trade_date": "tradedate"
                        })

                        all_data.append(df)
                        self.stats["index_weights_updated"] += len(df)

                except Exception as e:
                    self.logger.error(f"æ›´æ–°æŒ‡æ•° {index_code} æƒé‡å¤±è´¥: {e}")
                    self.stats["errors"].append(f"index_weight_{index_code}: {str(e)}")
                    continue

            # ä¿å­˜æ•°æ®
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)

                output_file = self.data_dir / "index_weight.csv"

                # å¦‚æœå­˜åœ¨æ—§æ•°æ®ï¼Œåˆå¹¶åå»é‡
                if output_file.exists():
                    old_df = pd.read_csv(output_file)
                    combined_df = pd.concat([old_df, combined_df])
                    combined_df = combined_df.drop_duplicates(
                        subset=["tradedate", "symbol", "stock_code"],
                        keep="last"
                    )
                    combined_df = combined_df.sort_values(["tradedate", "symbol", "stock_code"])

                combined_df.to_csv(output_file, index=False)
                self.logger.info(f"âœ… æŒ‡æ•°æƒé‡æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
                self.logger.info(f"   å…±æ›´æ–° {len(all_data)} æ¡è®°å½•")
                return True
            else:
                self.logger.warning("æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°æŒ‡æ•°æƒé‡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return False

    def validate_data(self) -> bool:
        """
        éªŒè¯æ›´æ–°åçš„æ•°æ®è´¨é‡

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ•°æ®éªŒè¯")
        self.logger.info("=" * 60)

        validation_passed = True

        # éªŒè¯è‚¡ç¥¨æ•°æ®
        stock_file = self.data_dir / "stock_data.csv"
        if stock_file.exists():
            try:
                df = pd.read_csv(stock_file)
                self.logger.info(f"è‚¡ç¥¨æ•°æ®: {len(df)} æ¡è®°å½•")
                self.logger.info(f"  æ—¥æœŸèŒƒå›´: {df['tradedate'].min()} -> {df['tradedate'].max()}")
                self.logger.info(f"  è‚¡ç¥¨æ•°é‡: {df['symbol'].nunique()}")
            except Exception as e:
                self.logger.error(f"è‚¡ç¥¨æ•°æ®éªŒè¯å¤±è´¥: {e}")
                validation_passed = False

        # éªŒè¯æŒ‡æ•°æ•°æ®
        index_file = self.data_dir / "index_data.csv"
        if index_file.exists():
            try:
                df = pd.read_csv(index_file)
                self.logger.info(f"æŒ‡æ•°æ•°æ®: {len(df)} æ¡è®°å½•")
                self.logger.info(f"  æ—¥æœŸèŒƒå›´: {df['tradedate'].min()} -> {df['tradedate'].max()}")
                self.logger.info(f"  æŒ‡æ•°æ•°é‡: {df['symbol'].nunique()}")
            except Exception as e:
                self.logger.error(f"æŒ‡æ•°æ•°æ®éªŒè¯å¤±è´¥: {e}")
                validation_passed = False

        # éªŒè¯æŒ‡æ•°æƒé‡æ•°æ®
        weight_file = self.data_dir / "index_weight.csv"
        if weight_file.exists():
            try:
                df = pd.read_csv(weight_file)
                self.logger.info(f"æŒ‡æ•°æƒé‡æ•°æ®: {len(df)} æ¡è®°å½•")
                self.logger.info(f"  æ—¥æœŸèŒƒå›´: {df['tradedate'].min()} -> {df['tradedate'].max()}")
                self.logger.info(f"  æŒ‡æ•°æ•°é‡: {df['symbol'].nunique()}")
            except Exception as e:
                self.logger.error(f"æŒ‡æ•°æƒé‡æ•°æ®éªŒè¯å¤±è´¥: {e}")
                validation_passed = False

        if validation_passed:
            self.logger.info("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        else:
            self.logger.error("âŒ æ•°æ®éªŒè¯å¤±è´¥")

        return validation_passed

    def run(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çš„å¢é‡æ›´æ–°æµç¨‹

        Returns:
            æ›´æ–°æ˜¯å¦å…¨éƒ¨æˆåŠŸ
        """
        self.logger.info(f"é…ç½®ä¿¡æ¯:")
        self.logger.info(f"  æ•°æ®ç›®å½•: {self.data_dir}")
        self.logger.info(f"  å¼€å§‹æ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")

        try:
            # æ›´æ–°è‚¡ç¥¨æ•°æ®
            if self.config.get("update_stock", True):
                if not self.update_stock_data():
                    self.logger.error("è‚¡ç¥¨æ•°æ®æ›´æ–°å¤±è´¥")
                    return False

            # æ›´æ–°æŒ‡æ•°æ•°æ®
            if self.config.get("update_index", True):
                if not self.update_index_data():
                    self.logger.error("æŒ‡æ•°æ•°æ®æ›´æ–°å¤±è´¥")
                    return False

            # æ›´æ–°æŒ‡æ•°æƒé‡
            if self.config.get("update_index_weight", True):
                if not self.update_index_weight():
                    self.logger.error("æŒ‡æ•°æƒé‡æ›´æ–°å¤±è´¥")
                    return False

            # éªŒè¯æ•°æ®
            if self.config.get("validate_data", True):
                self.validate_data()

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self._print_summary()

            return True

        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}", exc_info=True)
            return False

    def _print_summary(self):
        """è¾“å‡ºæ›´æ–°ç»Ÿè®¡æ‘˜è¦"""
        elapsed_time = (datetime.now() - self.stats["start_time"]).total_seconds()

        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¢é‡æ›´æ–°å®Œæˆ - ç»Ÿè®¡æ‘˜è¦")
        self.logger.info("=" * 60)
        self.logger.info(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f} ç§’")
        self.logger.info(f"ğŸ“Š è‚¡ç¥¨æ•°æ®æ›´æ–°: {self.stats['stocks_updated']} æ¡")
        self.logger.info(f"ğŸ“ˆ æŒ‡æ•°æ•°æ®æ›´æ–°: {self.stats['indices_updated']} æ¡")
        self.logger.info(f"âš–ï¸  æŒ‡æ•°æƒé‡æ›´æ–°: {self.stats['index_weights_updated']} æ¡")

        if self.stats["errors"]:
            self.logger.warning(f"âš ï¸  é”™è¯¯æ•°é‡: {len(self.stats['errors'])}")
            for error in self.stats["errors"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                self.logger.warning(f"   - {error}")
            if len(self.stats["errors"]) > 5:
                self.logger.warning(f"   ... è¿˜æœ‰ {len(self.stats['errors']) - 5} ä¸ªé”™è¯¯")
        else:
            self.logger.info("âœ… æ²¡æœ‰é”™è¯¯")

        self.logger.info("=" * 60)


def load_config(config_path: str = None) -> Dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    if config_path is None:
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        script_dir = Path(__file__).parent
        config_path = script_dir / "tushare_incremental_config.yaml"

    config_file = Path(config_path)

    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    else:
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "data_dir": "~/.qlib/qlib_data/cn_data",
            "max_retries": 3,
            "retry_delay": 1.0,
            "rate_limit": 200,
            "log_level": "INFO",
            "enable_api_logging": False,
            "update_stock": True,
            "update_index": True,
            "update_index_weight": True,
            "validate_data": True,
            "stock_start_date": "20200101",
            "index_start_date": "20200101",
            "index_list": [
                "399300.SZ",  # æ²ªæ·±300
                "000905.SH",  # ä¸­è¯500
                "000300.SH",  # æ²ªæ·±300
                "000906.SH",  # ä¸­è¯800
                "000852.SH",  # ä¸­è¯1000
                "000985.SH"   # ä¸­è¯å…¨æŒ‡
            ],
            "index_weight_list": [
                "000905.SH",  # ä¸­è¯500
                "399300.SZ",  # æ²ªæ·±300
                "000906.SH",  # ä¸­è¯800
                "000852.SH",  # ä¸­è¯1000
                "000985.SH"   # ä¸­è¯å…¨æŒ‡
            ]
        }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("TuShare Aè‚¡æ•°æ®å¢é‡æ›´æ–°è„šæœ¬")
    print("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("TUSHARE_TOKEN"):
        print("âŒ é”™è¯¯: æœªè®¾ç½® TUSHARE_TOKEN ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½® TuShare API Token:")
        print("  export TUSHARE_TOKEN='your_token_here'")
        sys.exit(1)

    # åŠ è½½é…ç½®
    config = load_config()

    # åˆ›å»ºæ›´æ–°å™¨å¹¶æ‰§è¡Œæ›´æ–°
    updater = IncrementalDataUpdater(config)

    success = updater.run()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
