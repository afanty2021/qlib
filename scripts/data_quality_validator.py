#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡éªŒè¯å·¥å…·

å€Ÿé‰´ investment_data é¡¹ç›®çš„éªŒè¯æœºåˆ¶ï¼Œæä¾›æœ¬åœ°æ•°æ®è´¨é‡æ£€æŸ¥åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- å¤šæ•°æ®æºäº¤å‰éªŒè¯ï¼ˆTuShare vs Yahooï¼‰
- ä»·æ ¼èŒƒå›´éªŒè¯
- æˆäº¤é‡éªŒè¯
- å¤æƒå› å­éªŒè¯
- äº¤æ˜“æ—¥å†éªŒè¯
- æ•°æ®å®Œæ•´æ€§éªŒè¯

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/data_quality_validator.py
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import warnings

import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings('ignore')


class DataQualityValidator:
    """
    æ•°æ®è´¨é‡éªŒè¯å™¨

    æä¾›å…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥åŠŸèƒ½ã€‚
    """

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–éªŒè¯å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•
        """
        self.data_dir = Path(data_dir or "~/.qlib/qlib_data/cn_data").expanduser()
        self.logger = self._setup_logger()

        # éªŒè¯è§„åˆ™é…ç½®
        self.rules = {
            "price": {
                "min_price": 0.01,        # æœ€ä½ä»·æ ¼ï¼ˆé¿å…é™¤æƒåä»·æ ¼ä¸º0ï¼‰
                "max_price": 10000,       # æœ€é«˜ä»·æ ¼
                "max_change_pct": 0.50,    # æœ€å¤§å•æ—¥æ¶¨è·Œå¹…ï¼ˆ50%ï¼‰
                "allow_zero": False       # æ˜¯å¦å…è®¸ä»·æ ¼ä¸º0
            },
            "volume": {
                "min_volume": 0,          # æœ€å°æˆäº¤é‡
                "amount_tolerance": 0.05,  # æˆäº¤é¢ä¸æˆäº¤é‡Ã—ä»·æ ¼çš„è¯¯å·®å®¹å¿åº¦
            },
            "completeness": {
                "min_stocks_per_day": 1000,  # æ¯æ—¥æœ€å°‘è‚¡ç¥¨æ•°
                "max_missing_pct": 0.10,     # æœ€å¤§ç¼ºå¤±æ¯”ä¾‹
            }
        }

        self.validation_results = {
            "total_records": 0,
            "total_errors": 0,
            "errors_by_type": {},
            "error_details": []
        }

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("DataQualityValidator")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def load_data(self, file_name: str = "stock_data.csv") -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®æ–‡ä»¶

        Args:
            file_name: æ•°æ®æ–‡ä»¶å

        Returns:
            æ•°æ® DataFrame
        """
        file_path = self.data_dir / file_name

        if not file_path.exists():
            self.logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()

        try:
            df = pd.read_csv(file_path)
            self.logger.info(f"âœ… åŠ è½½æ•°æ®: {file_path} ({len(df):,} è¡Œ)")
            return df
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def validate_price_range(self, df: pd.DataFrame) -> List[Dict]:
        """
        éªŒè¯ä»·æ ¼èŒƒå›´

        æ£€æŸ¥é¡¹ï¼š
        1. ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°
        2. ä»·æ ¼ä¸èƒ½ä¸ºé›¶ï¼ˆé™¤éé…ç½®å…è®¸ï¼‰
        3. æœ€é«˜ä»· >= æœ€ä½ä»·
        4. æ”¶ç›˜ä»·åœ¨ [æœ€ä½ä»·, æœ€é«˜ä»·] èŒƒå›´å†…
        5. ä»·æ ¼åœ¨åˆç†èŒƒå›´å†…

        Args:
            df: æ•°æ® DataFrame

        Returns:
            é”™è¯¯åˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("éªŒè¯ä»·æ ¼èŒƒå›´")
        self.logger.info("=" * 60)

        errors = []

        for idx, row in df.iterrows():
            # æ£€æŸ¥è´Ÿä»·æ ¼
            if row["close"] < 0:
                errors.append({
                    "type": "negative_price",
                    "severity": "critical",
                    "row_idx": idx,
                    "tradedate": row.get("tradedate", "N/A"),
                    "symbol": row.get("symbol", "N/A"),
                    "field": "close",
                    "value": row["close"],
                    "message": "æ”¶ç›˜ä»·ä¸èƒ½ä¸ºè´Ÿæ•°"
                })

            # æ£€æŸ¥é›¶ä»·æ ¼
            if row["close"] == 0 and not self.rules["price"]["allow_zero"]:
                errors.append({
                    "type": "zero_price",
                    "severity": "warning",
                    "row_idx": idx,
                    "tradedate": row.get("tradedate", "N/A"),
                    "symbol": row.get("symbol", "N/A"),
                    "field": "close",
                    "value": row["close"],
                    "message": "æ”¶ç›˜ä»·ä¸ºé›¶ï¼ˆå¯èƒ½éœ€è¦å¤æƒï¼‰"
                })

            # æ£€æŸ¥ä»·æ ¼èŒƒå›´
            if row["close"] < self.rules["price"]["min_price"]:
                errors.append({
                    "type": "price_too_low",
                    "severity": "warning",
                    "row_idx": idx,
                    "tradedate": row.get("tradedate", "N/A"),
                    "symbol": row.get("symbol", "N/A"),
                    "field": "close",
                    "value": row["close"],
                    "min_allowed": self.rules["price"]["min_price"],
                    "message": f"ä»·æ ¼ä½äºæœ€å°å€¼ {self.rules['price']['min_price']}"
                })

            if row["close"] > self.rules["price"]["max_price"]:
                errors.append({
                    "type": "price_too_high",
                    "severity": "warning",
                    "row_idx": idx,
                    "tradedate": row.get("tradedate", "N/A"),
                    "symbol": row.get("symbol", "N/A"),
                    "field": "close",
                    "value": row["close"],
                    "max_allowed": self.rules["price"]["max_price"],
                    "message": f"ä»·æ ¼é«˜äºæœ€å¤§å€¼ {self.rules['price']['max_price']}"
                })

            # æ£€æŸ¥é«˜ä½ä»·å…³ç³»
            if pd.notna(row["high"]) and pd.notna(row["low"]):
                if row["high"] < row["low"]:
                    errors.append({
                        "type": "high_less_than_low",
                        "severity": "critical",
                        "row_idx": idx,
                        "tradedate": row.get("tradedate", "N/A"),
                        "symbol": row.get("symbol", "N/A"),
                        "high": row["high"],
                        "low": row["low"],
                        "message": "æœ€é«˜ä»·å°äºæœ€ä½ä»·"
                    })

                # æ£€æŸ¥æ”¶ç›˜ä»·æ˜¯å¦åœ¨èŒƒå›´å†…
                if pd.notna(row["close"]):
                    if not (row["low"] <= row["close"] <= row["high"]):
                        errors.append({
                            "type": "close_out_of_range",
                            "severity": "critical",
                            "row_idx": idx,
                            "tradedate": row.get("tradedate", "N/A"),
                            "symbol": row.get("symbol", "N/A"),
                            "close": row["close"],
                            "low": row["low"],
                            "high": row["high"],
                            "message": "æ”¶ç›˜ä»·ä¸åœ¨[æœ€ä½ä»·, æœ€é«˜ä»·]èŒƒå›´å†…"
                        })

        self.logger.info(f"å‘ç° {len(errors)} ä¸ªä»·æ ¼èŒƒå›´é”™è¯¯")
        return errors

    def validate_volume(self, df: pd.DataFrame) -> List[Dict]:
        """
        éªŒè¯æˆäº¤é‡

        æ£€æŸ¥é¡¹ï¼š
        1. æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°
        2. æˆäº¤é¢åº”è¯¥çº¦ç­‰äº æˆäº¤é‡ Ã— æ”¶ç›˜ä»·

        Args:
            df: æ•°æ® DataFrame

        Returns:
            é”™è¯¯åˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("éªŒè¯æˆäº¤é‡")
        self.logger.info("=" * 60)

        errors = []

        for idx, row in df.iterrows():
            # æ£€æŸ¥è´Ÿæˆäº¤é‡
            if pd.notna(row["volume"]) and row["volume"] < 0:
                errors.append({
                    "type": "negative_volume",
                    "severity": "critical",
                    "row_idx": idx,
                    "tradedate": row.get("tradedate", "N/A"),
                    "symbol": row.get("symbol", "N/A"),
                    "volume": row["volume"],
                    "message": "æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°"
                })

            # æ£€æŸ¥æˆäº¤é¢ä¸æˆäº¤é‡çš„å…³ç³»
            if pd.notna(row["volume"]) and pd.notna(row["amount"]) and pd.notna(row["close"]):
                # ä¼°ç®—æˆäº¤é¢ = æˆäº¤é‡ Ã— æ”¶ç›˜ä»·
                estimated_amount = row["volume"] * row["close"]

                if row["amount"] > 0:
                    error_pct = abs(row["amount"] - estimated_amount) / row["amount"]

                    if error_pct > self.rules["volume"]["amount_tolerance"]:
                        errors.append({
                            "type": "amount_mismatch",
                            "severity": "warning",
                            "row_idx": idx,
                            "tradedate": row.get("tradedate", "N/A"),
                            "symbol": row.get("symbol", "N/A"),
                            "actual_amount": row["amount"],
                            "estimated_amount": estimated_amount,
                            "error_pct": error_pct,
                            "message": f"æˆäº¤é¢ä¸ä¼°ç®—å€¼è¯¯å·® {error_pct:.2%}"
                        })

        self.logger.info(f"å‘ç° {len(errors)} ä¸ªæˆäº¤é‡é”™è¯¯")
        return errors

    def validate_completeness(self, df: pd.DataFrame) -> List[Dict]:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§

        æ£€æŸ¥é¡¹ï¼š
        1. æ¯æ—¥è‚¡ç¥¨æ•°é‡æ˜¯å¦åˆç†
        2. ç¼ºå¤±å€¼æ£€æŸ¥
        3. é‡å¤è®°å½•æ£€æŸ¥

        Args:
            df: æ•°æ® DataFrame

        Returns:
            é”™è¯¯åˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("éªŒè¯æ•°æ®å®Œæ•´æ€§")
        self.logger.info("=" * 60)

        errors = []

        # æ£€æŸ¥æ¯æ—¥è‚¡ç¥¨æ•°é‡
        if "tradedate" in df.columns:
            daily_counts = df.groupby("tradedate")["symbol"].nunique()

            for tradedate, count in daily_counts.items():
                if count < self.rules["completeness"]["min_stocks_per_day"]:
                    errors.append({
                        "type": "insufficient_stocks",
                        "severity": "warning",
                        "tradedate": tradedate,
                        "stock_count": count,
                        "min_required": self.rules["completeness"]["min_stocks_per_day"],
                        "message": f"äº¤æ˜“æ—¥æœŸ {tradedate} è‚¡ç¥¨æ•°é‡ä¸è¶³: {count} < {self.rules['completeness']['min_stocks_per_day']}"
                    })

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_counts = df.isnull().sum()
        for column, count in missing_counts.items():
            if count > 0:
                missing_pct = count / len(df)
                if missing_pct > self.rules["completeness"]["max_missing_pct"]:
                    errors.append({
                        "type": "too_many_missing",
                        "severity": "warning",
                        "column": column,
                        "missing_count": count,
                        "missing_pct": missing_pct,
                        "message": f"å­—æ®µ {column} ç¼ºå¤±å€¼è¿‡å¤š: {missing_pct:.2%}"
                    })

        # æ£€æŸ¥é‡å¤è®°å½•
        if "tradedate" in df.columns and "symbol" in df.columns:
            duplicates = df.duplicated(subset=["tradedate", "symbol"], keep=False)
            dup_count = duplicates.sum()

            if dup_count > 0:
                dup_samples = df[duplicates][["tradedate", "symbol"]].head(10).to_dict('records')
                errors.append({
                    "type": "duplicate_records",
                    "severity": "error",
                    "duplicate_count": dup_count,
                    "samples": dup_samples,
                    "message": f"å‘ç° {dup_count} æ¡é‡å¤è®°å½•ï¼ˆtradedate + symbolï¼‰"
                })

        self.logger.info(f"å‘ç° {len(errors)} ä¸ªå®Œæ•´æ€§é”™è¯¯")
        return errors

    def validate_price_continuity(self, df: pd.DataFrame) -> List[Dict]:
        """
        éªŒè¯ä»·æ ¼è¿ç»­æ€§

        æ£€æŸ¥é¡¹ï¼š
        1. æ£€æµ‹å¼‚å¸¸çš„ä»·æ ¼è·³å˜ï¼ˆé™¤æƒé™¤æ¯å¤–ï¼‰
        2. æ£€æµ‹åœç‰Œåä»·æ ¼è¿ç»­æ€§

        Args:
            df: æ•°æ® DataFrame

        Returns:
            é”™è¯¯åˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("éªŒè¯ä»·æ ¼è¿ç»­æ€§")
        self.logger.info("=" * 60)

        errors = []

        if "tradedate" not in df.columns or "symbol" not in df.columns:
            self.logger.warning("âš ï¸  æ•°æ®ç¼ºå°‘ tradedate æˆ– symbol å­—æ®µï¼Œè·³è¿‡è¿ç»­æ€§éªŒè¯")
            return errors

        # æŒ‰è‚¡ç¥¨åˆ†ç»„æ£€æŸ¥
        for symbol in df["symbol"].unique():
            symbol_data = df[df["symbol"] == symbol].sort_values("tradedate")

            # è®¡ç®—ä»·æ ¼å˜åŒ–
            symbol_data["prev_close"] = symbol_data["close"].shift(1)
            symbol_data["change_pct"] = symbol_data["close"].pct_change()

            # æ£€æµ‹å¼‚å¸¸å˜åŒ–ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰
            max_change = self.rules["price"]["max_change_pct"]
            abnormal_changes = symbol_data[
                (symbol_data["change_pct"].abs() > max_change) &
                (symbol_data["change_pct"].notna())
            ]

            for idx, row in abnormal_changes.iterrows():
                errors.append({
                    "type": "abnormal_price_change",
                    "severity": "warning",
                    "symbol": symbol,
                    "tradedate": row["tradedate"],
                    "prev_close": row["prev_close"],
                    "current_close": row["close"],
                    "change_pct": row["change_pct"],
                    "message": f"å•æ—¥ä»·æ ¼å˜åŒ– {row['change_pct']:.2%}ï¼Œå¯èƒ½æ˜¯é™¤æƒé™¤æ¯æˆ–æ•°æ®é”™è¯¯"
                })

        self.logger.info(f"å‘ç° {len(errors)} ä¸ªä»·æ ¼è¿ç»­æ€§å¼‚å¸¸")
        return errors

    def validate_trading_calendar(self, df: pd.DataFrame) -> List[Dict]:
        """
        éªŒè¯äº¤æ˜“æ—¥å†

        æ£€æŸ¥é¡¹ï¼š
        1. æ£€æµ‹å‘¨æœ«æ•°æ®ï¼ˆé™¤éæ˜¯ç‰¹æ®Šæƒ…å†µï¼‰
        2. æ£€æµ‹èŠ‚å‡æ—¥æ•°æ®

        Args:
            df: æ•°æ® DataFrame

        Returns:
            é”™è¯¯åˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("éªŒè¯äº¤æ˜“æ—¥å†")
        self.logger.info("=" * 60)

        errors = []

        if "tradedate" not in df.columns:
            self.logger.warning("âš ï¸  æ•°æ®ç¼ºå°‘ tradedate å­—æ®µï¼Œè·³è¿‡äº¤æ˜“æ—¥å†éªŒè¯")
            return errors

        # æ£€æŸ¥æ—¥æœŸæ ¼å¼
        try:
            df["tradedate_dt"] = pd.to_datetime(df["tradedate"], format="%Y%m%d", errors="coerce")

            # æ£€æŸ¥æ— æ•ˆæ—¥æœŸ
            invalid_dates = df[df["tradedate_dt"].isna()]
            if len(invalid_dates) > 0:
                errors.append({
                    "type": "invalid_date_format",
                    "severity": "error",
                    "count": len(invalid_dates),
                    "samples": invalid_dates["tradedate"].head(10).tolist(),
                    "message": f"å‘ç° {len(invalid_dates)} æ¡æ— æ•ˆæ—¥æœŸæ ¼å¼"
                })

            # æ£€æŸ¥å‘¨æœ«ï¼ˆå‘¨å…­ã€å‘¨æ—¥ï¼‰
            if len(df) > 0:
                df["day_of_week"] = df["tradedate_dt"].dt.dayofweek
                weekend_data = df[df["day_of_week"] >= 5]  # 5=å‘¨å…­, 6=å‘¨æ—¥

                if len(weekend_data) > 0:
                    errors.append({
                        "type": "weekend_data",
                        "severity": "warning",
                        "count": len(weekend_data),
                        "dates": weekend_data["tradedate"].unique().tolist()[:10],
                        "message": f"å‘ç° {len(weekend_data)} æ¡å‘¨æœ«æ•°æ®ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸çš„è¡¥æ•°æ®ï¼‰"
                    })

        except Exception as e:
            errors.append({
                "type": "date_parsing_error",
                "severity": "error",
                "message": f"æ—¥æœŸè§£æå¤±è´¥: {str(e)}"
            })

        self.logger.info(f"å‘ç° {len(errors)} ä¸ªäº¤æ˜“æ—¥å†é—®é¢˜")
        return errors

    def run_validation(self, file_name: str = "stock_data.csv") -> Dict:
        """
        è¿è¡Œå®Œæ•´éªŒè¯

        Args:
            file_name: æ•°æ®æ–‡ä»¶å

        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("å¼€å§‹æ•°æ®è´¨é‡éªŒè¯")
        self.logger.info("=" * 60)

        # åŠ è½½æ•°æ®
        df = self.load_data(file_name)

        if df.empty:
            self.logger.error("âŒ æ•°æ®ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")
            return {"success": False, "errors": []}

        self.validation_results["total_records"] = len(df)

        # æ‰§è¡Œå„é¡¹éªŒè¯
        all_errors = []

        all_errors.extend(self.validate_price_range(df))
        all_errors.extend(self.validate_volume(df))
        all_errors.extend(self.validate_completeness(df))
        all_errors.extend(self.validate_price_continuity(df))
        all_errors.extend(self.validate_trading_calendar(df))

        # æ±‡æ€»ç»“æœ
        self.validation_results["total_errors"] = len(all_errors)
        self.validation_results["errors_by_type"] = {}
        self.validation_results["error_details"] = all_errors

        for error in all_errors:
            error_type = error["type"]
            if error_type not in self.validation_results["errors_by_type"]:
                self.validation_results["errors_by_type"][error_type] = 0
            self.validation_results["errors_by_type"][error_type] += 1

        # æ‰“å°éªŒè¯æŠ¥å‘Š
        self._print_validation_report()

        return {
            "success": len([e for e in all_errors if e.get("severity") == "critical"]) == 0,
            "errors": all_errors,
            "summary": self.validation_results
        }

    def _print_validation_report(self):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("æ•°æ®è´¨é‡éªŒè¯æŠ¥å‘Š")
        print("=" * 60)

        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {self.validation_results['total_records']:,}")
        print(f"   é”™è¯¯æ€»æ•°: {self.validation_results['total_errors']}")

        if self.validation_results["errors_by_type"]:
            print(f"\nğŸ” é”™è¯¯åˆ†ç±»:")
            for error_type, count in sorted(
                self.validation_results["errors_by_type"].items(),
                key=lambda x: x[1],
                reverse=True
            ):
                print(f"   - {error_type}: {count}")

        # ç»Ÿè®¡ä¸¥é‡çº§åˆ«
        severity_counts = {}
        for error in self.validation_results["error_details"]:
            severity = error.get("severity", "unknown")
            if severity not in severity_counts:
                severity_counts[severity] = 0
            severity_counts[severity] += 1

        print(f"\nâš ï¸  ä¸¥é‡çº§åˆ«:")
        for severity, count in sorted(severity_counts.items()):
            print(f"   - {severity}: {count}")

        # æ˜¾ç¤ºéƒ¨åˆ†é”™è¯¯è¯¦æƒ…
        if self.validation_results["error_details"]:
            print(f"\nğŸ“ é”™è¯¯è¯¦æƒ…ï¼ˆå‰20ä¸ªï¼‰:")
            for i, error in enumerate(self.validation_results["error_details"][:20], 1):
                print(f"   {i}. [{error.get('severity', 'unknown')}] {error.get('message', 'N/A')}")
                if "tradedate" in error and "symbol" in error:
                    print(f"      æ—¥æœŸ: {error['tradedate']}, è‚¡ç¥¨: {error['symbol']}")

        # åˆ¤æ–­éªŒè¯ç»“æœ
        critical_errors = [
            e for e in self.validation_results["error_details"]
            if e.get("severity") == "critical"
        ]

        print("\n" + "=" * 60)
        if len(critical_errors) == 0:
            print("âœ… éªŒè¯é€šè¿‡ï¼šæ²¡æœ‰ä¸¥é‡é”™è¯¯")
        else:
            print(f"âŒ éªŒè¯å¤±è´¥ï¼šå‘ç° {len(critical_errors)} ä¸ªä¸¥é‡é”™è¯¯")
        print("=" * 60)

    def export_errors_to_csv(self, output_file: str = None):
        """
        å¯¼å‡ºé”™è¯¯åˆ° CSV æ–‡ä»¶

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.validation_results["error_details"]:
            self.logger.info("æ²¡æœ‰é”™è¯¯éœ€è¦å¯¼å‡º")
            return

        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.data_dir / f"validation_errors_{timestamp}.csv"

        # è½¬æ¢ä¸º DataFrame
        errors_df = pd.DataFrame(self.validation_results["error_details"])
        errors_df.to_csv(output_file, index=False)

        self.logger.info(f"âœ… é”™è¯¯å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ•°æ®è´¨é‡éªŒè¯å·¥å…·")
    parser.add_argument(
        "--data-dir",
        default="~/.qlib/qlib_data/cn_data",
        help="æ•°æ®ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--file",
        default="stock_data.csv",
        help="æ•°æ®æ–‡ä»¶å"
    )

    args = parser.parse_args()

    # åˆ›å»ºéªŒè¯å™¨
    validator = DataQualityValidator(data_dir=args.data_dir)

    # è¿è¡ŒéªŒè¯
    result = validator.run_validation(file_name=args.file)

    # å¯¼å‡ºé”™è¯¯
    if result["errors"]:
        validator.export_errors_to_csv()

    return 0 if result["success"] else 1


if __name__ == "__main__":
    sys.exit(main())
