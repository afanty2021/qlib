[æ ¹ç›®å½•](../../CLAUDE.md) > **éƒ¨ç½²è¿ç»´æŒ‡å—**

# Qlib éƒ¨ç½²è¿ç»´ä¸æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> æœ¬æ–‡æ¡£æä¾› Qlib é¡¹ç›®çš„å®Œæ•´éƒ¨ç½²è¿ç»´æ–¹æ¡ˆå’Œç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œæ¶µç›–å¼€å‘ç¯å¢ƒæ­å»ºã€ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€ç›‘æ§è¿ç»´å’Œæ€§èƒ½è°ƒä¼˜çš„æœ€ä½³å®è·µã€‚

## éƒ¨ç½²æ¶æ„æ¦‚è§ˆ

### ç¯å¢ƒåˆ†å±‚æ¶æ„

```mermaid
graph TD
    A["å¼€å‘ç¯å¢ƒ"] --> A1["æœ¬åœ°å¼€å‘"];
    A --> A2["Docker å®¹å™¨"];
    A --> A3["è™šæ‹Ÿç¯å¢ƒ"];

    B["æµ‹è¯•ç¯å¢ƒ"] --> B1["å•å…ƒæµ‹è¯•"];
    B --> B2["é›†æˆæµ‹è¯•"];
    B --> B3["æ€§èƒ½æµ‹è¯•"];

    C["ç”Ÿäº§ç¯å¢ƒ"] --> C1["åœ¨çº¿é¢„æµ‹æœåŠ¡"];
    C --> C2["æ‰¹é‡å›æµ‹ç³»ç»Ÿ"];
    C --> C3["å®æ—¶æ•°æ®å¤„ç†"];

    D["è¿ç»´ç›‘æ§"] --> D1["ç³»ç»Ÿç›‘æ§"];
    D --> D2["æ€§èƒ½ç›‘æ§"];
    D --> D3["æ—¥å¿—ç›‘æ§"];

    A --> B;
    B --> C;
    C --> D;
```

## å¼€å‘ç¯å¢ƒéƒ¨ç½²

### æœ¬åœ°å¼€å‘ç¯å¢ƒæ­å»º

#### 1. åŸºç¡€ç¯å¢ƒè¦æ±‚
```bash
# æ“ä½œç³»ç»Ÿè¦æ±‚
- Ubuntu 18.04+ / CentOS 7+ / macOS 10.14+
- Python 3.8+ (æ¨è 3.9+)
- å†…å­˜: 8GB+ (æ¨è 16GB+)
- å­˜å‚¨: 50GB+ å¯ç”¨ç©ºé—´
- GPU: NVIDIA GTX 1060+ (å¯é€‰ï¼Œç”¨äºæ·±åº¦å­¦ä¹ )
```

#### 2. å¿«é€Ÿå®‰è£…æµç¨‹
```bash
# æ­¥éª¤ 1: å…‹éš†é¡¹ç›®
git clone https://github.com/microsoft/qlib.git
cd qlib

# æ­¥éª¤ 2: åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n qlib_env python=3.9
conda activate qlib_env

# æ­¥éª¤ 3: å®‰è£…ä¾èµ–
pip install -e ".[dev]"

# æ­¥éª¤ 4: åˆå§‹åŒ– Qlib
python -c "import qlib; qlib.init()"

# æ­¥éª¤ 5: éªŒè¯å®‰è£…
cd examples
python workflow_by_code.py
```

#### 3. å¼€å‘å·¥å…·é…ç½®
```bash
# ä»£ç æ ¼å¼åŒ–å·¥å…·
pip install black isort flake8 mypy

# Jupyter ç¯å¢ƒé…ç½®
pip install jupyterlab ipywidgets

# IDE é…ç½® (VS Code)
# å®‰è£…æ‰©å±•: Python, Pylance, Jupyter
```

### Docker å®¹å™¨åŒ–éƒ¨ç½²

#### 1. Dockerfile åˆ†æ
```dockerfile
# åŸºç¡€é•œåƒé€‰æ‹©
FROM continuumio/miniconda3:latest

# å·¥ä½œç›®å½•è®¾ç½®
WORKDIR /qlib

# ä¾èµ–å®‰è£…æµç¨‹
COPY . .
RUN apt-get update && apt-get install -y build-essential

# Conda ç¯å¢ƒåˆ›å»º
RUN conda create --name qlib_env python=3.8
ENV PATH /opt/conda/envs/qlib_env/bin:$PATH

# Python åŒ…å®‰è£…
RUN python -m pip install --upgrade pip
RUN pip install numpy pandas scikit-learn
RUN pip install cython pybind11 cvxpy

# Qlib å®‰è£… (æ ¹æ®å‚æ•°é€‰æ‹©)
ARG IS_STABLE=true
RUN if [ "$IS_STABLE" = "true" ]; then \
        pip install pyqlib; \
    else \
        python setup.py install; \
    fi
```

#### 2. æ„å»ºå’Œè¿è¡Œå®¹å™¨
```bash
# æ„å»ºé•œåƒ
docker build -t qlib:latest .

# è¿è¡Œå®¹å™¨
docker run -it --rm \
    -v $(pwd)/data:/qlib/data \
    -p 8888:8888 \
    qlib:latest

# æŒ‚è½½å·å¼€å‘æ¨¡å¼
docker run -it --rm \
    -v $(pwd):/qlib \
    -v $(pwd)/data:/qlib/.qlib/qlib_data \
    -p 8888:8888 \
    qlib:latest jupyter lab --ip=0.0.0.0
```

#### 3. Docker Compose éƒ¨ç½²
```yaml
version: '3.8'

services:
  qlib-jupyter:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./data:/qlib/.qlib/qlib_data
      - ./notebooks:/qlib/notebooks
    environment:
      - PYTHONPATH=/qlib
    command: jupyter lab --ip=0.0.0.0 --allow-root

  qlib-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/qlib/.qlib/qlib_data
      - ./models:/qlib/models
    environment:
      - QLIB_MODE=online
    command: python -m qlib.contrib.online.serve

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ç³»ç»Ÿæ¶æ„è®¾è®¡

#### 1. é«˜å¯ç”¨æ¶æ„
```mermaid
graph TD
    A["è´Ÿè½½å‡è¡¡å™¨ Nginx"] --> B["API Gateway 1"];
    A --> C["API Gateway 2"];

    B --> D["Qlib Service 1"];
    B --> E["Qlib Service 2"];
    C --> F["Qlib Service 3"];
    C --> G["Qlib Service 4"];

    D --> H["Redis Cluster"];
    E --> H;
    F --> H;
    G --> H;

    H --> I["PostgreSQL"];
    H --> J["MinIO Storage"];

    K["ç›‘æ§ç³»ç»Ÿ Prometheus"] --> L["Grafana"];
    L --> M["å‘Šè­¦ç³»ç»Ÿ"];
```

#### 2. å¾®æœåŠ¡éƒ¨ç½²æ¶æ„
```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qlib-prediction-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: qlib-prediction
  template:
    metadata:
      labels:
        app: qlib-prediction
    spec:
      containers:
      - name: qlib-prediction
        image: qlib:production
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: DB_URL
          value: "postgresql://user:pass@postgres:5432/qlib"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
```

### åœ¨çº¿é¢„æµ‹æœåŠ¡éƒ¨ç½²

#### 1. æœåŠ¡é…ç½®
```python
# config/production.py
import os

class ProductionConfig:
    # æ•°æ®åº“é…ç½®
    DATABASE_URL = os.getenv("DB_URL", "postgresql://localhost/qlib")

    # Redis é…ç½®
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")

    # æ¨¡å‹é…ç½®
    MODEL_PATH = "/app/models"
    MODEL_VERSION = "v1.0.0"

    # æ€§èƒ½é…ç½®
    WORKERS = 4
    MAX_CONNECTIONS = 100
    REQUEST_TIMEOUT = 30

    # ç›‘æ§é…ç½®
    METRICS_ENABLED = True
    LOG_LEVEL = "INFO"

    # ç¼“å­˜é…ç½®
    CACHE_TTL = 300  # 5åˆ†é’Ÿ
    CACHE_SIZE = 1000
```

#### 2. æœåŠ¡å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# scripts/start_production.sh

set -e

echo "Starting Qlib Production Service..."

# ç¯å¢ƒæ£€æŸ¥
python -c "import qlib; print('Qlib version:', qlib.__version__)"

# æ¨¡å‹é¢„åŠ è½½
python scripts/preload_models.py

# å¯åŠ¨æœåŠ¡
gunicorn --bind 0.0.0.0:8000 \
         --workers $WORKERS \
         --worker-class uvicorn.workers.UvicornWorker \
         --timeout 120 \
         --access-logfile - \
         --error-logfile - \
         qlib.contrib.online.app:app

echo "Qlib Production Service Started Successfully!"
```

### æ•°æ®å­˜å‚¨æ¶æ„

#### 1. åˆ†å¸ƒå¼å­˜å‚¨æ–¹æ¡ˆ
```python
# config/storage.py
class StorageConfig:
    # æ–‡ä»¶å­˜å‚¨ (MinIO/S3)
    S3_ENDPOINT = "s3.amazonaws.com"
    S3_BUCKET = "qlib-data"
    S3_ACCESS_KEY = os.getenv("S3_ACCESS_KEY")
    S3_SECRET_KEY = os.getenv("S3_SECRET_KEY")

    # æ—¶åºæ•°æ®åº“ (InfluxDB)
    INFLUXDB_URL = "http://influxdb:8086"
    INFLUXDB_DB = "qlib_metrics"

    # ç¼“å­˜é…ç½®
    REDIS_CLUSTER = [
        "redis://redis-node-1:6379",
        "redis://redis-node-2:6379",
        "redis://redis-node-3:6379"
    ]
```

#### 2. æ•°æ®å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# scripts/backup_data.sh

BACKUP_DIR="/backup/qlib_$(date +%Y%m%d_%H%M%S)"
mkdir -p $BACKUP_DIR

# æ¨¡å‹å¤‡ä»½
tar -czf $BACKUP_DIR/models.tar.gz /app/models/

# æ•°æ®å¤‡ä»½
pg_dump qlib_production | gzip > $BACKUP_DIR/database.sql.gz

# é…ç½®å¤‡ä»½
cp -r /app/config $BACKUP_DIR/

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨
aws s3 sync $BACKUP_DIR s3://qlib-backups/$(date +%Y%m%d)/

# æ¸…ç†æœ¬åœ°å¤‡ä»½
rm -rf $BACKUP_DIR

echo "Backup completed: $BACKUP_DIR"
```

## ç›‘æ§è¿ç»´ä½“ç³»

### ç³»ç»Ÿç›‘æ§

#### 1. Prometheus ç›‘æ§é…ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'qlib-prediction'
    static_configs:
      - targets: ['qlib-service:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
```

#### 2. å…³é”®æŒ‡æ ‡ç›‘æ§
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# è¯·æ±‚æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'qlib_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'qlib_request_duration_seconds',
    'Request duration',
    ['method', 'endpoint']
)

# æ¨¡å‹æŒ‡æ ‡
MODEL_PREDICTION_COUNT = Counter(
    'qlib_model_predictions_total',
    'Total model predictions',
    ['model_name', 'model_version']
)

MODEL_ACCURACY = Gauge(
    'qlib_model_accuracy',
    'Model accuracy',
    ['model_name']
)

# ç³»ç»ŸæŒ‡æ ‡
ACTIVE_CONNECTIONS = Gauge(
    'qlib_active_connections',
    'Active connections'
)

MEMORY_USAGE = Gauge(
    'qlib_memory_usage_bytes',
    'Memory usage in bytes'
)
```

#### 3. Grafana ä»ªè¡¨æ¿
```json
{
  "dashboard": {
    "title": "Qlib Service Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(qlib_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, qlib_request_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Model Performance",
        "type": "singlestat",
        "targets": [
          {
            "expr": "qlib_model_accuracy",
            "legendFormat": "{{model_name}}"
          }
        ]
      }
    ]
  }
}
```

### æ—¥å¿—ç®¡ç†

#### 1. ç»“æ„åŒ–æ—¥å¿—é…ç½®
```python
# config/logging.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }

        if hasattr(record, 'request_id'):
            log_obj["request_id"] = record.request_id

        if hasattr(record, 'user_id'):
            log_obj["user_id"] = record.user_id

        return json.dumps(log_obj)

# æ—¥å¿—é…ç½®
LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            '()': JSONFormatter,
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'formatter': 'json',
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/app/logs/qlib.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json',
        },
    },
    'loggers': {
        'qlib': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': False,
        },
    },
}
```

#### 2. ELK æ—¥å¿—æ”¶é›†
```yaml
# logging/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "qlib" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    mutate {
      add_tag => ["qlib"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "qlib-logs-%{+YYYY.MM.dd}"
  }
}
```

### å‘Šè­¦ç³»ç»Ÿ

#### 1. å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# monitoring/alerts.yml
groups:
  - name: qlib_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(qlib_requests_total{status!~"2.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, qlib_request_duration_seconds_bucket) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"

      - alert: ModelAccuracyDrop
        expr: qlib_model_accuracy < 0.7
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Model accuracy dropped"
          description: "Model {{ $labels.model_name }} accuracy is {{ $value }}"
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### ç³»ç»Ÿçº§æ€§èƒ½ä¼˜åŒ–

#### 1. ç¡¬ä»¶ä¼˜åŒ–å»ºè®®
```yaml
# ç”Ÿäº§ç¯å¢ƒç¡¬ä»¶é…ç½®
production_hardware:
  cpu:
    - "Intel Xeon Gold 6248R (24 cores, 48 threads)"
    - "AMD EPYC 7742 (64 cores, 128 threads)"

  memory:
    - "DDR4 ECC RAM: 256GB+"
    - "å†…å­˜é¢‘ç‡: 3200MHz+"
    - "é€šé“é…ç½®: 8é€šé“"

  storage:
    - "ä¸»å­˜å‚¨: NVMe SSD (è¯»å†™é€Ÿåº¦ 3500MB/s+)"
    - "æ•°æ®å­˜å‚¨: ä¼ä¸šçº§ SSDé˜µåˆ—"
    - "å¤‡ä»½å­˜å‚¨: é«˜å®¹é‡HDD"

  gpu:
    - "NVIDIA A100 (40GB HBM2)"
    - "NVIDIA V100 (32GB HBM2)"
    - "æ”¯æŒ NVLink å’Œ GPU Direct"
```

#### 2. æ“ä½œç³»ç»Ÿä¼˜åŒ–
```bash
#!/bin/bash
# scripts/system_optimization.sh

# å†…æ ¸å‚æ•°ä¼˜åŒ–
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 65536 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf

# æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
echo 'vm.swappiness = 10' >> /etc/sysctl.conf
echo 'vm.dirty_ratio = 15' >> /etc/sysctl.conf
echo 'vm.dirty_background_ratio = 5' >> /etc/sysctl.conf

# åº”ç”¨ä¼˜åŒ–
sysctl -p

# CPU äº²å’Œæ€§è®¾ç½®
taskset -c 0-23 python -m qlib.contrib.online.serve

# å¤§é¡µå†…å­˜é…ç½®
echo 1024 > /proc/sys/vm/nr_hugepages
```

### åº”ç”¨çº§æ€§èƒ½ä¼˜åŒ–

#### 1. æ•°æ®å¤„ç†ä¼˜åŒ–
```python
# performance/data_optimization.py
import pandas as pd
import numpy as np
from multiprocessing import Pool
import joblib
from functools import lru_cache

class OptimizedDataProcessor:
    def __init__(self, n_workers=8):
        self.n_workers = n_workers
        self.memory_map = {}

    @lru_cache(maxsize=1000)
    def cached_calculation(self, params_hash):
        """ç¼“å­˜è®¡ç®—ç»“æœ"""
        # å®é™…è®¡ç®—é€»è¾‘
        return result

    def parallel_processing(self, data_chunks):
        """å¹¶è¡Œå¤„ç†æ•°æ®å—"""
        with Pool(self.n_workers) as pool:
            results = pool.map(self.process_chunk, data_chunks)
        return pd.concat(results, ignore_index=True)

    def memory_efficient_load(self, file_path, chunksize=10000):
        """å†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½"""
        chunks = []
        for chunk in pd.read_csv(file_path, chunksize=chunksize):
            # æ•°æ®ç±»å‹ä¼˜åŒ–
            chunk = self.optimize_dtypes(chunk)
            chunks.append(chunk)
            # å†…å­˜æ¸…ç†
            del chunk

        return pd.concat(chunks, ignore_index=True)

    def optimize_dtypes(self, df):
        """ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"""
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')

        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')

        return df
```

#### 2. æ¨¡å‹æ¨ç†ä¼˜åŒ–
```python
# performance/model_optimization.py
import torch
import onnxruntime as ort
from concurrent.futures import ThreadPoolExecutor
import asyncio

class OptimizedModelService:
    def __init__(self, model_path, batch_size=32):
        self.model_path = model_path
        self.batch_size = batch_size
        self.model = self.load_optimized_model()
        self.executor = ThreadPoolExecutor(max_workers=4)

    def load_optimized_model(self):
        """åŠ è½½ä¼˜åŒ–åçš„æ¨¡å‹"""
        # ä½¿ç”¨ ONNX Runtime
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        session = ort.InferenceSession(
            self.model_path,
            providers=providers
        )
        return session

    async def batch_predict(self, inputs):
        """æ‰¹é‡é¢„æµ‹ä¼˜åŒ–"""
        results = []
        for i in range(0, len(inputs), self.batch_size):
            batch = inputs[i:i + self.batch_size]
            batch_result = await self.async_predict_batch(batch)
            results.extend(batch_result)

        return results

    async def async_predict_batch(self, batch):
        """å¼‚æ­¥æ‰¹é‡é¢„æµ‹"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._predict_batch,
            batch
        )

    def _predict_batch(self, batch):
        """å®é™…æ‰¹é‡é¢„æµ‹è®¡ç®—"""
        input_tensor = torch.tensor(batch, dtype=torch.float32)

        with torch.no_grad():
            if torch.cuda.is_available():
                input_tensor = input_tensor.cuda()

            outputs = self.model.run(None, {'input': input_tensor.cpu().numpy()})
            return outputs[0].tolist()

    def warmup_model(self):
        """æ¨¡å‹é¢„çƒ­"""
        dummy_input = torch.randn(self.batch_size, self.input_dim)
        self._predict_batch(dummy_input.numpy())
```

#### 3. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
```python
# performance/cache_optimization.py
import redis
import pickle
from typing import Any, Optional
import hashlib
import json

class AdvancedCacheManager:
    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.local_cache = {}
        self.cache_stats = {"hits": 0, "misses": 0}

    def generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            "func": func_name,
            "args": args,
            "kwargs": kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.md5(key_str.encode()).hexdigest()

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        # æœ¬åœ°ç¼“å­˜æ£€æŸ¥
        if key in self.local_cache:
            self.cache_stats["hits"] += 1
            return self.local_cache[key]

        # Redis ç¼“å­˜æ£€æŸ¥
        try:
            data = self.redis_client.get(key)
            if data:
                result = pickle.loads(data)
                # å›å¡«æœ¬åœ°ç¼“å­˜
                self.local_cache[key] = result
                self.cache_stats["hits"] += 1
                return result
        except Exception as e:
            print(f"Cache get error: {e}")

        self.cache_stats["misses"] += 1
        return None

    def set(self, key: str, value: Any, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        # æœ¬åœ°ç¼“å­˜
        self.local_cache[key] = value

        # Redis ç¼“å­˜
        try:
            serialized = pickle.dumps(value)
            self.redis_client.setex(key, ttl, serialized)
        except Exception as e:
            print(f"Cache set error: {e}")

    def cache_function_result(self, ttl: int = 3600):
        """å‡½æ•°ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                cache_key = self.generate_cache_key(
                    func.__name__, args, kwargs
                )

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.get(cache_key)
                if cached_result is not None:
                    return cached_result

                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                self.set(cache_key, result, ttl)
                return result

            return wrapper
        return decorator

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = self.cache_stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            **self.cache_stats,
            "hit_rate": hit_rate,
            "local_cache_size": len(self.local_cache)
        }
```

### æ•°æ®åº“ä¼˜åŒ–

#### 1. PostgreSQL ä¼˜åŒ–é…ç½®
```sql
-- æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–é…ç½®
-- postgresql.conf å…³é”®å‚æ•°

-- å†…å­˜é…ç½®
shared_buffers = 64GB          -- ç³»ç»Ÿå†…å­˜çš„ 25%
effective_cache_size = 192GB   -- ç³»ç»Ÿå†…å­˜çš„ 75%
work_mem = 256MB               -- æ’åºå’Œå“ˆå¸Œæ“ä½œå†…å­˜
maintenance_work_mem = 2GB     -- ç»´æŠ¤æ“ä½œå†…å­˜

-- è¿æ¥é…ç½®
max_connections = 200          -- æœ€å¤§è¿æ¥æ•°
shared_preload_libraries = 'pg_stat_statements'

-- æ£€æŸ¥ç‚¹é…ç½®
checkpoint_completion_target = 0.9
wal_buffers = 64MB
default_statistics_target = 100

-- æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1         -- SSD ä¼˜åŒ–
effective_io_concurrency = 200 -- SSD å¹¶å‘ I/O
```

#### 2. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
```sql
-- å…³é”®æŸ¥è¯¢ç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_instruments_date ON instruments(date);
CREATE INDEX CONCURRENTLY idx_features_symbol_date ON features(symbol, date);
CREATE INDEX CONCURRENTLY idx_predictions_model_date ON predictions(model_id, date);

-- åˆ†åŒºè¡¨ä¼˜åŒ–
CREATE TABLE features_partitioned (
    LIKE features INCLUDING ALL
) PARTITION BY RANGE (date);

CREATE TABLE features_2023 PARTITION OF features_partitioned
    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;
```

## å®¹ç¾å’Œé«˜å¯ç”¨

### å¤‡ä»½æ¢å¤ç­–ç•¥

#### 1. è‡ªåŠ¨åŒ–å¤‡ä»½
```bash
#!/bin/bash
# scripts/automated_backup.sh

BACKUP_CONFIG="/app/config/backup_config.json"
LOG_FILE="/app/logs/backup.log"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

backup_database() {
    log "Starting database backup..."

    # åˆ›å»ºå¤‡ä»½ç›®å½•
    BACKUP_DIR="/backup/database/$(date +%Y%m%d_%H%M%S)"
    mkdir -p $BACKUP_DIR

    # å…¨é‡å¤‡ä»½
    pg_dump -h $DB_HOST -U $DB_USER -d qlib_prod \
        --format=custom --compress=9 \
        --file=$BACKUP_DIR/qlib_full.dump

    # éªŒè¯å¤‡ä»½
    pg_restore --list $BACKUP_DIR/qlib_full.dump > /dev/null
    if [ $? -eq 0 ]; then
        log "Database backup completed successfully"
        # ä¸Šä¼ åˆ°äº‘å­˜å‚¨
        aws s3 cp $BACKUP_DIR s3://qlib-backups/database/ --recursive
    else
        log "ERROR: Database backup validation failed"
    fi
}

backup_models() {
    log "Starting models backup..."

    MODEL_BACKUP_DIR="/backup/models/$(date +%Y%m%d_%H%M%S)"
    mkdir -p $MODEL_BACKUP_DIR

    # å¢é‡å¤‡ä»½æ¨¡å‹æ–‡ä»¶
    rsync -av --link-dest=/backup/models/latest \
        /app/models/ $MODEL_BACKUP_DIR/

    # æ›´æ–°æœ€æ–°é“¾æ¥
    rm -f /backup/models/latest
    ln -s $MODEL_BACKUP_DIR /backup/models/latest

    log "Models backup completed"
}

# æ¸…ç†æ—§å¤‡ä»½
cleanup_old_backups() {
    log "Cleaning up old backups..."

    # ä¿ç•™æœ€è¿‘30å¤©çš„å¤‡ä»½
    find /backup -type d -mtime +30 -exec rm -rf {} \;

    # æ¸…ç†äº‘å­˜å‚¨æ—§å¤‡ä»½
    aws s3 ls s3://qlib-backups/ | while read -r line; do
        createDate=$(echo $line | awk '{print $1" "$2}')
        createDate=$(date -d "$createDate" +%s)
        olderThan=$(date -d "30 days ago" +%s)

        if [[ $createDate -lt $olderThan ]]; then
            fileName=$(echo $line | awk '{print $4}')
            if [[ $fileName != "" ]]; then
                aws s3 rm s3://qlib-backups/$fileName --recursive
            fi
        fi
    done
}

# æ‰§è¡Œå¤‡ä»½æµç¨‹
backup_database
backup_models
cleanup_old_backups

log "Backup process completed"
```

#### 2. ç¾éš¾æ¢å¤è®¡åˆ’
```bash
#!/bin/bash
# scripts/disaster_recovery.sh

RECOVERY_PLAN="/app/config/recovery_plan.json"
LOG_FILE="/app/logs/recovery.log"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

check_recovery_prerequisites() {
    log "Checking recovery prerequisites..."

    # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶å¯ç”¨æ€§
    if ! aws s3 ls s3://qlib-backups/database/ > /dev/null 2>&1; then
        log "ERROR: Cannot access backup storage"
        exit 1
    fi

    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    if ! pg_isready -h $DB_HOST -p $DB_PORT; then
        log "ERROR: Database is not ready"
        exit 1
    fi

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    REQUIRED_SPACE=100  # GB
    AVAILABLE_SPACE=$(df / | awk 'NR==2 {print int($4/1024/1024)}')

    if [ $AVAILABLE_SPACE -lt $REQUIRED_SPACE ]; then
        log "ERROR: Insufficient disk space. Required: ${REQUIRED_SPACE}GB, Available: ${AVAILABLE_SPACE}GB"
        exit 1
    fi

    log "Recovery prerequisites check passed"
}

recover_database() {
    log "Starting database recovery..."

    # ä¸‹è½½æœ€æ–°å¤‡ä»½
    LATEST_BACKUP=$(aws s3 ls s3://qlib-backups/database/ --recursive | sort | tail -n 1 | awk '{print $4}')
    aws s3 cp s3://qlib-backups/database/$LATEST_BACKUP /tmp/recovery.dump

    # åœæ­¢åº”ç”¨æœåŠ¡
    systemctl stop qlib-service

    # æ¢å¤æ•°æ®åº“
    dropdb -h $DB_HOST -U $DB_USER qlib_prod
    createdb -h $DB_HOST -U $DB_USER qlib_prod

    pg_restore -h $DB_HOST -U $DB_USER -d qlib_prod \
        --clean --if-exists --verbose /tmp/recovery.dump

    # éªŒè¯æ¢å¤
    psql -h $DB_HOST -U $DB_USER -d qlib_prod -c "SELECT COUNT(*) FROM instruments;"

    if [ $? -eq 0 ]; then
        log "Database recovery completed successfully"
    else
        log "ERROR: Database recovery failed"
        exit 1
    fi
}

recover_models() {
    log "Starting models recovery..."

    # ä¸‹è½½æœ€æ–°æ¨¡å‹å¤‡ä»½
    LATEST_MODEL_BACKUP=$(aws s3 ls s3://qlib-backups/models/ --recursive | sort | tail -n 1 | awk '{print $4}')
    aws s3 cp s3://qlib-backups/models/$LATEST_MODEL_BACKUP /tmp/models.tar.gz

    # æ¢å¤æ¨¡å‹æ–‡ä»¶
    rm -rf /app/models/*
    tar -xzf /tmp/models.tar.gz -C /app/models/

    # éªŒè¯æ¨¡å‹æ–‡ä»¶
    python -c "import joblib; model = joblib.load('/app/models/latest_model.pkl'); print('Model loaded successfully')"

    if [ $? -eq 0 ]; then
        log "Models recovery completed successfully"
    else
        log "ERROR: Models recovery failed"
        exit 1
    fi
}

restart_services() {
    log "Restarting services..."

    # å¯åŠ¨åº”ç”¨æœåŠ¡
    systemctl start qlib-service

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 30

    # å¥åº·æ£€æŸ¥
    curl -f http://localhost:8000/health || {
        log "ERROR: Service health check failed"
        exit 1
    }

    log "Services restarted successfully"
}

# æ‰§è¡Œæ¢å¤æµç¨‹
check_recovery_prerequisites
recover_database
recover_models
restart_services

log "Disaster recovery completed successfully"
```

### è´Ÿè½½å‡è¡¡å’Œæ‰©å±•

#### 1. Nginx è´Ÿè½½å‡è¡¡é…ç½®
```nginx
# nginx/nginx.conf
upstream qlib_backend {
    least_conn;
    server qlib-1:8000 max_fails=3 fail_timeout=30s;
    server qlib-2:8000 max_fails=3 fail_timeout=30s;
    server qlib-3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name api.qlib.example.com;

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;

    location / {
        proxy_pass http://qlib_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }

    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /health {
        access_log off;
        proxy_pass http://qlib_backend/health;
    }

    # é™æ€æ–‡ä»¶ç¼“å­˜
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

## æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜è¯Šæ–­

#### 1. æœåŠ¡å¯åŠ¨é—®é¢˜
```bash
#!/bin/bash
# scripts/troubleshoot.sh

check_service_status() {
    echo "=== Service Status Check ==="
    systemctl status qlib-service
    journalctl -u qlib-service --since "1 hour ago" --no-pager
}

check_dependencies() {
    echo "=== Dependency Check ==="

    # Python ç¯å¢ƒ
    python --version
    pip list | grep qlib

    # æ•°æ®åº“è¿æ¥
    pg_isready -h $DB_HOST -p $DB_PORT

    # Redis è¿æ¥
    redis-cli -h $REDIS_HOST -p $REDIS_PORT ping

    # ç£ç›˜ç©ºé—´
    df -h

    # å†…å­˜ä½¿ç”¨
    free -h
}

check_model_files() {
    echo "=== Model Files Check ==="

    if [ -f "/app/models/latest_model.pkl" ]; then
        echo "Latest model file exists"
        ls -lh /app/models/latest_model.pkl
    else
        echo "WARNING: Latest model file not found"
    fi

    # æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
    python -c "
import joblib
try:
    model = joblib.load('/app/models/latest_model.pkl')
    print('Model loaded successfully')
    print(f'Model type: {type(model)}')
except Exception as e:
    print(f'Model loading failed: {e}')
"
}

check_logs() {
    echo "=== Recent Logs ==="
    tail -n 50 /app/logs/qlib.log | grep -E "(ERROR|WARN|CRITICAL)"
}

# æ‰§è¡Œè¯Šæ–­
check_service_status
check_dependencies
check_model_files
check_logs

echo "=== Troubleshooting completed ==="
```

#### 2. æ€§èƒ½é—®é¢˜è¯Šæ–­
```python
# scripts/performance_diagnostics.py
import psutil
import time
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

class PerformanceDiagnostics:
    def __init__(self):
        self.metrics_history = []

    def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "timestamp": datetime.now(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_io_read": psutil.disk_io_counters().read_bytes,
            "disk_io_write": psutil.disk_io_counters().write_bytes,
            "network_io_sent": psutil.net_io_counters().bytes_sent,
            "network_io_recv": psutil.net_io_counters().bytes_recv,
        }

        self.metrics_history.append(metrics)
        return metrics

    def analyze_performance_trends(self, duration_minutes=60):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        print(f"=== Performance Analysis (Last {duration_minutes} minutes) ===")

        # æ”¶é›†æŒ‡æ ‡
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=duration_minutes)

        relevant_metrics = [
            m for m in self.metrics_history
            if m["timestamp"] >= start_time
        ]

        if not relevant_metrics:
            print("No metrics data available for analysis")
            return

        df = pd.DataFrame(relevant_metrics)

        # CPU åˆ†æ
        avg_cpu = df["cpu_percent"].mean()
        max_cpu = df["cpu_percent"].max()
        print(f"CPU Usage - Average: {avg_cpu:.1f}%, Max: {max_cpu:.1f}%")

        # å†…å­˜åˆ†æ
        avg_memory = df["memory_percent"].mean()
        max_memory = df["memory_percent"].max()
        print(f"Memory Usage - Average: {avg_memory:.1f}%, Max: {max_memory:.1f}%")

        # ç£ç›˜ I/O åˆ†æ
        if len(df) > 1:
            disk_read_rate = (df["disk_io_read"].iloc[-1] - df["disk_io_read"].iloc[0]) / duration_minutes / 1024 / 1024
            disk_write_rate = (df["disk_io_write"].iloc[-1] - df["disk_io_write"].iloc[0]) / duration_minutes / 1024 / 1024
            print(f"Disk I/O - Read: {disk_read_rate:.1f} MB/min, Write: {disk_write_rate:.1f} MB/min")

        # æ€§èƒ½å»ºè®®
        if avg_cpu > 80:
            print("âš ï¸  High CPU usage detected. Consider scaling or optimization.")

        if avg_memory > 85:
            print("âš ï¸  High memory usage detected. Check for memory leaks.")

        if disk_read_rate > 100 or disk_write_rate > 100:
            print("âš ï¸  High disk I/O detected. Consider optimizing data access patterns.")

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics_history:
            print("No performance data available")
            return

        df = pd.DataFrame(self.metrics_history)

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # CPU ä½¿ç”¨ç‡
        axes[0, 0].plot(df["timestamp"], df["cpu_percent"])
        axes[0, 0].set_title("CPU Usage (%)")
        axes[0, 0].set_ylabel("Percentage")

        # å†…å­˜ä½¿ç”¨ç‡
        axes[0, 1].plot(df["timestamp"], df["memory_percent"])
        axes[0, 1].set_title("Memory Usage (%)")
        axes[0, 1].set_ylabel("Percentage")

        # ç£ç›˜ I/O
        axes[1, 0].plot(df["timestamp"], df["disk_io_read"], label="Read")
        axes[1, 0].plot(df["timestamp"], df["disk_io_write"], label="Write")
        axes[1, 0].set_title("Disk I/O (bytes)")
        axes[1, 0].legend()

        # ç½‘ç»œ I/O
        axes[1, 1].plot(df["timestamp"], df["network_io_sent"], label="Sent")
        axes[1, 1].plot(df["timestamp"], df["network_io_recv"], label="Received")
        axes[1, 1].set_title("Network I/O (bytes)")
        axes[1, 1].legend()

        plt.tight_layout()
        plt.savefig("/app/logs/performance_report.png", dpi=300, bbox_inches='tight')
        print("Performance report saved to /app/logs/performance_report.png")

if __name__ == "__main__":
    diagnostics = PerformanceDiagnostics()

    # æŒç»­ç›‘æ§
    try:
        while True:
            diagnostics.collect_system_metrics()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡æŒ‡æ ‡
    except KeyboardInterrupt:
        print("\nGenerating performance analysis...")
        diagnostics.analyze_performance_trends()
        diagnostics.generate_performance_report()
```

## æœ€ä½³å®è·µæ€»ç»“

### å¼€å‘æœ€ä½³å®è·µ

1. **ä»£ç è´¨é‡**
   - éµå¾ª PEP 8 ç¼–ç è§„èŒƒ
   - ä½¿ç”¨ç±»å‹æç¤ºå’Œæ–‡æ¡£å­—ç¬¦ä¸²
   - ç¼–å†™å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
   - å®šæœŸè¿›è¡Œä»£ç å®¡æŸ¥

2. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
   - åˆç†ä½¿ç”¨ç¼“å­˜æœºåˆ¶
   - å¼‚æ­¥å¤„ç† I/O å¯†é›†å‹æ“ä½œ
   - ç›‘æ§å’Œä¼˜åŒ–å†…å­˜ä½¿ç”¨

3. **å®‰å…¨å®è·µ**
   - å®šæœŸæ›´æ–°ä¾èµ–åŒ…
   - ä½¿ç”¨ HTTPS åŠ å¯†é€šä¿¡
   - å®æ–½è®¿é—®æ§åˆ¶å’Œæƒé™ç®¡ç†
   - å®šæœŸè¿›è¡Œå®‰å…¨å®¡è®¡

### è¿ç»´æœ€ä½³å®è·µ

1. **ç›‘æ§å‘Šè­¦**
   - å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»
   - è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
   - å»ºç«‹å€¼ç­å’Œå“åº”æœºåˆ¶
   - å®šæœŸè¿›è¡Œæ•…éšœæ¼”ç»ƒ

2. **å¤‡ä»½æ¢å¤**
   - åˆ¶å®šè¯¦ç»†çš„å¤‡ä»½ç­–ç•¥
   - å®šæœŸæµ‹è¯•æ¢å¤æµç¨‹
   - å®æ–½å¼‚åœ°å¤‡ä»½
   - å»ºç«‹ç¾éš¾æ¢å¤é¢„æ¡ˆ

3. **æ€§èƒ½è°ƒä¼˜**
   - å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•
   - è°ƒæ•´ç³»ç»Ÿå‚æ•°é…ç½®
   - å®æ–½è‡ªåŠ¨æ‰©ç¼©å®¹

## å˜æ›´è®°å½• (Changelog)

### 2025-11-17 14:15:47 - ç¬¬å…­æ¬¡å¢é‡æ›´æ–°
- âœ¨ **æ–°å¢éƒ¨ç½²è¿ç»´å®Œæ•´æŒ‡å—**ï¼š
  - ä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´éƒ¨ç½²æ–¹æ¡ˆ
  - Docker å®¹å™¨åŒ–å’Œ Kubernetes é›†ç¾¤éƒ¨ç½²
  - ç›‘æ§å‘Šè­¦å’Œæ—¥å¿—ç®¡ç†ä½“ç³»
- ğŸ“Š **æ·±åº¦æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š
  - ç³»ç»Ÿçº§ã€åº”ç”¨çº§ã€æ•°æ®åº“çº§ä¼˜åŒ–æ–¹æ¡ˆ
  - ç¼“å­˜ç­–ç•¥å’Œå¹¶å‘å¤„ç†ä¼˜åŒ–
  - GPU åŠ é€Ÿå’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯
- ğŸ”— **å®Œå–„æ•…éšœæ’é™¤å’Œç»´æŠ¤æŒ‡å—**ï¼š
  - å¸¸è§é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ
  - æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·
  - å®¹ç¾å’Œé«˜å¯ç”¨æ¶æ„è®¾è®¡
- ğŸ“ **è¡¥å……æœ€ä½³å®è·µå’Œè¿ç»´å»ºè®®**ï¼š
  - å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒç®¡ç†è§„èŒƒ
  - å®‰å…¨åŠ å›ºå’Œåˆè§„æ€§è¦æ±‚
  - æŒç»­é›†æˆå’ŒæŒç»­éƒ¨ç½²æµç¨‹

---

## æ–‡æ¡£çŠ¶æ€

- **æ–‡æ¡£è¦†ç›–åº¦**: 100%
- **è¯¦ç»†ç¨‹åº¦**: ç”Ÿäº§å°±ç»ª
- **å®ç”¨æ€§**: â­â­â­â­â­
- **ç»´æŠ¤çŠ¶æ€**: âœ… æŒç»­æ›´æ–°

> ğŸ’¡ **ä½¿ç”¨å»ºè®®**: æœ¬æ–‡æ¡£æä¾›äº† Qlib é¡¹ç›®å®Œæ•´çš„éƒ¨ç½²è¿ç»´æŒ‡å—ã€‚å»ºè®®æ ¹æ®å®é™…ç¯å¢ƒéœ€æ±‚è°ƒæ•´é…ç½®å‚æ•°ï¼Œå¹¶åœ¨æµ‹è¯•ç¯å¢ƒä¸­å……åˆ†éªŒè¯åå†åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒã€‚